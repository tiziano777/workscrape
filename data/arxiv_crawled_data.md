[{'id': 'http://arxiv.org/abs/2508.05629v1', 'title': 'On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification', 'published': '2025-08-07T17:59:04Z', 'updated': '2025-08-07T17:59:04Z', 'abstract': 'We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.', 'authors': ['Yongliang Wu', 'Yizhou Zhou', 'Zhou Ziheng', 'Yingzhe Peng', 'Xinyu Ye', 'Xinting Hu', 'Wenbo Zhu', 'Lu Qi', 'Ming-Hsuan Yang', 'Xu Yang']}, {'id': 'http://arxiv.org/abs/2508.05625v1', 'title': 'How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in   Multi-Turn Conversations', 'published': '2025-08-07T17:58:41Z', 'updated': '2025-08-07T17:58:41Z', 'abstract': 'Large Language Models (LLMs) have started to demonstrate the ability to persuade humans, yet our understanding of how this dynamic transpires is limited. Recent work has used linear probes, lightweight tools for analyzing model representations, to study various LLM skills such as the ability to model user sentiment and political perspective. Motivated by this, we apply probes to study persuasion dynamics in natural, multi-turn conversations. We leverage insights from cognitive science to train probes on distinct aspects of persuasion: persuasion success, persuadee personality, and persuasion strategy. Despite their simplicity, we show that they capture various aspects of persuasion at both the sample and dataset levels. For instance, probes can identify the point in a conversation where the persuadee was persuaded or where persuasive success generally occurs across the entire dataset. We also show that in addition to being faster than expensive prompting-based approaches, probes can do just as well and even outperform prompting in some settings, such as when uncovering persuasion strategy. This suggests probes as a plausible avenue for studying other complex behaviours such as deception and manipulation, especially in multi-turn settings and large-scale dataset analysis where prompting-based methods would be computationally inefficient.', 'authors': ['Brandon Jaipersaud', 'David Krueger', 'Ekdeep Singh Lubana']}, {'id': 'http://arxiv.org/abs/2508.05622v1', 'title': 'Simulating Human-Like Learning Dynamics with LLM-Empowered Agents', 'published': '2025-08-07T17:57:46Z', 'updated': '2025-08-07T17:57:46Z', 'abstract': 'Capturing human learning behavior based on deep learning methods has become a major research focus in both psychology and intelligent systems. Recent approaches rely on controlled experiments or rule-based models to explore cognitive processes. However, they struggle to capture learning dynamics, track progress over time, or provide explainability. To address these challenges, we introduce LearnerAgent, a novel multi-agent framework based on Large Language Models (LLMs) to simulate a realistic teaching environment. To explore human-like learning dynamics, we construct learners with psychologically grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free General Learner to inspect the base LLM\'s default behavior. Through weekly knowledge acquisition, monthly strategic choices, periodic tests, and peer interaction, we can track the dynamic learning progress of individual learners over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis reveals that only Deep Learner achieves sustained cognitive growth. Our specially designed "trap questions" effectively diagnose Surface Learner\'s shallow knowledge. 2) The behavioral and cognitive patterns of distinct learners align closely with their psychological profiles. 3) Learners\' self-concept scores evolve realistically, with the General Learner developing surprisingly high self-efficacy despite its cognitive limitations. 4) Critically, the default profile of base LLM is a "diligent but brittle Surface Learner"-an agent that mimics the behaviors of a good student but lacks true, generalizable understanding. Extensive simulation experiments demonstrate that LearnerAgent aligns well with real scenarios, yielding more insightful findings about LLMs\' behavior.', 'authors': ['Yu Yuan', 'Lili Zhao', 'Wei Chen', 'Guangting Zheng', 'Kai Zhang', 'Mengdi Zhang', 'Qi Liu']}, {'id': 'http://arxiv.org/abs/2508.05616v1', 'title': 'TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven   Evolution', 'published': '2025-08-07T17:55:10Z', 'updated': '2025-08-07T17:55:10Z', 'abstract': 'Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.', 'authors': ['Zhikai Zhao', 'Chuanbo Hua', 'Federico Berto', 'Kanghoon Lee', 'Zihan Ma', 'Jiachen Li', 'Jinkyoo Park']}, {'id': 'http://arxiv.org/abs/2508.05613v1', 'title': 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning   for Large Language Models', 'published': '2025-08-07T17:53:56Z', 'updated': '2025-08-07T17:53:56Z', 'abstract': 'Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.', 'authors': ['Haitao Hong', 'Yuchen Yan', 'Xingyu Wu', 'Guiyang Hou', 'Wenqi Zhang', 'Weiming Lu', 'Yongliang Shen', 'Jun Xiao']}, {'id': 'http://arxiv.org/abs/2508.05606v1', 'title': 'Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision', 'published': '2025-08-07T17:45:17Z', 'updated': '2025-08-07T17:45:17Z', 'abstract': 'Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/', 'authors': ['Luozheng Qin', 'Jia Gong', 'Yuqing Sun', 'Tianjiao Li', 'Mengping Yang', 'Xiaomeng Yang', 'Chao Qu', 'Zhiyu Tan', 'Hao Li']}, {'id': 'http://arxiv.org/abs/2508.05592v1', 'title': 'MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy', 'published': '2025-08-07T17:32:14Z', 'updated': '2025-08-07T17:32:14Z', 'abstract': 'Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities.', 'authors': ['Shaoxiong Zhan', 'Yanlin Lai', 'Ziyu Lu', 'Dahua Lin', 'Ziqing Yang', 'Fei Tang']}, {'id': 'http://arxiv.org/abs/2508.05585v1', 'title': 'DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label   Recognition', 'published': '2025-08-07T17:22:33Z', 'updated': '2025-08-07T17:22:33Z', 'abstract': 'Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness.', 'authors': ['Haijing Liu', 'Tao Pu', 'Hefeng Wu', 'Keze Wang', 'Liang Lin']}, {'id': 'http://arxiv.org/abs/2508.05581v1', 'title': 'Iterative Learning of Computable Phenotypes for Treatment Resistant   Hypertension using Large Language Models', 'published': '2025-08-07T17:15:17Z', 'updated': '2025-08-07T17:15:17Z', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities for medical question answering and programming, but their potential for generating interpretable computable phenotypes (CPs) is under-explored. In this work, we investigate whether LLMs can generate accurate and concise CPs for six clinical phenotypes of varying complexity, which could be leveraged to enable scalable clinical decision support to improve care for patients with hypertension. In addition to evaluating zero-short performance, we propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback. Our results show that LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.', 'authors': ['Guilherme Seidyo Imai Aldeia', 'Daniel S. Herman', 'William G. La Cava']}, {'id': 'http://arxiv.org/abs/2508.05571v1', 'title': 'Fairy$\\pm i$: the First 2-bit Complex LLM with All Parameters in   $\\{\\pm1, \\pm i\\}$', 'published': '2025-08-07T17:02:23Z', 'updated': '2025-08-07T17:02:23Z', 'abstract': 'Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity $\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy$\\pm i$ outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.', 'authors': ['Feiyu Wang', 'Guoan Wang', 'Yihao Zhang', 'Shengfan Wang', 'Weitao Li', 'Bokai Huang', 'Shimao Chen', 'Zihan Jiang', 'Rui Xu', 'Tong Yang']}, {'id': 'http://arxiv.org/abs/2508.05553v1', 'title': 'Do Political Opinions Transfer Between Western Languages? An Analysis of   Unaligned and Aligned Multilingual LLMs', 'published': '2025-08-07T16:33:45Z', 'updated': '2025-08-07T16:33:45Z', 'abstract': "Public opinion surveys show cross-cultural differences in political opinions between socio-cultural contexts. However, there is no clear evidence whether these differences translate to cross-lingual differences in multilingual large language models (MLLMs). We analyze whether opinions transfer between languages or whether there are separate opinions for each language in MLLMs of various sizes across five Western languages. We evaluate MLLMs' opinions by prompting them to report their (dis)agreement with political statements from voting advice applications. To better understand the interaction between languages in the models, we evaluate them both before and after aligning them with more left or right views using direct preference optimization and English alignment data only. Our findings reveal that unaligned models show only very few significant cross-lingual differences in the political opinions they reflect. The political alignment shifts opinions almost uniformly across all five languages. We conclude that in Western language contexts, political opinions transfer between languages, demonstrating the challenges in achieving explicit socio-linguistic, cultural, and political alignment of MLLMs.", 'authors': ['Franziska Weeber', 'Tanise Ceron', 'Sebastian Padó']}, {'id': 'http://arxiv.org/abs/2508.05544v1', 'title': 'Conformal Sets in Multiple-Choice Question Answering under Black-Box   Settings with Provable Coverage Guarantees', 'published': '2025-08-07T16:22:49Z', 'updated': '2025-08-07T16:22:49Z', 'abstract': "Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications.", 'authors': ['Guang Yang', 'Xinyang Liu']}, {'id': 'http://arxiv.org/abs/2508.05545v1', 'title': 'PRvL: Quantifying the Capabilities and Risks of Large Language Models   for PII Redaction', 'published': '2025-08-07T16:22:49Z', 'updated': '2025-08-07T16:22:49Z', 'abstract': 'Redacting Personally Identifiable Information (PII) from unstructured text is critical for ensuring data privacy in regulated domains. While earlier approaches have relied on rule-based systems and domain-specific Named Entity Recognition (NER) models, these methods fail to generalize across formats and contexts. Recent advances in Large Language Models (LLMs) offer a promising alternative, yet the effect of architectural and training choices on redaction performance remains underexplored. LLMs have demonstrated strong performance in tasks that require contextual language understanding, including the redaction of PII in free-form text. Prior work suggests that with appropriate adaptation, LLMs can become effective contextual privacy learners. However, the consequences of architectural and training choices for PII Redaction remain underexplored. In this work, we present a comprehensive analysis of LLMs as privacy-preserving PII Redaction systems. We evaluate a range of LLM architectures and training strategies for their effectiveness in PII Redaction. Our analysis measures redaction performance, semantic preservation, and PII leakage, and compares these outcomes against latency and computational cost. The results provide practical guidance for configuring LLM-based redactors that are accurate, efficient, and privacy-aware. To support reproducibility and real-world deployment, we release PRvL, an open-source suite of fine-tuned models, and evaluation tools for general-purpose PII Redaction. PRvL is built entirely on open-source LLMs and supports multiple inference settings for flexibility and compliance. It is designed to be easily customized for different domains and fully operable within secure, self-managed environments. This enables data owners to perform redactions without relying on third-party services or exposing sensitive content beyond their own infrastructure.', 'authors': ['Leon Garza', 'Anantaa Kotal', 'Aritran Piplai', 'Lavanya Elluri', 'Prajit Das', 'Aman Chadha']}, {'id': 'http://arxiv.org/abs/2508.05535v1', 'title': 'Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation', 'published': '2025-08-07T16:09:12Z', 'updated': '2025-08-07T16:09:12Z', 'abstract': "Effective robotic systems for long-horizon human-robot collaboration must adapt to a wide range of human partners, whose physical behavior, willingness to assist, and understanding of the robot's capabilities may change over time. This demands a tightly coupled communication loop that grants both agents the flexibility to propose, accept, or decline requests as they coordinate toward completing the task effectively. We apply a Mixed-Initiative dialog paradigm to Collaborative human-roBot teaming and propose MICoBot, a system that handles the common scenario where both agents, using natural language, take initiative in formulating, accepting, or rejecting proposals on who can best complete different steps of a task. To handle diverse, task-directed dialog, and find successful collaborative strategies that minimize human effort, MICoBot makes decisions at three levels: (1) a meta-planner considers human dialog to formulate and code a high-level collaboration strategy, (2) a planner optimally allocates the remaining steps to either agent based on the robot's capabilities (measured by a simulation-pretrained affordance model) and the human's estimated availability to help, and (3) an action executor decides the low-level actions to perform or words to say to the human. Our extensive evaluations in simulation and real-world -- on a physical robot with 18 unique human participants over 27 hours -- demonstrate the ability of our method to effectively collaborate with diverse human users, yielding significantly improved task success and user experience than a pure LLM baseline and other agent allocation models. See additional videos and materials at https://robin-lab.cs.utexas.edu/MicoBot/.", 'authors': ['Albert Yu', 'Chengshu Li', 'Luca Macesanu', 'Arnav Balaji', 'Ruchira Ray', 'Raymond Mooney', 'Roberto Martín-Martín']}, {'id': 'http://arxiv.org/abs/2508.05534v1', 'title': 'CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text   Generation', 'published': '2025-08-07T16:06:58Z', 'updated': '2025-08-07T16:06:58Z', 'abstract': "Due to their ability to process long and complex contexts, LLMs can offer key benefits to the Legal domain, but their adoption has been hindered by their tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While Retrieval-Augmented Generation offers a promising solution by grounding generations in external knowledge, it offers no guarantee that the provided context will be effectively integrated. To address this, context-aware decoding strategies have been proposed to amplify the influence of relevant context, but they usually do not explicitly enforce faithfulness to the context. In this work, we introduce Confidence-guided Copy-based Decoding for Legal Text Generation (CoCoLex)-a decoding strategy that dynamically interpolates the model produced vocabulary distribution with a distribution derived based on copying from the context. CoCoLex encourages direct copying based on the model's confidence, ensuring greater fidelity to the source. Experimental results on five legal benchmarks demonstrate that CoCoLex outperforms existing context-aware decoding methods, particularly in long-form generation tasks.", 'authors': ['Santosh T. Y. S. S', 'Youssef Tarek Elkhayat', 'Oana Ichim', 'Pranav Shetty', 'Dongsheng Wang', 'Zhiqiang Ma', 'Armineh Nourbakhsh', 'Xiaomo Liu']}, {'id': 'http://arxiv.org/abs/2508.05527v1', 'title': 'AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in   Content Moderation for Brand Safety', 'published': '2025-08-07T15:55:46Z', 'updated': '2025-08-07T15:55:46Z', 'abstract': 'As the volume of video content online grows exponentially, the demand for moderation of unsafe videos has surpassed human capabilities, posing both operational and mental health challenges. While recent studies demonstrated the merits of Multimodal Large Language Models (MLLMs) in various video understanding tasks, their application to multimodal content moderation, a domain that requires nuanced understanding of both visual and textual cues, remains relatively underexplored. In this work, we benchmark the capabilities of MLLMs in brand safety classification, a critical subset of content moderation for safe-guarding advertising integrity. To this end, we introduce a novel, multimodal and multilingual dataset, meticulously labeled by professional reviewers in a multitude of risk categories. Through a detailed comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini, GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost efficiency compared to professional human reviewers. Furthermore, we present an in-depth discussion shedding light on limitations of MLLMs and failure cases. We are releasing our dataset alongside this paper to facilitate future research on effective and responsible brand safety and content moderation.', 'authors': ['Adi Levi', 'Or Levi', 'Sardhendu Mishra', 'Jonathan Morra']}, {'id': 'http://arxiv.org/abs/2508.05525v1', 'title': "The World According to LLMs: How Geographic Origin Influences LLMs'   Entity Deduction Capabilities", 'published': '2025-08-07T15:53:30Z', 'updated': '2025-08-07T15:53:30Z', 'abstract': 'Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities. Notably, the language in which the game is played has minimal impact on performance gaps. These findings demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups. By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes. We release the dataset (Geo20Q+) and code at https://sites.google.com/view/llmbias20q/home.', 'authors': ['Harsh Nishant Lalai', 'Raj Sanjay Shah', 'Jiaxin Pei', 'Sashank Varma', 'Yi-Chia Wang', 'Ali Emami']}, {'id': 'http://arxiv.org/abs/2508.05512v1', 'title': 'RankArena: A Unified Platform for Evaluating Retrieval, Reranking and   RAG with Human and LLM Feedback', 'published': '2025-08-07T15:46:53Z', 'updated': '2025-08-07T15:46:53Z', 'abstract': 'Evaluating the quality of retrieval-augmented generation (RAG) and document reranking systems remains challenging due to the lack of scalable, user-centric, and multi-perspective evaluation tools. We introduce RankArena, a unified platform for comparing and analysing the performance of retrieval pipelines, rerankers, and RAG systems using structured human and LLM-based feedback as well as for collecting such feedback. RankArena supports multiple evaluation modes: direct reranking visualisation, blind pairwise comparisons with human or LLM voting, supervised manual document annotation, and end-to-end RAG answer quality assessment. It captures fine-grained relevance feedback through both pairwise preferences and full-list annotations, along with auxiliary metadata such as movement metrics, annotation time, and quality ratings. The platform also integrates LLM-as-a-judge evaluation, enabling comparison between model-generated rankings and human ground truth annotations. All interactions are stored as structured evaluation datasets that can be used to train rerankers, reward models, judgment agents, or retrieval strategy selectors. Our platform is publicly available at https://rankarena.ngrok.io/, and the Demo video is provided https://youtu.be/jIYAP4PaSSI.', 'authors': ['Abdelrahman Abdallah', 'Mahmoud Abdalla', 'Bhawna Piryani', 'Jamshid Mozafari', 'Mohammed Ali', 'Adam Jatowt']}, {'id': 'http://arxiv.org/abs/2508.05509v1', 'title': 'LAG: Logic-Augmented Generation from a Cartesian Perspective', 'published': '2025-08-07T15:42:00Z', 'updated': '2025-08-07T15:42:00Z', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet exhibit critical limitations in knowledge-intensive tasks, often generating hallucinations when faced with questions requiring specialized expertise. While retrieval-augmented generation (RAG) mitigates this by integrating external knowledge, it struggles with complex reasoning scenarios due to its reliance on direct semantic retrieval and lack of structured logical organization. Inspired by Cartesian principles from \\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented Generation (LAG), a novel paradigm that reframes knowledge augmentation through systematic question decomposition and dependency-aware reasoning. Specifically, LAG first decomposes complex questions into atomic sub-questions ordered by logical dependencies. It then resolves these sequentially, using prior answers to guide context retrieval for subsequent sub-questions, ensuring stepwise grounding in logical chain. To prevent error propagation, LAG incorporates a logical termination mechanism that halts inference upon encountering unanswerable sub-questions and reduces wasted computation on excessive reasoning. Finally, it synthesizes all sub-resolutions to generate verified responses. Experiments on four benchmark datasets demonstrate that LAG significantly enhances reasoning robustness, reduces hallucination, and aligns LLM problem-solving with human cognition, offering a principled alternative to existing RAG systems.", 'authors': ['Yilin Xiao', 'Chuang Zhou', 'Qinggang Zhang', 'Su Dong', 'Shengyuan Chen', 'Xiao Huang']}, {'id': 'http://arxiv.org/abs/2508.05508v1', 'title': 'Auto-Eval Judge: Towards a General Agentic Framework for Task Completion   Evaluation', 'published': '2025-08-07T15:39:48Z', 'updated': '2025-08-07T15:39:48Z', 'abstract': "The increasing adoption of foundation models as agents across diverse domains necessitates a robust evaluation framework. Current methods, such as LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step reasoning that drives agentic decision-making. Meanwhile, existing Agent-as-a-Judge systems, where one agent evaluates another's task completion, are typically designed for narrow, domain-specific settings. To address this gap, we propose a generalizable, modular framework for evaluating agent task completion independent of the task domain. The framework emulates human-like evaluation by decomposing tasks into sub-tasks and validating each step using available information, such as the agent's output and reasoning. Each module contributes to a specific aspect of the evaluation process, and their outputs are aggregated to produce a final verdict on task completion. We validate our framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA and BigCodeBench. Our Judge Agent predicts task success with closer agreement to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy, respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This demonstrates the potential of our proposed general-purpose evaluation framework.", 'authors': ['Roshita Bhonsle', 'Rishav Dutta', 'Sneha Vavilapalli', 'Harsh Seth', 'Abubakarr Jaye', 'Yapei Chang', 'Mukund Rungta', 'Emmanuel Aboah Boateng', 'Sadid Hasan', 'Ehi Nosakhare', 'Soundar Srinivasan']}, {'id': 'http://arxiv.org/abs/2508.05503v1', 'title': 'AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated   Industrial Anomaly Detection', 'published': '2025-08-07T15:36:38Z', 'updated': '2025-08-07T15:36:38Z', 'abstract': 'Industrial anomaly detection (IAD) is critical for manufacturing quality control, but conventionally requires significant manual effort for various application scenarios. This paper introduces AutoIAD, a multi-agent collaboration framework, specifically designed for end-to-end automated development of industrial visual anomaly detection. AutoIAD leverages a Manager-Driven central agent to orchestrate specialized sub-agents (including Data Preparation, Data Loader, Model Designer, Trainer) and integrates a domain-specific knowledge base, which intelligently handles the entire pipeline using raw industrial image data to develop a trained anomaly detection model. We construct a comprehensive benchmark using MVTec AD datasets to evaluate AutoIAD across various LLM backends. Extensive experiments demonstrate that AutoIAD significantly outperforms existing general-purpose agentic collaboration frameworks and traditional AutoML frameworks in task completion rate and model performance (AUROC), while effectively mitigating issues like hallucination through iterative refinement. Ablation studies further confirm the crucial roles of the Manager central agent and the domain knowledge base module in producing robust and high-quality IAD solutions.', 'authors': ['Dongwei Ji', 'Bingzhang Hu', 'Yi Zhou']}, {'id': 'http://arxiv.org/abs/2508.05498v1', 'title': 'GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval   Augmented Reasoning', 'published': '2025-08-07T15:34:41Z', 'updated': '2025-08-07T15:34:41Z', 'abstract': 'Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) techniques have exhibited remarkable performance across a wide range of domains. However, existing RAG approaches primarily operate on unstructured data and demonstrate limited capability in handling structured knowledge such as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally struggle to capture holistic graph structures while simultaneously facing precision control challenges that manifest as either critical information gaps or excessive redundant connections, collectively undermining reasoning performance. To address this challenge, we propose GRAIL: Graph-Retrieval Augmented Interactive Learning, a framework designed to interact with large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL integrates LLM-guided random exploration with path filtering to establish a data synthesis pipeline, where a fine-grained reasoning trajectory is automatically generated for each task. Based on the synthesized data, we then employ a two-stage training process to learn a policy that dynamically decides the optimal actions at each reasoning step. The overall objective of precision-conciseness balance in graph retrieval is decoupled into fine-grained process-supervised rewards to enhance data efficiency and training stability. In practical deployment, GRAIL adopts an interactive retrieval paradigm, enabling the model to autonomously explore graph paths while dynamically balancing retrieval breadth and precision. Extensive experiments have shown that GRAIL achieves an average accuracy improvement of 21.01% and F1 improvement of 22.43% on three knowledge graph question-answering datasets. Our source code and datasets is available at https://github.com/Changgeww/GRAIL.', 'authors': ['Ge Chang', 'Jinbo Su', 'Jiacheng Liu', 'Pengfei Yang', 'Yuhao Shang', 'Huiwen Zheng', 'Hongli Ma', 'Yan Liang', 'Yuanchun Li', 'Yunxin Liu']}, {'id': 'http://arxiv.org/abs/2508.05496v1', 'title': 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs   to Enhance Reasoning Capabilities', 'published': '2025-08-07T15:34:06Z', 'updated': '2025-08-07T15:34:06Z', 'abstract': 'Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.', 'authors': ['Shuo Cai', 'Su Lu', 'Qi Zhou', 'Kejing Yang', 'Zhijie Sang', 'Congkai Xie', 'Hongxia Yang']}, {'id': 'http://arxiv.org/abs/2508.05492v1', 'title': 'MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical   Prediction Modelling', 'published': '2025-08-07T15:28:34Z', 'updated': '2025-08-07T15:28:34Z', 'abstract': 'Multimodal electronic health record (EHR) data provide richer, complementary insights into patient health compared to single-modality data. However, effectively integrating diverse data modalities for clinical prediction modeling remains challenging due to the substantial data requirements. We introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed to leverage multiple large language model (LLM) agents for clinical prediction tasks using multimodal EHR data. MoMA employs specialized LLM agents ("specialist agents") to convert non-textual modalities, such as medical images and laboratory results, into structured textual summaries. These summaries, together with clinical notes, are combined by another LLM ("aggregator agent") to generate a unified multimodal summary, which is then used by a third LLM ("predictor agent") to produce clinical predictions. Evaluating MoMA on three prediction tasks using real-world datasets with different modality combinations and prediction settings, MoMA outperforms current state-of-the-art methods, highlighting its enhanced accuracy and flexibility across various tasks.', 'authors': ['Jifan Gao', 'Mahmudur Rahman', 'John Caskey', 'Madeline Oguss', "Ann O'Rourke", 'Randy Brown', 'Anne Stey', 'Anoop Mayampurath', 'Matthew M. Churpek', 'Guanhua Chen', 'Majid Afshar']}, {'id': 'http://arxiv.org/abs/2508.05474v1', 'title': 'Can Large Language Models Generate Effective Datasets for Emotion   Recognition in Conversations?', 'published': '2025-08-07T15:13:55Z', 'updated': '2025-08-07T15:13:55Z', 'abstract': 'Emotion recognition in conversations (ERC) focuses on identifying emotion shifts within interactions, representing a significant step toward advancing machine intelligence. However, ERC data remains scarce, and existing datasets face numerous challenges due to their highly biased sources and the inherent subjectivity of soft labels. Even though Large Language Models (LLMs) have demonstrated their quality in many affective tasks, they are typically expensive to train, and their application to ERC tasks--particularly in data generation--remains limited. To address these challenges, we employ a small, resource-efficient, and general-purpose LLM to synthesize ERC datasets with diverse properties, supplementing the three most widely used ERC benchmarks. We generate six novel datasets, with two tailored to enhance each benchmark. We evaluate the utility of these datasets to (1) supplement existing datasets for ERC classification, and (2) analyze the effects of label imbalance in ERC. Our experimental results indicate that ERC classifier models trained on the generated datasets exhibit strong robustness and consistently achieve statistically significant performance improvements on existing ERC benchmarks.', 'authors': ['Burak Can Kaplan', 'Hugo Cesar De Castro Carneiro', 'Stefan Wermter']}, {'id': 'http://arxiv.org/abs/2508.05473v1', 'title': 'Embedding Alignment in Code Generation for Audio', 'published': '2025-08-07T15:13:42Z', 'updated': '2025-08-07T15:13:42Z', 'abstract': "LLM-powered code generation has the potential to revolutionize creative coding endeavors, such as live-coding, by enabling users to focus on structural motifs over syntactic details. In such domains, when prompting an LLM, users may benefit from considering multiple varied code candidates to better realize their musical intentions. Code generation models, however, struggle to present unique and diverse code candidates, with no direct insight into the code's audio output. To better establish a relationship between code candidates and produced audio, we investigate the topology of the mapping between code and audio embedding spaces. We find that code and audio embeddings do not exhibit a simple linear relationship, but supplement this with a constructed predictive model that shows an embedding alignment map could be learned. Supplementing the aim for musically diverse output, we present a model that given code predicts output audio embedding, constructing a code-audio embedding alignment map.", 'authors': ['Sam Kouteili', 'Hiren Madhu', 'George Typaldos', 'Mark Santolucito']}, {'id': 'http://arxiv.org/abs/2508.05470v1', 'title': 'Rethinking Creativity Evaluation: A Critical Analysis of Existing   Creativity Evaluations', 'published': '2025-08-07T15:11:48Z', 'updated': '2025-08-07T15:11:48Z', 'abstract': "We systematically examine, analyze, and compare representative creativity measures--creativity index, perplexity, syntactic templates, and LLM-as-a-Judge--across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity index's focus on lexical diversity, perplexity's sensitivity to model confidence, and syntactic templates' inability to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.", 'authors': ['Li-Chun Lu', 'Miri Liu', 'Pin-Chun Lu', 'Yufei Tian', 'Shao-Hua Sun', 'Nanyun Peng']}, {'id': 'http://arxiv.org/abs/2508.05469v1', 'title': "Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond   Vibes", 'published': '2025-08-07T15:11:43Z', 'updated': '2025-08-07T15:11:43Z', 'abstract': 'We develop mechanisms for evaluating AI systems without ground truth by exploiting a connection between gaming resistance and output quality. The data processing inequality ensures post-hoc attempts to game a metric degrades both information content and task performance. We prove that f-mutual information measures are the unique gaming resistant mechanisms under natural conditions, with the overseer acting as an agent. While Shannon mutual information faces exponential sample complexity, bounded measures like total variation distance remain tractable. Empirically, across ten domains from translation to peer review, all information-theoretic mechanisms achieve perfect discrimination (d > 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit systematic evaluation inversion, preferring fabricated content over accurate summaries. Our mechanisms show 10-100x better robustness to adversarial manipulation than current practices. We also find performance follows an inverted-U curve with compression ratio, peaking at 10:1 where agent responses exhibit optimal information diversity (3 effective dimensions), giving a bias-variance perspective on when our approach is expected to be most effective.', 'authors': ['Zachary Robertson', 'Sanmi Koyejo']}, {'id': 'http://arxiv.org/abs/2508.05468v1', 'title': 'TASE: Token Awareness and Structured Evaluation for Multilingual   Language Models', 'published': '2025-08-07T15:11:17Z', 'updated': '2025-08-07T15:11:17Z', 'abstract': "While large language models (LLMs) have demonstrated remarkable performance on high-level semantic tasks, they often struggle with fine-grained, token-level understanding and structural reasoning--capabilities that are essential for applications requiring precision and control. We introduce TASE, a comprehensive benchmark designed to evaluate LLMs' ability to perceive and reason about token-level information across languages. TASE covers 10 tasks under two core categories: token awareness and structural understanding, spanning Chinese, English, and Korean, with a 35,927-instance evaluation set and a scalable synthetic data generation pipeline for training. Tasks include character counting, token alignment, syntactic structure parsing, and length constraint satisfaction. We evaluate over 30 leading commercial and open-source LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a custom Qwen2.5-14B model using the GRPO training method. Results show that human performance significantly outpaces current LLMs, revealing persistent weaknesses in token-level reasoning. TASE sheds light on these limitations and provides a new diagnostic lens for future improvements in low-level language understanding and cross-lingual generalization. Our code and dataset are publicly available at https://github.com/cyzcz/Tase .", 'authors': ['Chenzhuo Zhao', 'Xinda Wang', 'Yue Huang', 'Junting Lu', 'Ziqian Liu']}, {'id': 'http://arxiv.org/abs/2508.05464v1', 'title': 'Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?', 'published': '2025-08-07T15:03:39Z', 'updated': '2025-08-07T15:03:39Z', 'abstract': 'The rapid advancement of General Purpose AI (GPAI) models necessitates robust evaluation frameworks, especially with emerging regulations like the EU AI Act and its associated Code of Practice (CoP). Current AI evaluation practices depend heavily on established benchmarks, but these tools were not designed to measure the systemic risks that are the focus of the new regulatory landscape. This research addresses the urgent need to quantify this "benchmark-regulation gap." We introduce Bench-2-CoP, a novel, systematic framework that uses validated LLM-as-judge analysis to map the coverage of 194,955 questions from widely-used benchmarks against the EU AI Act\'s taxonomy of model capabilities and propensities. Our findings reveal a profound misalignment: the evaluation ecosystem is overwhelmingly focused on a narrow set of behavioral propensities, such as "Tendency to hallucinate" (53.7% of the corpus) and "Discriminatory bias" (28.9%), while critical functional capabilities are dangerously neglected. Crucially, capabilities central to loss-of-control scenarios, including evading human oversight, self-replication, and autonomous AI development, receive zero coverage in the entire benchmark corpus. This translates to a near-total evaluation gap for systemic risks like "Loss of Control" (0.4% coverage) and "Cyber Offence" (0.8% coverage). This study provides the first comprehensive, quantitative analysis of this gap, offering critical insights for policymakers to refine the CoP and for developers to build the next generation of evaluation tools, ultimately fostering safer and more compliant AI.', 'authors': ['Matteo Prandi', 'Vincenzo Suriani', 'Federico Pierucci', 'Marcello Galisai', 'Daniele Nardi', 'Piercosma Bisconti']}, {'id': 'http://arxiv.org/abs/2508.05452v1', 'title': 'LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair   Evaluation of Large Language Models', 'published': '2025-08-07T14:46:30Z', 'updated': '2025-08-07T14:46:30Z', 'abstract': 'Existing evaluation of Large Language Models (LLMs) on static benchmarks is vulnerable to data contamination and leaderboard overfitting, critical issues that obscure true model capabilities. To address this, we introduce LLMEval-3, a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary bank of 220k graduate-level questions, from which it dynamically samples unseen test sets for each evaluation run. Its automated pipeline ensures integrity via contamination-resistant data curation, a novel anti-cheating architecture, and a calibrated LLM-as-a-judge process achieving 90% agreement with human experts, complemented by a relative ranking system for fair comparison. An 20-month longitudinal study of nearly 50 leading models reveals a performance ceiling on knowledge memorization and exposes data contamination vulnerabilities undetectable by static benchmarks. The framework demonstrates exceptional robustness in ranking stability and consistency, providing strong empirical validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and credible methodology for assessing the true capabilities of LLMs beyond leaderboard scores, promoting the development of more trustworthy evaluation standards.', 'authors': ['Ming Zhang', 'Yujiong Shen', 'Jingyi Deng', 'Yuhui Wang', 'Yue Zhang', 'Junzhe Wang', 'Shichun Liu', 'Shihan Dou', 'Huayu Sha', 'Qiyuan Peng', 'Changhao Jiang', 'Jingqi Tong', 'Yilong Wu', 'Zhihao Zhang', 'Mingqi Wu', 'Zhiheng Xi', 'Mingxu Chai', 'Tao Liang', 'Zhihui Fei', 'Zhen Wang', 'Mingyang Wan', 'Guojun Ma', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']}, {'id': 'http://arxiv.org/abs/2508.05433v1', 'title': 'Discovering Interpretable Programmatic Policies via Multimodal   LLM-assisted Evolutionary Search', 'published': '2025-08-07T14:24:03Z', 'updated': '2025-08-07T14:24:03Z', 'abstract': 'Interpretability and high performance are essential goals in designing control policies, particularly for safety-critical tasks. Deep reinforcement learning has greatly enhanced performance, yet its inherent lack of interpretability often undermines trust and hinders real-world deployment. This work addresses these dual challenges by introducing a novel approach for programmatic policy discovery, called Multimodal Large Language Model-assisted Evolutionary Search (MLES). MLES utilizes multimodal large language models as policy generators, combining them with evolutionary mechanisms for automatic policy optimization. It integrates visual feedback-driven behavior analysis within the policy generation process to identify failure patterns and facilitate targeted improvements, enhancing the efficiency of policy discovery and producing adaptable, human-aligned policies. Experimental results show that MLES achieves policy discovery capabilities and efficiency comparable to Proximal Policy Optimization (PPO) across two control tasks, while offering transparent control logic and traceable design processes. This paradigm overcomes the limitations of predefined domain-specific languages, facilitates knowledge transfer and reuse, and is scalable across various control tasks. MLES shows promise as a leading approach for the next generation of interpretable control policy discovery.', 'authors': ['Qinglong Hu', 'Xialiang Tong', 'Mingxuan Yuan', 'Fei Liu', 'Zhichao Lu', 'Qingfu Zhang']}, {'id': 'http://arxiv.org/abs/2508.05429v1', 'title': "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource   Language Constraints", 'published': '2025-08-07T14:17:43Z', 'updated': '2025-08-07T14:17:43Z', 'abstract': 'Large Language Models (LLMs) often exhibit cultural biases due to training data dominated by high-resource languages like English and Chinese. This poses challenges for accurately representing and evaluating diverse cultural contexts, particularly in low-resource language settings. To address this, we introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on Malaysian culture across six pillars: arts, attire, customs, entertainment, food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks, MyCulture employs a novel open-ended multiple-choice question format without predefined options, thereby reducing guessing and mitigating format bias. We provide a theoretical justification for the effectiveness of this open-ended structure in improving both fairness and discriminative power. Furthermore, we analyze structural bias by comparing model performance on structured versus free-form outputs, and assess language bias through multilingual prompt variations. Our evaluation across a range of regional and international LLMs reveals significant disparities in cultural comprehension, highlighting the urgent need for culturally grounded and linguistically inclusive benchmarks in the development and assessment of LLMs.', 'authors': ['Zhong Ken Hew', 'Jia Xin Low', 'Sze Jue Yang', 'Chee Seng chan']}, {'id': 'http://arxiv.org/abs/2508.05428v1', 'title': 'Group Causal Policy Optimization for Post-Training Large Language Models', 'published': '2025-08-07T14:17:28Z', 'updated': '2025-08-07T14:17:28Z', 'abstract': 'Recent advances in large language models (LLMs) have broadened their applicability across diverse tasks, yet specialized domains still require targeted post training. Among existing methods, Group Relative Policy Optimization (GRPO) stands out for its efficiency, leveraging groupwise relative rewards while avoiding costly value function learning. However, GRPO treats candidate responses as independent, overlooking semantic interactions such as complementarity and contradiction. To address this challenge, we first introduce a Structural Causal Model (SCM) that reveals hidden dependencies among candidate responses induced by conditioning on a final integrated output forming a collider structure. Then, our causal analysis leads to two insights: (1) projecting responses onto a causally informed subspace improves prediction quality, and (2) this projection yields a better baseline than query only conditioning. Building on these insights, we propose Group Causal Policy Optimization (GCPO), which integrates causal structure into optimization through two key components: a causally informed reward adjustment and a novel KL regularization term that aligns the policy with a causally projected reference distribution. Comprehensive experimental evaluations demonstrate that GCPO consistently surpasses existing methods, including GRPO across multiple reasoning benchmarks.', 'authors': ['Ziyin Gu', 'Jingyao Wang', 'Ran Zuo', 'Chuxiong Sun', 'Zeen Song', 'Changwen Zheng', 'Wenwen Qiang']}, {'id': 'http://arxiv.org/abs/2508.05427v1', 'title': 'Large Language Models Transform Organic Synthesis From Reaction   Prediction to Automation', 'published': '2025-08-07T14:17:23Z', 'updated': '2025-08-07T14:17:23Z', 'abstract': 'Large language models (LLMs) are beginning to reshape how chemists plan and run reactions in organic synthesis. Trained on millions of reported transformations, these text-based models can propose synthetic routes, forecast reaction outcomes and even instruct robots that execute experiments without human supervision. Here we survey the milestones that turned LLMs from speculative tools into practical lab partners. We show how coupling LLMs with graph neural networks, quantum calculations and real-time spectroscopy shrinks discovery cycles and supports greener, data-driven chemistry. We discuss limitations, including biased datasets, opaque reasoning and the need for safety gates that prevent unintentional hazards. Finally, we outline community initiatives open benchmarks, federated learning and explainable interfaces that aim to democratize access while keeping humans firmly in control. These advances chart a path towards rapid, reliable and inclusive molecular innovation powered by artificial intelligence and automation.', 'authors': ['Kartar Kumar Lohana Tharwani', 'Rajesh Kumar', ' Sumita', 'Numan Ahmed', 'Yong Tang']}, {'id': 'http://arxiv.org/abs/2508.05421v1', 'title': 'LLM-based Multi-Agent Copilot for Quantum Sensor', 'published': '2025-08-07T14:14:08Z', 'updated': '2025-08-07T14:14:08Z', 'abstract': 'Large language models (LLM) exhibit broad utility but face limitations in quantum sensor development, stemming from interdisciplinary knowledge barriers and involving complex optimization processes. Here we present QCopilot, an LLM-based multi-agent framework integrating external knowledge access, active learning, and uncertainty quantification for quantum sensor design and diagnosis. Comprising commercial LLMs with few-shot prompt engineering and vector knowledge base, QCopilot employs specialized agents to adaptively select optimization methods, automate modeling analysis, and independently perform problem diagnosis. Applying QCopilot to atom cooling experiments, we generated 10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a few hours, representing $\\sim$100$\\times$ speedup over manual experimentation. Notably, by continuously accumulating prior knowledge and enabling dynamic modeling, QCopilot can autonomously identify anomalous parameters in multi-parameter experimental settings. Our work reduces barriers to large-scale quantum sensor deployment and readily extends to other quantum information systems.', 'authors': ['Rong Sha', 'Binglin Wang', 'Jun Yang', 'Xiaoxiao Ma', 'Chengkun Wu', 'Liang Yan', 'Chao Zhou', 'Jixun Liu', 'Guochao Wang', 'Shuhua Yan', 'Lingxiao Zhu']}, {'id': 'http://arxiv.org/abs/2508.05387v1', 'title': 'Echo: Decoupling Inference and Training for Large-Scale RL Alignment on   Heterogeneous Swarms', 'published': '2025-08-07T13:37:04Z', 'updated': '2025-08-07T13:37:04Z', 'abstract': 'Modern RL-based post-training for large language models (LLMs) co-locate trajectory sampling and policy optimisation on the same GPU cluster, forcing the system to switch between inference and training workloads. This serial context switching violates the single-program-multiple-data (SPMD) assumption underlying today\'s distributed training systems. We present Echo, the RL system that cleanly decouples these two phases across heterogeneous "inference" and "training" swarms while preserving statistical efficiency. Echo introduces two lightweight synchronization protocols: a sequential pull mode that refreshes sampler weights on every API call for minimal bias, and an asynchronous push-pull mode that streams version-tagged rollouts through a replay buffer to maximise hardware utilisation. Training three representative RL workloads with Qwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster, Echo matches a fully co-located Verl baseline in convergence speed and final reward while off-loading trajectory generation to commodity edge hardware. These promising results demonstrate that large-scale RL for LLMs could achieve datacentre-grade performance using decentralised, heterogeneous resources.', 'authors': ['Jie Xiao', 'Shaoduo Gan', 'Changyuan Fan', 'Qingnan Ren', 'Alfred Long', 'Yuchen Zhang', 'Rymon Yu', 'Eric Yang', 'Lynn Ai']}, {'id': 'http://arxiv.org/abs/2508.05370v1', 'title': 'Simulating LLM training workloads for heterogeneous compute and network   infrastructure', 'published': '2025-08-07T13:15:59Z', 'updated': '2025-08-07T13:15:59Z', 'abstract': 'The growing demand for large-scale GPU clusters in distributed model training presents a significant barrier to innovation, particularly in model optimization, performance tuning, and system-level enhancements. To address this challenge, LLM training simulators are employed to estimate training time and guide design decisions. However, the state-of-the-art LLM training simulators assume homogeneous compute and network infrastructure. In practice, device heterogeneity is inevitable due to resource sharing in cloud environments, frequent shifts in device generations, and inherent intra-chip interconnect heterogeneity. To address the gap between state-of-the-art and practical requirements, we propose the design of a heterogeneity-aware distributed LLM simulator capable of predicting training time while enabling abstractions to specify custom configurations for device groups and device-to-parallelism mapping. We present the design requirements and challenges in building a heterogeneity-aware distributed ML training simulator, and design components such as non-uniform workload partitioning. Our initial simulation results demonstrate the impact of heterogeneity on the model computation and communication time.', 'authors': ['Sumit Kumar', 'Arjun Temura', 'Naman Sharma', 'Ramanjeet Singh', 'Meet Dadhania', 'Praveen Tammana', 'Satananda Burla', 'Abed Mohammad Kamaluddin', 'Rinku Shah']}, {'id': 'http://arxiv.org/abs/2508.05366v1', 'title': 'Can Language Models Critique Themselves? Investigating Self-Feedback for   Retrieval Augmented Generation at BioASQ 2025', 'published': '2025-08-07T13:13:19Z', 'updated': '2025-08-07T13:13:19Z', 'abstract': "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim to enable autonomous search processes where Large Language Models (LLMs) iteratively refine outputs. However, applying these systems to domain-specific professional search, such as biomedical research, presents challenges, as automated systems may reduce user involvement and misalign with expert information needs. Professional search tasks often demand high levels of user expertise and transparency. The BioASQ CLEF 2025 challenge, using expert-formulated questions, can serve as a platform to study these issues. We explored the performance of current reasoning and nonreasoning LLMs like Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our methodology was a self-feedback mechanism where LLMs generated, evaluated, and then refined their outputs for query expansion and for multiple answer types (yes/no, factoid, list, ideal). We investigated whether this iterative self-correction improves performance and if reasoning models are more capable of generating useful feedback. Preliminary results indicate varied performance for the self-feedback strategy across models and tasks. This work offers insights into LLM self-correction and informs future work on comparing the effectiveness of LLM-generated feedback with direct human expert input in these search systems.", 'authors': ['Samy Ateia', 'Udo Kruschwitz']}, {'id': 'http://arxiv.org/abs/2508.05344v1', 'title': 'NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During   Collaborative Law-Making', 'published': '2025-08-07T12:49:44Z', 'updated': '2025-08-07T12:49:44Z', 'abstract': 'Recent advancements in large language models (LLMs) have extended their capabilities from basic text processing to complex reasoning tasks, including legal interpretation, argumentation, and strategic interaction. However, empirical understanding of LLM behavior in open-ended, multi-agent settings especially those involving deliberation over legal and ethical dilemmas remains limited. We introduce NomicLaw, a structured multi-agent simulation where LLMs engage in collaborative law-making, responding to complex legal vignettes by proposing rules, justifying them, and voting on peer proposals. We quantitatively measure trust and reciprocity via voting patterns and qualitatively assess how agents use strategic language to justify proposals and influence outcomes. Experiments involving homogeneous and heterogeneous LLM groups demonstrate how agents spontaneously form alliances, betray trust, and adapt their rhetoric to shape collective decisions. Our results highlight the latent social reasoning and persuasive capabilities of ten open-source LLMs and provide insights into the design of future AI systems capable of autonomous negotiation, coordination and drafting legislation in legal settings.', 'authors': ['Asutosh Hota', 'Jussi P. P. Jokinen']}, {'id': 'http://arxiv.org/abs/2508.05342v1', 'title': 'Information-Theoretic Graph Fusion with Vision-Language-Action Model for   Policy Reasoning and Dual Robotic Control', 'published': '2025-08-07T12:48:09Z', 'updated': '2025-08-07T12:48:09Z', 'abstract': 'Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.', 'authors': ['Shunlei Li', 'Longsen Gao', 'Jin Wang', 'Chang Che', 'Xi Xiao', 'Jiuwen Cao', 'Yingbai Hu', 'Hamid Reza Karimi']}, {'id': 'http://arxiv.org/abs/2508.05311v1', 'title': 'A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM   Agents', 'published': '2025-08-07T12:11:53Z', 'updated': '2025-08-07T12:11:53Z', 'abstract': 'We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework. Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system. Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and mediates communication across agents and external tools, enabling reasoning over both structured and unstructured inputs.   The system achieves strong performance on reasoning benchmarks. On \\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through logic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in multistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it boosts abstraction accuracy by +6.0\\% through integration of symbolic oracles. Applications in clinical decision support and scientific discovery show how the system encodes domain rules symbolically while leveraging LLMs for contextual inference and hypothesis generation. This architecture offers a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning.', 'authors': ['Andrew Kiruluta']}, {'id': 'http://arxiv.org/abs/2508.05305v1', 'title': 'SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings   and Speaks in Tokens', 'published': '2025-08-07T12:03:44Z', 'updated': '2025-08-07T12:03:44Z', 'abstract': 'The recently proposed Large Concept Model (LCM) generates text by predicting a sequence of sentence-level embeddings and training with either mean-squared error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer that "thinks" in the same continuous SONAR embedding space, yet is supervised through token-level cross-entropy propagated via the frozen SONAR decoder. This hybrid objective retains the semantic abstraction of LCM while eliminating its diffusion sampler and restoring a likelihood-based training signal. Across model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive generation quality. We report scaling trends, ablations, benchmark results, and release the complete training code and all pretrained checkpoints to foster reproducibility and future research.', 'authors': ['Nikita Dragunov', 'Temurbek Rahmatullaev', 'Elizaveta Goncharova', 'Andrey Kuznetsov', 'Anton Razzhigaev']}, {'id': 'http://arxiv.org/abs/2508.05299v1', 'title': 'VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing   Projection Test', 'published': '2025-08-07T11:59:50Z', 'updated': '2025-08-07T11:59:50Z', 'abstract': 'The Drawing Projection Test (DPT) is an essential tool in art therapy, allowing psychologists to assess participants\' mental states through their sketches. Specifically, through sketches with the theme of "a person picking an apple from a tree (PPAT)", it can be revealed whether the participants are in mental states such as depression. Compared with scales, the DPT can enrich psychologists\' understanding of an individual\'s mental state. However, the interpretation of the PPAT is laborious and depends on the experience of the psychologists. To address this issue, we propose an effective identification method to support psychologists in conducting a large-scale automatic DPT. Unlike traditional sketch recognition, DPT more focus on the overall evaluation of the sketches, such as color usage and space utilization. Moreover, PPAT imposes a time limit and prohibits verbal reminders, resulting in low drawing accuracy and a lack of detailed depiction. To address these challenges, we propose the following efforts: (1) Providing an experimental environment for automated analysis of PPAT sketches for depression assessment; (2) Offering a Visual-Semantic depression assessment based on LLM (VS-LLM) method; (3) Experimental results demonstrate that our method improves by 17.6% compared to the psychologist assessment method. We anticipate that this work will contribute to the research in mental state assessment based on PPAT sketches\' elements recognition. Our datasets and codes are available at https://github.com/wmeiqi/VS-LLM.', 'authors': ['Meiqi Wu', 'Yaxuan Kang', 'Xuchen Li', 'Shiyu Hu', 'Xiaotang Chen', 'Yunfeng Kang', 'Weiqiang Wang', 'Kaiqi Huang']}, {'id': 'http://arxiv.org/abs/2508.05298v1', 'title': 'GhostShell: Streaming LLM Function Calls for Concurrent Embodied   Programming', 'published': '2025-08-07T11:55:46Z', 'updated': '2025-08-07T11:55:46Z', 'abstract': 'We present GhostShell, a novel approach that leverages Large Language Models (LLMs) to enable streaming and concurrent behavioral programming for embodied systems. In contrast to conventional methods that rely on pre-scheduled action sequences or behavior trees, GhostShell drives embodied systems to act on-the-fly by issuing function calls incrementally as tokens are streamed from the LLM. GhostShell features a streaming XML function token parser, a dynamic function interface mapper, and a multi-channel scheduler that orchestrates intra-channel synchronous and inter-channel asynchronous function calls, thereby coordinating serial-parallel embodied actions across multiple robotic components as directed by the LLM. We evaluate GhostShell on our robot prototype COCO through comprehensive grounded experiments across 34 real-world interaction tasks and multiple LLMs. The results demonstrate that our approach achieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4 Sonnet and up to 66X faster response times compared to LLM native function calling APIs. GhostShell also proves effective in long-horizon multimodal tasks, demonstrating strong robustness and generalization.', 'authors': ['Jian Gong', 'Youwei Huang', 'Bo Yuan', 'Ming Zhu', 'Juncheng Zhan', 'Jinke Wang', 'Hang Shu', 'Mingyue Xiong', 'Yanjun Ye', 'Yufan Zu', 'Yang Zhou', 'Yihan Ding', 'Xuannian Chen', 'Xingyu Lu', 'Runjie Ban', 'Bingchao Huang', 'Fusen Liu']}, {'id': 'http://arxiv.org/abs/2508.05294v1', 'title': 'Towards Embodied Agentic AI: Review and Classification of LLM- and   VLM-Driven Robot Autonomy and Interaction', 'published': '2025-08-07T11:48:03Z', 'updated': '2025-08-07T11:48:03Z', 'abstract': "Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (BLMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those words advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.", 'authors': ['Sahar Salimpour', 'Lei Fu', 'Farhad Keramat', 'Leonardo Militano', 'Giovanni Toffetti', 'Harry Edelman', 'Jorge Peña Queralta']}, {'id': 'http://arxiv.org/abs/2508.05289v1', 'title': 'RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in   Conversational Recommenders', 'published': '2025-08-07T11:36:55Z', 'updated': '2025-08-07T11:36:55Z', 'abstract': 'Conversational recommender systems (CRS) based on Large Language Models (LLMs) need to constantly be aligned to the user preferences to provide satisfying and context-relevant item recommendations. The traditional supervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell time, sentiment polarity, or engagement patterns. In this paper, we share a fine-tuning solution using human feedback reinforcement learning (RLHF) to maximize implied user feedback (IUF) in a multi-turn recommendation context. We specify a reward model $R_{\\phi}$ learnt on weakly-labelled engagement information and maximize user-centric utility by optimizing the foundational LLM M_{\\theta} through a proximal policy optimization (PPO) approach. The architecture models conversational state transitions $s_t \\to a_t \\to s_{t +1}$, where the action $a_t$ is associated with LLM-generated item suggestions only on condition of conversation history in the past. The evaluation across synthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that our RLHF-fine-tuned models can perform better in terms of top-$k$ recommendation accuracy, coherence, and user satisfaction compared to (arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give up This paper shows that implicit signal alignment can be efficient in achieving scalable and user-adaptive design of CRS.', 'authors': ['Zhongheng Yang', 'Aijia Sun', 'Yushang Zhao', 'Yinuo Yang', 'Dannier Li', 'Chengrui Zhou']}, {'id': 'http://arxiv.org/abs/2508.05283v1', 'title': 'Decision-Making with Deliberation: Meta-reviewing as a Document-grounded   Dialogue', 'published': '2025-08-07T11:27:43Z', 'updated': '2025-08-07T11:27:43Z', 'abstract': 'Meta-reviewing is a pivotal stage in the peer-review process, serving as the final step in determining whether a paper is recommended for acceptance. Prior research on meta-reviewing has treated this as a summarization problem over review reports. However, complementary to this perspective, meta-reviewing is a decision-making process that requires weighing reviewer arguments and placing them within a broader context. Prior research has demonstrated that decision-makers can be effectively assisted in such scenarios via dialogue agents. In line with this framing, we explore the practical challenges for realizing dialog agents that can effectively assist meta-reviewers. Concretely, we first address the issue of data scarcity for training dialogue agents by generating synthetic data using Large Language Models (LLMs) based on a self-refinement strategy to improve the relevance of these dialogues to expert domains. Our experiments demonstrate that this method produces higher-quality synthetic data and can serve as a valuable resource towards training meta-reviewing assistants. Subsequently, we utilize this data to train dialogue agents tailored for meta-reviewing and find that these agents outperform \\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our agents in real-world meta-reviewing scenarios and confirm their effectiveness in enhancing the efficiency of meta-reviewing.\\footnote{Code and Data: https://github.com/UKPLab/arxiv2025-meta-review-as-dialog', 'authors': ['Sukannya Purkayastha', 'Nils Dycke', 'Anne Lauscher', 'Iryna Gurevych']}, {'id': 'http://arxiv.org/abs/2508.05282v1', 'title': 'ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for   Late-Stage Fragility in LLMs', 'published': '2025-08-07T11:26:40Z', 'updated': '2025-08-07T11:26:40Z', 'abstract': 'Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Large Language Models (LLMs), yet the reliability of these reasoning chains remains a critical challenge. A widely held "cascading failure" hypothesis suggests that errors are most detrimental when they occur early in the reasoning process. This paper challenges that assumption through systematic error-injection experiments, revealing a counter-intuitive phenomenon we term "Late-Stage Fragility": errors introduced in the later stages of a CoT chain are significantly more likely to corrupt the final answer than identical errors made at the beginning. To address this specific vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought (ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive Verification Manager (AVM) operates first, followed by the Multi-Perspective Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score function I(k) that assigns different weights based on the position within the reasoning chains, addressing the Late-Stage Fragility issue by identifying and prioritizing high-risk, late-stage steps. Once these critical steps are identified, the MSCE applies robust, dual-path correction specifically to the failure parts. Extensive experiments on benchmarks such as GSM8K and MATH demonstrate that ASCoT achieves outstanding accuracy, outperforming strong baselines, including standard CoT. Our work underscores the importance of diagnosing specific failure modes in LLM reasoning and advocates for a shift from uniform verification strategies to adaptive, vulnerability-aware correction mechanisms.', 'authors': ['Dongxu Zhang', 'Ning Yang', 'Jihua Zhu', 'Jinnan Yang', 'Miao Xin', 'Baoliang Tian']}, {'id': 'http://arxiv.org/abs/2508.05269v1', 'title': 'B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding', 'published': '2025-08-07T11:11:56Z', 'updated': '2025-08-07T11:11:56Z', 'abstract': 'Understanding dynamic outdoor environments requires capturing complex object interactions and their evolution over time. LiDAR-based 4D point clouds provide precise spatial geometry and rich temporal cues, making them ideal for representing real-world scenes. However, despite their potential, 4D LiDAR remains underexplored in the context of Multimodal Large Language Models (MLLMs) due to the absence of high-quality, modality-specific annotations and the lack of MLLM architectures capable of processing its high-dimensional composition. To address these challenges, we introduce B4DL, a new benchmark specifically designed for training and evaluating MLLMs on 4D LiDAR understanding. In addition, we propose a scalable data generation pipeline and an MLLM model that, for the first time, directly processes raw 4D LiDAR by bridging it with language understanding. Combined with our dataset and benchmark, our model offers a unified solution for spatio-temporal reasoning in dynamic outdoor environments. We provide rendered 4D LiDAR videos, generated dataset, and inference outputs on diverse scenarios at: https://mmb4dl.github.io/mmb4dl/', 'authors': ['Changho Choi', 'Youngwoo Shin', 'Gyojin Han', 'Dong-Jae Lee', 'Junmo Kim']}, {'id': 'http://arxiv.org/abs/2508.05267v1', 'title': 'An Explainable Natural Language Framework for Identifying and Notifying   Target Audiences In Enterprise Communication', 'published': '2025-08-07T11:02:40Z', 'updated': '2025-08-07T11:02:40Z', 'abstract': 'In large-scale maintenance organizations, identifying subject matter experts and managing communications across complex entities relationships poses significant challenges -- including information overload and longer response times -- that traditional communication approaches fail to address effectively. We propose a novel framework that combines RDF graph databases with LLMs to process natural language queries for precise audience targeting, while providing transparent reasoning through a planning-orchestration architecture. Our solution enables communication owners to formulate intuitive queries combining concepts such as equipment, manufacturers, maintenance engineers, and facilities, delivering explainable results that maintain trust in the system while improving communication efficiency across the organization.', 'authors': ['Vítor N. Lourenço', 'Mohnish Dubey', 'Yunfei Bai', 'Audrey Depeige', 'Vivek Jain']}, {'id': 'http://arxiv.org/abs/2508.05266v1', 'title': 'Understanding and Mitigating Errors of LLM-Generated RTL Code', 'published': '2025-08-07T11:02:32Z', 'updated': '2025-08-07T11:02:32Z', 'abstract': 'Despite the promising potential of large language model (LLM) based register-transfer-level (RTL) code generation, the overall success rate remains unsatisfactory. Errors arise from various factors, with limited understanding of specific failure causes hindering improvement. To address this, we conduct a comprehensive error analysis and manual categorization. Our findings reveal that most errors stem not from LLM reasoning limitations, but from insufficient RTL programming knowledge, poor understanding of circuit concepts, ambiguous design descriptions, or misinterpretation of complex multimodal inputs. Leveraging in-context learning, we propose targeted error correction techniques. Specifically, we construct a domain-specific knowledge base and employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge. To mitigate ambiguity errors, we introduce design description rules and implement a rule-checking mechanism. For multimodal misinterpretation, we integrate external tools to convert inputs into LLM-compatible meta-formats. For remaining errors, we adopt an iterative debugging loop (simulation-error localization-correction). Integrating these techniques into an LLM-based framework significantly improves performance. We incorporate these error correction techniques into a foundational LLM-based RTL code generation framework, resulting in significantly improved performance. Experimental results show that our enhanced framework achieves 91.0\\% accuracy on the VerilogEval benchmark, surpassing the baseline code generation approach by 32.7\\%, demonstrating the effectiveness of our methods.', 'authors': ['Jiazheng Zhang', 'Cheng Liu', 'Huawei Li']}, {'id': 'http://arxiv.org/abs/2508.05257v1', 'title': 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs', 'published': '2025-08-07T10:48:24Z', 'updated': '2025-08-07T10:48:24Z', 'abstract': 'The Mixture-of-Experts (MoE) architecture has become a predominant paradigm for scaling large language models (LLMs). Despite offering strong performance and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and Kimi-K2-Instruct present serious challenges due to substantial memory requirements in deployment. While recent works have explored MoE compression to address this issue, existing methods often suffer from considerable accuracy drops (e.g., 7-14% relatively) even at modest compression rates. This paper introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model compression while incurring minimal accuracy drops. Specifically, each up/gate matrix in an expert is decomposed via a rank decomposition as W = AB, where matrix A is unique to each expert. The relatively larger matrix B is further re-parameterized as a linear combination of basis matrices {Bi} shared across all experts within a given MoE layer. The factorization is learned by minimizing the reconstruction error relative to the original weight matrices. Experiments demonstrate that MoBE achieves notably lower accuracy drops compared to prior works. For instance, MoBE can reduce the parameter counts of Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by 24%-30% with only 1%-2% accuracy drop (about 2% drops when measured relatively).', 'authors': ['Xiaodong Chen', 'Mingming Ha', 'Zhenzhong Lan', 'Jing Zhang', 'Jianguo Li']}, {'id': 'http://arxiv.org/abs/2508.05242v1', 'title': 'CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets   with RL', 'published': '2025-08-07T10:31:24Z', 'updated': '2025-08-07T10:31:24Z', 'abstract': 'Code large language models (LLMs) have become indispensable tools for building efficient and automated coding pipelines. Existing models are typically post-trained using reinforcement learning (RL) from general-purpose LLMs using "human instruction-final answer" pairs, where the instructions are usually from manual annotations. However, collecting high-quality coding instructions is both labor-intensive and difficult to scale. On the other hand, code snippets are abundantly available from various sources. This imbalance presents a major bottleneck in instruction-based post-training. We propose CodeBoost, a post-training framework that enhances code LLMs purely from code snippets, without relying on human-annotated instructions. CodeBoost introduces the following key components: (1) maximum-clique curation, which selects a representative and diverse training corpus from code; (2) bi-directional prediction, which enables the model to learn from both forward and backward prediction objectives; (3) error-aware prediction, which incorporates learning signals from both correct and incorrect outputs; (4) heterogeneous augmentation, which diversifies the training distribution to enrich code semantics; and (5) heterogeneous rewarding, which guides model learning through multiple reward types including format correctness and execution feedback from both successes and failures. Extensive experiments across several code LLMs and benchmarks verify that CodeBoost consistently improves performance, demonstrating its effectiveness as a scalable and effective training pipeline.', 'authors': ['Sijie Wang', 'Quanjiang Guo', 'Kai Zhao', 'Yawei Zhang', 'Xin Li', 'Xiang Li', 'Siqi Li', 'Rui She', 'Shangshu Yu', 'Wee Peng Tay']}, {'id': 'http://arxiv.org/abs/2508.05239v1', 'title': 'Pruning Large Language Models by Identifying and Preserving Functional   Networks', 'published': '2025-08-07T10:27:01Z', 'updated': '2025-08-07T10:27:01Z', 'abstract': 'Structured pruning is one of the representative techniques for compressing large language models (LLMs) to reduce GPU memory consumption and accelerate inference speed. It offers significant practical value in improving the efficiency of LLMs in real-world applications. Current structured pruning methods typically rely on assessment of the importance of the structure units and pruning the units with less importance. Most of them overlooks the interaction and collaboration among artificial neurons that are crucial for the functionalities of LLMs, leading to a disruption in the macro functional architecture of LLMs and consequently a pruning performance degradation. Inspired by the inherent similarities between artificial neural networks and functional neural networks in the human brain, we alleviate this challenge and propose to prune LLMs by identifying and preserving functional networks within LLMs in this study. To achieve this, we treat an LLM as a digital brain and decompose the LLM into functional networks, analogous to identifying functional brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving the key neurons within these functional networks. Experimental results demonstrate that the proposed method can successfully identify and locate functional networks and key neurons in LLMs, enabling efficient model pruning. Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.', 'authors': ['Yiheng Liu', 'Junhao Ning', 'Sichen Xia', 'Xiaohui Gao', 'Ning Qiang', 'Bao Ge', 'Junwei Han', 'Xintao Hu']}, {'id': 'http://arxiv.org/abs/2508.05238v1', 'title': 'Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using   Large Language Models', 'published': '2025-08-07T10:26:28Z', 'updated': '2025-08-07T10:26:28Z', 'abstract': 'Level 3 automated driving systems allows drivers to engage in secondary tasks while diminishing their perception of risk. In the event of an emergency necessitating driver intervention, the system will alert the driver with a limited window for reaction and imposing a substantial cognitive burden. To address this challenge, this study employs a Large Language Model (LLM) to assist drivers in maintaining an appropriate attention on road conditions through a "humanized" persuasive advice. Our tool leverages the road conditions encountered by Level 3 systems as triggers, proactively steering driver behavior via both visual and auditory routes. Empirical study indicates that our tool is effective in sustaining driver attention with reduced cognitive load and coordinating secondary tasks with takeover behavior. Our work provides insights into the potential of using LLMs to support drivers during multi-task automated driving.', 'authors': ['Wei Xiang', 'Muchen Li', 'Jie Yan', 'Manling Zheng', 'Hanfei Zhu', 'Mengyun Jiang', 'Lingyun Sun']}, {'id': 'http://arxiv.org/abs/2508.05234v1', 'title': 'Resource-Limited Joint Multimodal Sentiment Reasoning and Classification   via Chain-of-Thought Enhancement and Distillation', 'published': '2025-08-07T10:23:14Z', 'updated': '2025-08-07T10:23:14Z', 'abstract': 'The surge in rich multimodal content on social media platforms has greatly advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs) further accelerating progress in this field. Current approaches primarily leverage the knowledge and reasoning capabilities of parameter-heavy (Multimodal) LLMs for sentiment classification, overlooking autonomous multimodal sentiment reasoning generation in resource-constrained environments. Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment Reasoning and Classification task, JMSRC, which simultaneously performs multimodal sentiment reasoning chain generation and sentiment classification only with a lightweight model. We propose a Multimodal Chain-of-Thought Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a "Teacher-Assistant-Student" distillation paradigm to address deployment constraints in resource-limited environments. We first leverage a high-performance Multimodal Large Language Model (MLLM) to generate the initial reasoning dataset and train a medium-sized assistant model with a multi-task learning mechanism. A lightweight student model is jointly trained to perform efficient multimodal sentiment reasoning generation and classification. Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B parameters achieves strong performance on JMSRC, while exhibiting robust generalization and enhanced interpretability.', 'authors': ['Haonan Shangguan', 'Xiaocui Yang', 'Shi Feng', 'Daling Wang', 'Yifei Zhang', 'Ge Yu']}, {'id': 'http://arxiv.org/abs/2508.05232v1', 'title': 'Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous   LLMs', 'published': '2025-08-07T10:21:08Z', 'updated': '2025-08-07T10:21:08Z', 'abstract': 'Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are tightly coupled with the base model architecture, which constrains their applicability across heterogeneous pretrained large language models (LLMs). To address this limitation, we introduce Cross-LoRA, a data-free framework for transferring LoRA modules between diverse base models without requiring additional training data. Cross-LoRA consists of two key components: (a) LoRA-Align, which performs subspace alignment between source and target base models through rank-truncated singular value decomposition (SVD) and Frobenius-optimal linear transformation, ensuring compatibility under dimension mismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project source LoRA weight updates into the target model parameter space. Both components are data-free, training-free, and enable lightweight adaptation on a commodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that Cross-LoRA achieves relative gains of up to 5.26% over base models. Across other commonsense reasoning benchmarks, Cross-LoRA maintains performance comparable to that of directly trained LoRA adapters.', 'authors': ['Feifan Xia', 'Mingyang Liao', 'Yuyang Fang', 'Defang Li', 'Yantong Xie', 'Weikang Li', 'Yang Li', 'Deguo Xia', 'Jizhou Huang']}, {'id': 'http://arxiv.org/abs/2508.05202v1', 'title': 'SPEX: A Vision-Language Model for Land Cover Extraction on Spectral   Remote Sensing Images', 'published': '2025-08-07T09:37:45Z', 'updated': '2025-08-07T09:37:45Z', 'abstract': 'Spectral information has long been recognized as a critical cue in remote sensing observations. Although numerous vision-language models have been developed for pixel-level interpretation, spectral information remains underutilized, resulting in suboptimal performance, particularly in multispectral scenarios. To address this limitation, we construct a vision-language instruction-following dataset named SPIE, which encodes spectral priors of land-cover objects into textual attributes recognizable by large language models (LLMs), based on classical spectral index computations. Leveraging this dataset, we propose SPEX, a multimodal LLM designed for instruction-driven land cover extraction. To this end, we introduce several carefully designed components and training strategies, including multiscale feature aggregation, token context condensation, and multispectral visual pre-training, to achieve precise and flexible pixel-level interpretation. To the best of our knowledge, SPEX is the first multimodal vision-language model dedicated to land cover extraction in spectral remote sensing imagery. Extensive experiments on five public multispectral datasets demonstrate that SPEX consistently outperforms existing state-of-the-art methods in extracting typical land cover categories such as vegetation, buildings, and water bodies. Moreover, SPEX is capable of generating textual explanations for its predictions, thereby enhancing interpretability and user-friendliness. Code will be released at: https://github.com/MiliLab/SPEX.', 'authors': ['Dongchen Si', 'Di Wang', 'Erzhong Gao', 'Xiaolei Qin', 'Liu Zhao', 'Jing Zhang', 'Minqiang Xu', 'Jianbo Zhan', 'Jianshe Wang', 'Lin Liu', 'Bo Du', 'Liangpei Zhang']}, {'id': 'http://arxiv.org/abs/2508.05201v1', 'title': 'FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in   finance', 'published': '2025-08-07T09:37:14Z', 'updated': '2025-08-07T09:37:14Z', 'abstract': 'Hallucination remains a critical challenge for deploying Large Language Models (LLMs) in finance. Accurate extraction and precise calculation from tabular data are essential for reliable financial analysis, since even minor numerical errors can undermine decision-making and regulatory compliance. Financial applications have unique requirements, often relying on context-dependent, numerical, and proprietary tabular data that existing hallucination benchmarks rarely capture. In this study, we develop a rigorous and scalable framework for evaluating intrinsic hallucinations in financial LLMs, conceptualized as a context-aware masked span prediction task over real-world financial documents. Our main contributions are: (1) a novel, automated dataset creation paradigm using a masking strategy; (2) a new hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a comprehensive evaluation of intrinsic hallucination patterns in state-of-the-art LLMs on financial tabular data. Our work provides a robust methodology for in-house LLM evaluation and serves as a critical step toward building more trustworthy and reliable financial Generative AI systems.', 'authors': ['Mengao Zhang', 'Jiayu Fu', 'Tanya Warrier', 'Yuwen Wang', 'Tianhui Tan', 'Ke-wei Huang']}, {'id': 'http://arxiv.org/abs/2508.05193v1', 'title': 'STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension   and Fine-Grained Execution Reasoning', 'published': '2025-08-07T09:28:43Z', 'updated': '2025-08-07T09:28:43Z', 'abstract': 'In recent years, large language models (LLMs) have made significant progress in code intelligence, yet systematically evaluating their code understanding and reasoning abilities remains challenging. Mainstream benchmarks such as HumanEval and MBPP primarily assess functional correctness, while reasoning benchmarks like CRUXEVAL are limited to single-function, low-complexity scenarios. As a result, advanced models achieve nearly saturated scores, limiting their discriminative power. To address this, we present STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex multi-function understanding and fine-grained execution reasoning. SX-Bench features tasks involving collaboration among multiple sub-functions (e.g., chained calls, nested loops), shifting evaluation towards overall control and data flow modeling. It defines "computation steps" as the minimal execution unit and requires models to predict the total number of steps in reasoning tasks, thereby assessing a model\'s in-depth understanding of dynamic execution beyond simple I/O matching. Evaluation on over 20 mainstream models (including 14 reasoning-enhanced models) demonstrates that SX-Bench is highly discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent accuracy on Hard-Reasoning tasks, much lower than its saturated scores on previous benchmarks, thereby revealing bottlenecks in complex and fine-grained reasoning. We also release an automated pipeline combining program synthesis, symbolic execution, and LLM-aided validation for efficient benchmark generation and quality assurance. SX-Bench advances code evaluation from "single-function verification" to "multi-function dynamic reasoning," providing a key tool for the in-depth assessment of advanced code intelligence models.', 'authors': ['Kaiwen Yan', 'Yuhang Chang', 'Zirui Guo', 'Yaling Mou', 'Jiang Ming', 'Jingwei Sun']}, {'id': 'http://arxiv.org/abs/2508.05192v1', 'title': 'AI-assisted JSON Schema Creation and Mapping', 'published': '2025-08-07T09:27:10Z', 'updated': '2025-08-07T09:27:10Z', 'abstract': 'Model-Driven Engineering (MDE) places models at the core of system and data engineering processes. In the context of research data, these models are typically expressed as schemas that define the structure and semantics of datasets. However, many domains still lack standardized models, and creating them remains a significant barrier, especially for non-experts. We present a hybrid approach that combines large language models (LLMs) with deterministic techniques to enable JSON Schema creation, modification, and schema mapping based on natural language inputs by the user. These capabilities are integrated into the open-source tool MetaConfigurator, which already provides visual model editing, validation, code generation, and form generation from models. For data integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and YAML data using LLMs, while ensuring scalability and reliability through deterministic execution of generated mapping rules. The applicability of our work is demonstrated in an application example in the field of chemistry. By combining natural language interaction with deterministic safeguards, this work significantly lowers the barrier to structured data modeling and data integration for non-experts.', 'authors': ['Felix Neubauer', 'Jürgen Pleiss', 'Benjamin Uekermann']}, {'id': 'http://arxiv.org/abs/2508.05188v1', 'title': 'Incident Response Planning Using a Lightweight Large Language Model with   Reduced Hallucination', 'published': '2025-08-07T09:23:25Z', 'updated': '2025-08-07T09:23:25Z', 'abstract': 'Timely and effective incident response is key to managing the growing frequency of cyberattacks. However, identifying the right response actions for complex systems is a major technical challenge. A promising approach to mitigate this challenge is to use the security knowledge embedded in large language models (LLMs) to assist security operators during incident handling. Recent research has demonstrated the potential of this approach, but current methods are mainly based on prompt engineering of frontier LLMs, which is costly and prone to hallucinations. We address these limitations by presenting a novel way to use an LLM for incident response planning with reduced hallucination. Our method includes three steps: fine-tuning, information retrieval, and lookahead planning. We prove that our method generates response plans with a bounded probability of hallucination and that this probability can be made arbitrarily small at the expense of increased planning time under certain assumptions. Moreover, we show that our method is lightweight and can run on commodity hardware. We evaluate our method on logs from incidents reported in the literature. The experimental results show that our method a) achieves up to 22% shorter recovery times than frontier LLMs and b) generalizes to a broad range of incident types and response actions.', 'authors': ['Kim Hammar', 'Tansu Alpcan', 'Emil C. Lupu']}, {'id': 'http://arxiv.org/abs/2508.05179v1', 'title': 'ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in   Question Answering', 'published': '2025-08-07T09:15:15Z', 'updated': '2025-08-07T09:15:15Z', 'abstract': 'This paper presents the contributions of the ATLANTIS team to SemEval-2025 Task 3, focusing on detecting hallucinated text spans in question answering systems. Large Language Models (LLMs) have significantly advanced Natural Language Generation (NLG) but remain susceptible to hallucinations, generating incorrect or misleading content. To address this, we explored methods both with and without external context, utilizing few-shot prompting with a LLM, token-level classification or LLM fine-tuned on synthetic data. Notably, our approaches achieved top rankings in Spanish and competitive placements in English and German. This work highlights the importance of integrating relevant context to mitigate hallucinations and demonstrate the potential of fine-tuned models and prompt engineering.', 'authors': ['Catherine Kobus', 'François Lancelot', 'Marion-Cécile Martin', 'Nawal Ould Amer']}, {'id': 'http://arxiv.org/abs/2508.05170v1', 'title': 'Posterior-GRPO: Rewarding Reasoning Processes in Code Generation', 'published': '2025-08-07T09:04:10Z', 'updated': '2025-08-07T09:04:10Z', 'abstract': "Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model's internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.", 'authors': ['Lishui Fan', 'Yu Zhang', 'Mouxiang Chen', 'Zhongxin Liu']}, {'id': 'http://arxiv.org/abs/2508.05165v1', 'title': 'Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic   Reward Models', 'published': '2025-08-07T08:54:27Z', 'updated': '2025-08-07T08:54:27Z', 'abstract': "Aligning LLMs with user preferences is crucial for real-world use but often requires costly fine-tuning or expensive inference, forcing trade-offs between alignment quality and computational cost. Existing inference-time methods typically ignore this balance, focusing solely on the optimized policy's performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a tuning-free, black-box-compatible approach that uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering to reduce inference calls while preserving alignment quality. On real-world prompt datasets, HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and greedy search baselines in multi-objective, goal-conditioned tasks under the same inference budget. We also find that HIA is effective under low-inference budgets with as little as one or two response queries, offering a practical solution for scalable, personalized LLM deployment.", 'authors': ['Mason Nakamura', 'Saaduddin Mahmud', 'Kyle H. Wray', 'Hamed Zamani', 'Shlomo Zilberstein']}, {'id': 'http://arxiv.org/abs/2508.05149v1', 'title': 'Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages', 'published': '2025-08-07T08:33:42Z', 'updated': '2025-08-07T08:33:42Z', 'abstract': 'Large language models (LLMs) have demonstrated potential in handling spoken inputs for high-resource languages, reaching state-of-the-art performance in various tasks. However, their applicability is still less explored in low-resource settings. This work investigates the use of Speech LLMs for low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a trainable lightweight projector connects a speech encoder and a LLM. Firstly, we assess training data volume requirements to match Whisper-only performance, re-emphasizing the challenges of limited data. Secondly, we show that leveraging mono- or multilingual projectors pretrained on high-resource languages reduces the impact of data scarcity, especially with small training sets. Using multilingual LLMs (EuroLLM, Salamandra) with whisper-large-v3-turbo, we evaluate performance on several public benchmarks, providing insights for future research on optimizing Speech LLMs for low-resource languages and multilinguality.', 'authors': ['Seraphina Fong', 'Marco Matassoni', 'Alessio Brutti']}, {'id': 'http://arxiv.org/abs/2508.05132v1', 'title': 'Towards Assessing Medical Ethics from Knowledge to Practice', 'published': '2025-08-07T08:10:14Z', 'updated': '2025-08-07T08:10:14Z', 'abstract': "The integration of large language models into healthcare necessitates a rigorous evaluation of their ethical reasoning, an area current benchmarks often overlook. We introduce PrinciplismQA, a comprehensive benchmark with 3,648 questions designed to systematically assess LLMs' alignment with core medical ethics. Grounded in Principlism, our benchmark features a high-quality dataset. This includes multiple-choice questions curated from authoritative textbooks and open-ended questions sourced from authoritative medical ethics case study literature, all validated by medical experts. Our experiments reveal a significant gap between models' ethical knowledge and their practical application, especially in dynamically applying ethical principles to real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence, often over-emphasizing other principles. Frontier closed-source models, driven by strong general capabilities, currently lead the benchmark. Notably, medical domain fine-tuning can enhance models' overall ethical competence, but further progress requires better alignment with medical ethical knowledge. PrinciplismQA offers a scalable framework to diagnose these specific ethical weaknesses, paving the way for more balanced and responsible medical AI.", 'authors': ['Chang Hong', 'Minghao Wu', 'Qingying Xiao', 'Yuchi Wang', 'Xiang Wan', 'Guangjun Yu', 'Benyou Wang', 'Yan Hu']}, {'id': 'http://arxiv.org/abs/2508.05129v1', 'title': 'Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation   through Domain-Aware Retrieval and Latent Reasoning', 'published': '2025-08-07T08:08:13Z', 'updated': '2025-08-07T08:08:13Z', 'abstract': 'With the rapid and continuous increase in academic publications, identifying high-quality research has become an increasingly pressing challenge. While recent methods leveraging Large Language Models (LLMs) for automated paper evaluation have shown great promise, they are often constrained by outdated domain knowledge and limited reasoning capabilities. In this work, we present PaperEval, a novel LLM-based framework for automated paper evaluation that addresses these limitations through two key components: 1) a domain-aware paper retrieval module that retrieves relevant concurrent work to support contextualized assessments of novelty and contributions, and 2) a latent reasoning mechanism that enables deep understanding of complex motivations and methodologies, along with comprehensive comparison against concurrently related work, to support more accurate and reliable evaluation. To guide the reasoning process, we introduce a progressive ranking optimization strategy that encourages the LLM to iteratively refine its predictions with an emphasis on relative comparison. Experiments on two datasets demonstrate that PaperEval consistently outperforms existing methods in both academic impact and paper quality evaluation. In addition, we deploy PaperEval in a real-world paper recommendation system for filtering high-quality papers, which has gained strong engagement on social media -- amassing over 8,000 subscribers and attracting over 10,000 views for many filtered high-quality papers -- demonstrating the practical effectiveness of PaperEval.', 'authors': ['Wuqiang Zheng', 'Yiyan Xu', 'Xinyu Lin', 'Chongming Gao', 'Wenjie Wang', 'Fuli Feng']}, {'id': 'http://arxiv.org/abs/2508.05128v1', 'title': 'Attention Basin: Why Contextual Position Matters in Large Language   Models', 'published': '2025-08-07T08:08:08Z', 'updated': '2025-08-07T08:08:08Z', 'abstract': "The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.", 'authors': ['Zihao Yi', 'Delong Zeng', 'Zhenqing Ling', 'Haohao Luo', 'Zhe Xu', 'Wei Liu', 'Jian Luan', 'Wanxia Cao', 'Ying Shen']}, {'id': 'http://arxiv.org/abs/2508.05118v1', 'title': 'Exploring Superior Function Calls via Reinforcement Learning', 'published': '2025-08-07T07:51:38Z', 'updated': '2025-08-07T07:51:38Z', 'abstract': 'Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02\\% overall accuracy, outperforming standard GRPO by up to 6\\% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community.', 'authors': ['Bingguang Hao', 'Maolin Wang', 'Zengzhuang Xu', 'Yicheng Chen', 'Cunyin Peng', 'Jinjie GU', 'Chenyi Zhuang']}, {'id': 'http://arxiv.org/abs/2508.05113v1', 'title': 'EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search', 'published': '2025-08-07T07:47:07Z', 'updated': '2025-08-07T07:47:07Z', 'abstract': 'Analog circuit design is a time-consuming, experience-driven task in chip development. Despite advances in AI, developing universal, fast, and stable gate sizing methods for analog circuits remains a significant challenge. Recent approaches combine Large Language Models (LLMs) with heuristic search techniques to enhance generalizability, but they often depend on large model sizes and lack portability across different technology nodes. To overcome these limitations, we propose EasySize, the first lightweight gate sizing framework based on a finetuned Qwen3-8B model, designed for universal applicability across process nodes, design specifications, and circuit topologies. EasySize exploits the varying Ease of Attainability (EOA) of performance metrics to dynamically construct task-specific loss functions, enabling efficient heuristic search through global Differential Evolution (DE) and local Particle Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned solely on 350nm node data, EasySize achieves strong performance on 5 operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology nodes without additional targeted training, and outperforms AutoCkt, a widely-used Reinforcement Learning based sizing framework, on 86.67\\% of tasks with more than 96.67\\% of simulation resources reduction. We argue that EasySize can significantly reduce the reliance on human expertise and computational resources in gate sizing, thereby accelerating and simplifying the analog circuit design process. EasySize will be open-sourced at a later date.', 'authors': ['Xinyue Wu', 'Fan Hu', 'Shaik Jani Babu', 'Yi Zhao', 'Xinfei Guo']}, {'id': 'http://arxiv.org/abs/2508.05100v1', 'title': 'BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation', 'published': '2025-08-07T07:37:25Z', 'updated': '2025-08-07T07:37:25Z', 'abstract': 'With the rapid advancement of large language models (LLMs), retrieval-augmented generation (RAG) has emerged as a critical approach to supplement the inherent knowledge limitations of LLMs. However, due to the typically large volume of retrieved information, RAG tends to operate with long context lengths. From the perspective of entropy engineering, we identify unconstrained entropy growth and attention dilution due to long retrieval context as significant factors affecting RAG performance. In this paper, we propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves the adaptability of RAG systems to varying context lengths through the principle of entropy invariance. By leveraging balanced context entropy to reformulate attention dynamics, BEE-RAG separates attention sensitivity from context length, ensuring a stable entropy level. Building upon this, we introduce a zero-shot inference strategy for multi-importance estimation and a parameter-efficient adaptive fine-tuning mechanism to obtain the optimal balancing factor for different settings. Extensive experiments across multiple RAG tasks demonstrate the effectiveness of BEE-RAG.', 'authors': ['Yuhao Wang', 'Ruiyang Ren', 'Yucheng Wang', 'Jing Liu', 'Wayne Xin Zhao', 'Hua Wu', 'Haifeng Wang']}, {'id': 'http://arxiv.org/abs/2508.05087v1', 'title': 'JPS: Jailbreak Multimodal Large Language Models with Collaborative   Visual Perturbation and Textual Steering', 'published': '2025-08-07T07:14:01Z', 'updated': '2025-08-07T07:14:01Z', 'abstract': 'Jailbreak attacks against multimodal large language Models (MLLMs) are a significant research focus. Current research predominantly focuses on maximizing attack success rate (ASR), often overlooking whether the generated responses actually fulfill the attacker\'s malicious intent. This oversight frequently leads to low-quality outputs that bypass safety filters but lack substantial harmful content. To address this gap, we propose JPS, \\underline{J}ailbreak MLLMs with collaborative visual \\underline{P}erturbation and textual \\underline{S}teering, which achieves jailbreaks via corporation of visual image and textually steering prompt. Specifically, JPS utilizes target-guided adversarial image perturbations for effective safety bypass, complemented by "steering prompt" optimized via a multi-agent system to specifically guide LLM responses fulfilling the attackers\' intent. These visual and textual components undergo iterative co-optimization for enhanced performance. To evaluate the quality of attack outcomes, we propose the Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a Reasoning-LLM-based evaluator. Our experiments show JPS sets a new state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with analyses confirming its efficacy. Codes are available at \\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}. \\color{warningcolor}{Warning: This paper contains potentially sensitive contents.}', 'authors': ['Renmiao Chen', 'Shiyao Cui', 'Xuancheng Huang', 'Chengwei Pan', 'Victor Shea-Jay Huang', 'QingLin Zhang', 'Xuan Ouyang', 'Zhexin Zhang', 'Hongning Wang', 'Minlie Huang']}, {'id': 'http://arxiv.org/abs/2508.05078v1', 'title': "Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task   Learning", 'published': '2025-08-07T07:02:55Z', 'updated': '2025-08-07T07:02:55Z', 'abstract': 'Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large Language Models (LLMs). In practice, LLMs are often required to handle a diverse set of tasks from multiple domains, a scenario naturally addressed by multi-task learning (MTL). Within this MTL context, a prevailing trend involves LoRA variants with multiple adapters or heads, which advocate for structural diversity to capture task-specific knowledge. Our findings present a direct challenge to this paradigm. We first show that a simplified multi-head architecture with high inter-head similarity substantially outperforms complex multi-adapter and multi-head systems. This leads us to question the multi-component paradigm itself, and we further demonstrate that a standard single-adapter LoRA, with a sufficiently increased rank, also achieves highly competitive performance. These results lead us to a new hypothesis: effective MTL generalization hinges on learning robust shared representations, not isolating task-specific features. To validate this, we propose Align-LoRA, which incorporates an explicit loss to align task representations within the shared adapter space. Experiments confirm that Align-LoRA significantly surpasses all baselines, establishing a simpler yet more effective paradigm for adapting LLMs to multiple tasks. The code is available at https://github.com/jinda-liu/Align-LoRA.', 'authors': ['Jinda Liu', 'Bo Cheng', 'Yi Chang', 'Yuan Wu']}, {'id': 'http://arxiv.org/abs/2508.05064v1', 'title': 'A Study of the Framework and Real-World Applications of Language   Embedding for 3D Scene Understanding', 'published': '2025-08-07T06:33:08Z', 'updated': '2025-08-07T06:33:08Z', 'abstract': 'Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.', 'authors': ['Mahmoud Chick Zaouali', 'Todd Charter', 'Yehor Karpichev', 'Brandon Haworth', 'Homayoun Najjjaran']}, {'id': 'http://arxiv.org/abs/2508.05053v1', 'title': 'Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?', 'published': '2025-08-07T06:10:15Z', 'updated': '2025-08-07T06:10:15Z', 'abstract': "While Multi-modal Large Language Models (MLLMs) have shown impressive capabilities in document understanding tasks, their ability to locate and reason about fine-grained details within complex documents remains understudied. Consider searching a restaurant menu for a specific nutritional detail or identifying a disclaimer in a lengthy newspaper article tasks that demand careful attention to small but significant details within a broader narrative, akin to Finding Needles in Images (NiM). To address this gap, we introduce NiM, a carefully curated benchmark spanning diverse real-world documents including newspapers, menus, and lecture images, specifically designed to evaluate MLLMs' capability in these intricate tasks. Building on this, we further propose Spot-IT, a simple yet effective approach that enhances MLLMs capability through intelligent patch selection and Gaussian attention, motivated from how humans zoom and focus when searching documents. Our extensive experiments reveal both the capabilities and limitations of current MLLMs in handling fine-grained document understanding tasks, while demonstrating the effectiveness of our approach. Spot-IT achieves significant improvements over baseline methods, particularly in scenarios requiring precise detail extraction from complex layouts.", 'authors': ['Parth Thakkar', 'Ankush Agarwal', 'Prasad Kasu', 'Pulkit Bansal', 'Chaitanya Devaguptapu']}, {'id': 'http://arxiv.org/abs/2508.05028v1', 'title': 'Evaluation of LLMs in AMR Parsing', 'published': '2025-08-07T04:43:47Z', 'updated': '2025-08-07T04:43:47Z', 'abstract': 'Meaning Representation (AMR) is a semantic formalism that encodes sentence meaning as rooted, directed, acyclic graphs, where nodes represent concepts and edges denote semantic relations. Finetuning decoder only Large Language Models (LLMs) represent a promising novel straightfoward direction for AMR parsing. This paper presents a comprehensive evaluation of finetuning four distinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that straightfoward finetuning of decoder only LLMs can achieve comparable performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2 demonstrates competitive performance against SOTA AMR parsers given a straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5 excels in structural validity.', 'authors': ['Shu Han Ho']}, {'id': 'http://arxiv.org/abs/2508.05015v1', 'title': 'SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models', 'published': '2025-08-07T03:50:48Z', 'updated': '2025-08-07T03:50:48Z', 'abstract': 'Large language models (LLMs) have shown strong reasoning capabilities when fine-tuned with reinforcement learning (RL). However, such methods require extensive data and compute, making them impractical for smaller models. Current approaches to curriculum learning or data selection are largely heuristic-driven or demand extensive computational resources, limiting their scalability and generalizability. We propose \\textbf{SPaRFT}, a self-paced learning framework that enables efficient learning based on the capability of the model being trained through optimizing which data to use and when. First, we apply \\emph{cluster-based data reduction} to partition training data by semantics and difficulty, extracting a compact yet diverse subset that reduces redundancy. Then, a \\emph{multi-armed bandit} treats data clusters as arms, optimized to allocate training samples based on model current performance. Experiments across multiple reasoning benchmarks show that SPaRFT achieves comparable or better accuracy than state-of-the-art baselines while using up to \\(100\\times\\) fewer samples. Ablation studies and analyses further highlight the importance of both data clustering and adaptive selection. Our results demonstrate that carefully curated, performance-driven training curricula can unlock strong reasoning abilities in LLMs with minimal resources.', 'authors': ['Dai Do', 'Manh Nguyen', 'Svetha Venkatesh', 'Hung Le']}, {'id': 'http://arxiv.org/abs/2508.05012v1', 'title': 'Making Prompts First-Class Citizens for Adaptive LLM Pipelines', 'published': '2025-08-07T03:49:56Z', 'updated': '2025-08-07T03:49:56Z', 'abstract': 'Modern LLM pipelines increasingly resemble data-centric systems: they retrieve external context, compose intermediate outputs, validate results, and adapt based on runtime feedback. Yet, the central element guiding this process -- the prompt -- remains a brittle, opaque string, disconnected from the surrounding dataflow. This disconnect limits reuse, optimization, and runtime control.   In this paper, we describe our vision and an initial design for SPEAR, a language and runtime that fills this prompt management gap by making prompts structured, adaptive, and first-class components of the execution model. SPEAR enables (1) runtime prompt refinement -- modifying prompts dynamically in response to execution-time signals such as confidence, latency, or missing context; and (2) structured prompt management -- organizing prompt fragments into versioned views with support for introspection and logging.   SPEAR defines a prompt algebra that governs how prompts are constructed and adapted within a pipeline. It supports multiple refinement modes (manual, assisted, and automatic), giving developers a balance between control and automation. By treating prompt logic as structured data, SPEAR enables optimizations such as operator fusion, prefix caching, and view reuse. Preliminary experiments quantify the behavior of different refinement modes compared to static prompts and agentic retries, as well as the impact of prompt-level optimizations such as operator fusion.', 'authors': ['Ugur Cetintemel', 'Shu Chen', 'Alexander W. Lee', 'Deepti Raghavan']}, {'id': 'http://arxiv.org/abs/2508.05009v1', 'title': 'Can Large Language Models Integrate Spatial Data? Empirical Insights   into Reasoning Strengths and Computational Weaknesses', 'published': '2025-08-07T03:44:20Z', 'updated': '2025-08-07T03:44:20Z', 'abstract': 'We explore the application of large language models (LLMs) to empower domain experts in integrating large, heterogeneous, and noisy urban spatial datasets. Traditional rule-based integration methods are unable to cover all edge cases, requiring manual verification and repair. Machine learning approaches require collecting and labeling of large numbers of task-specific samples. In this study, we investigate the potential of LLMs for spatial data integration. Our analysis first considers how LLMs reason about environmental spatial relationships mediated by human experience, such as between roads and sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they struggle to connect the macro-scale environment with the relevant computational geometry tasks, often producing logically incoherent responses. But when provided relevant features, thereby reducing dependence on spatial reasoning, LLMs are able to generate high-performing results. We then adapt a review-and-refine method, which proves remarkably effective in correcting erroneous initial responses while preserving accurate responses. We discuss practical implications of employing LLMs for spatial data integration in real-world contexts and outline future research directions, including post-training, multi-modal integration methods, and support for diverse data formats. Our findings position LLMs as a promising and flexible alternative to traditional rule-based heuristics, advancing the capabilities of adaptive spatial data integration.', 'authors': ['Bin Han', 'Robert Wolfe', 'Anat Caspi', 'Bill Howe']}, {'id': 'http://arxiv.org/abs/2508.05005v1', 'title': 'Generative AI for Object-Oriented Programming: Writing the Right Code   and Reasoning the Right Logic', 'published': '2025-08-07T03:38:17Z', 'updated': '2025-08-07T03:38:17Z', 'abstract': 'We find ourselves in the midst of an explosion in artificial intelligence research, particularly with large language models (LLMs). These models have diverse applications spanning finance, commonsense knowledge graphs, medicine, and visual analysis. In the world of Object-Oriented Programming(OOP), a robust body of knowledge and methods has been developed for managing complex tasks through object-oriented thinking. However, the intersection of LLMs with OOP remains an underexplored territory. Empirically, we currently possess limited understanding of how LLMs can enhance the effectiveness of OOP learning and code writing, as well as how we can evaluate such AI-powered tools. Our work aims to address this gap by presenting a vision from the perspectives of key stakeholders involved in an OOP task: programmers, mariners, and experienced programmers. We identify critical junctures within typical coding workflows where the integration of LLMs can offer significant benefits. Furthermore, we propose ways to augment existing logical reasoning and code writing, ultimately enhancing the programming experience.', 'authors': ['Gang Xu', 'Airong Wang', 'Yushan Pan']}, {'id': 'http://arxiv.org/abs/2508.05004v1', 'title': 'R-Zero: Self-Evolving Reasoning LLM from Zero Data', 'published': '2025-08-07T03:38:16Z', 'updated': '2025-08-07T03:38:16Z', 'abstract': 'Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.', 'authors': ['Chengsong Huang', 'Wenhao Yu', 'Xiaoyang Wang', 'Hongming Zhang', 'Zongxia Li', 'Ruosen Li', 'Jiaxin Huang', 'Haitao Mi', 'Dong Yu']}, {'id': 'http://arxiv.org/abs/2508.04995v1', 'title': 'Situated Epistemic Infrastructures: A Diagnostic Framework for   Post-Coherence Knowledge', 'published': '2025-08-07T03:08:23Z', 'updated': '2025-08-07T03:08:23Z', 'abstract': 'Large Language Models (LLMs) such as ChatGPT have rendered visible the fragility of contemporary knowledge infrastructures by simulating coherence while bypassing traditional modes of citation, authority, and validation. This paper introduces the Situated Epistemic Infrastructures (SEI) framework as a diagnostic tool for analyzing how knowledge becomes authoritative across hybrid human-machine systems under post-coherence conditions. Rather than relying on stable scholarly domains or bounded communities of practice, SEI traces how credibility is mediated across institutional, computational, and temporal arrangements. Integrating insights from infrastructure studies, platform theory, and epistemology, the framework foregrounds coordination over classification, emphasizing the need for anticipatory and adaptive models of epistemic stewardship. The paper contributes to debates on AI governance, knowledge production, and the ethical design of information systems by offering a robust alternative to representationalist models of scholarly communication.', 'authors': ['Matthew Kelly']}, {'id': 'http://arxiv.org/abs/2508.04975v1', 'title': 'Sentiment-Aware Stock Price Prediction with Transformer and   LLM-Generated Formulaic Alpha', 'published': '2025-08-07T02:02:39Z', 'updated': '2025-08-07T02:02:39Z', 'abstract': 'Traditionally, traders and quantitative analysts address alpha decay by manually crafting formulaic alphas, mathematical expressions that identify patterns or signals in financial data, through domain expertise and trial-and-error. This process is often time-consuming and difficult to scale. With recent advances in large language models (LLMs), it is now possible to automate the generation of such alphas by leveraging the reasoning capabilities of LLMs. This paper introduces a novel framework that integrates a prompt-based LLM with a Transformer model for stock price prediction. The LLM first generates diverse and adaptive alphas using structured inputs such as historical stock features (Close, Open, High, Low, Volume), technical indicators, sentiment scores of both target and related companies. These alphas, instead of being used directly for trading, are treated as high-level features that capture complex dependencies within the financial data. To evaluate the effectiveness of these LLM-generated formulaic alphas, the alpha features are then fed into prediction models such as Transformer, LSTM, TCN, SVR, and Random Forest to forecast future stock prices. Experimental results demonstrate that the LLM-generated alphas significantly improve predictive accuracy. Moreover, the accompanying natural language reasoning provided by the LLM enhances the interpretability and transparency of the predictions, supporting more informed financial decision-making.', 'authors': ['Qizhao Chen', 'Hiroaki Kawashima']}, {'id': 'http://arxiv.org/abs/2508.04939v1', 'title': 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating   Linguistic Shibboleth Detection in LLM Hiring Evaluations', 'published': '2025-08-06T23:51:03Z', 'updated': '2025-08-06T23:51:03Z', 'abstract': "This paper introduces a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic markers that can inadvertently reveal demographic attributes such as gender, social class, or regional background. Through carefully constructed interview simulations using 100 validated question-response pairs, we demonstrate how LLMs systematically penalize certain linguistic patterns, particularly hedging language, despite equivalent content quality. Our benchmark generates controlled linguistic variations that isolate specific phenomena while maintaining semantic equivalence, which enables the precise measurement of demographic bias in automated evaluation systems. We validate our approach along multiple linguistic dimensions, showing that hedged responses receive 25.6% lower ratings on average, and demonstrate the benchmark's effectiveness in identifying model-specific biases. This work establishes a foundational framework for detecting and measuring linguistic discrimination in AI systems, with broad applications to fairness in automated decision-making contexts.", 'authors': ['Julia Kharchenko', 'Tanya Roosta', 'Aman Chadha', 'Chirag Shah']}, {'id': 'http://arxiv.org/abs/2508.04904v1', 'title': 'Root Cause Analysis Training for Healthcare Professionals With   AI-Powered Virtual Simulation: A Proof-of-Concept', 'published': '2025-08-06T22:02:45Z', 'updated': '2025-08-06T22:02:45Z', 'abstract': "Root Cause Analysis (RCA) is a critical tool for investigating adverse events in healthcare and improving patient safety. However, existing RCA training programs are often limited by high resource demands, leading to insufficient training and inconsistent implementation. To address this challenge, we present an AI-powered 3D simulation game that helps healthcare professionals develop RCA skills through interactive, immersive simulations. This approach offers a cost-effective, scalable, and accessible alternative to traditional training. The prototype simulates an RCA investigation following a death in the ICU, where learners interview five virtual avatars representing ICU team members to investigate the incident and complete a written report. The system enables natural, life-like interactions with avatars via large language models (LLMs), emotional text-to-speech, and AI-powered animations. An additional LLM component provides formative and summative feedback to support continual improvement. We conclude by outlining plans to empirically evaluate the system's efficacy.", 'authors': ['Yuqi Hu', 'Qiwen Xiong', 'Zhenzhen Qin', 'Brandon Watanabe', 'Yujing Wang', 'Mirjana Prpa', 'Ilmi Yoon']}, {'id': 'http://arxiv.org/abs/2508.04903v1', 'title': 'RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM   Systems with Structured Memory', 'published': '2025-08-06T21:59:34Z', 'updated': '2025-08-06T21:59:34Z', 'abstract': 'Multi-agent large language model (LLM) systems have shown strong potential in complex reasoning and collaborative decision-making tasks. However, most existing coordination schemes rely on static or full-context routing strategies, which lead to excessive token consumption, redundant memory exposure, and limited adaptability across interaction rounds. We introduce RCR-Router, a modular and role-aware context routing framework designed to enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge, this is the first routing approach that dynamically selects semantically relevant memory subsets for each agent based on its role and task stage, while adhering to a strict token budget. A lightweight scoring policy guides memory selection, and agent outputs are iteratively integrated into a shared memory store to facilitate progressive context refinement. To better evaluate model behavior, we further propose an Answer Quality Score metric that captures LLM-generated explanations beyond standard QA accuracy. Experiments on three multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate that RCR-Router reduces token usage (up to 30%) while improving or maintaining answer quality. These results highlight the importance of structured memory routing and output-aware evaluation in advancing scalable multi-agent LLM systems.', 'authors': ['Jun Liu', 'Zhenglun Kong', 'Changdi Yang', 'Fan Yang', 'Tianqi Li', 'Peiyan Dong', 'Joannah Nanjekye', 'Hao Tang', 'Geng Yuan', 'Wei Niu', 'Wenbin Zhang', 'Pu Zhao', 'Xue Lin', 'Dong Huang', 'Yanzhi Wang']}, {'id': 'http://arxiv.org/abs/2508.04894v1', 'title': 'Adversarial Attacks and Defenses on Graph-aware Large Language Models   (LLMs)', 'published': '2025-08-06T21:38:52Z', 'updated': '2025-08-06T21:38:52Z', 'abstract': 'Large Language Models (LLMs) are increasingly integrated with graph-structured data for tasks like node classification, a domain traditionally dominated by Graph Neural Networks (GNNs). While this integration leverages rich relational information to improve task performance, their robustness against adversarial attacks remains unexplored. We take the first step to explore the vulnerabilities of graph-aware LLMs by leveraging existing adversarial attack methods tailored for graph-based models, including those for poisoning (training-time attacks) and evasion (test-time attacks), on two representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al. 2024). Additionally, we discover a new attack surface for LLAGA where an attacker can inject malicious nodes as placeholders into the node sequence template to severely degrade its performance. Our systematic analysis reveals that certain design choices in graph encoding can enhance attack success, with specific findings that: (1) the node sequence template in LLAGA increases its vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater robustness; and (3) both approaches remain susceptible to imperceptible feature perturbation attacks. Finally, we propose an end-to-end defense framework GALGUARD, that combines an LLM-based feature correction module to mitigate feature-level perturbations and adapted GNN defenses to protect against structural attacks.', 'authors': ['Iyiola E. Olatunji', 'Franziska Boenisch', 'Jing Xu', 'Adam Dziedzic']}, {'id': 'http://arxiv.org/abs/2508.04865v1', 'title': 'Agnostics: Learning to Code in Any Programming Language via   Reinforcement with a Universal Learning Environment', 'published': '2025-08-06T20:30:55Z', 'updated': '2025-08-06T20:30:55Z', 'abstract': 'Large language models (LLMs) already excel at writing code in high-resource languages such as Python and JavaScript, yet stumble on low-resource languages that remain essential to science and engineering. Besides the obvious shortage of pre-training data, post-training itself is a bottleneck: every new language seems to require new datasets, test harnesses, and reinforcement-learning (RL) infrastructure.   We introduce Agnostics, a language-agnostic post-training pipeline that eliminates this per-language engineering. The key idea is to judge code solely by its externally observable behavior, so a single verifier can test solutions written in any language. Concretely, we (i) use an LLM to rewrite existing unit-test datasets into an I/O format, (ii) supply a short configuration that tells the verifier how to compile and run a target language, and (iii) apply reinforcement learning with verifiable rewards (RLVR) in a robust code execution environment.   Applied to five low-resource languages--Lua, Julia, R, OCaml, and Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other 16B-70B open-weight models; (2) scales cleanly to larger and diverse model families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for ${\\le} 16$B parameter models, sets new state-of-the-art pass@1 results on MultiPL-E and a new multi-language version LiveCodeBench that we introduce.   We will release the language-agnostic training datasets (Ag-MBPP-X, Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use configurations, making RL post-training in any programming language as simple as editing a short YAML file.', 'authors': ['Aleksander Boruch-Gruszecki', 'Yangtian Zi', 'Zixuan Wu', 'Tejas Oberoi', 'Carolyn Jane Anderson', 'Joydeep Biswas', 'Arjun Guha']}, {'id': 'http://arxiv.org/abs/2508.04853v1', 'title': 'Provable Post-Training Quantization: Theoretical Analysis of OPTQ and   Qronos', 'published': '2025-08-06T20:00:40Z', 'updated': '2025-08-06T20:00:40Z', 'abstract': "Post-training quantization (PTQ) has become a crucial tool for reducing the memory and compute costs of modern deep neural networks, including large language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as GPTQ-has emerged as a leading method due to its computational efficiency and strong empirical performance. Despite its widespread adoption, however, OPTQ lacks rigorous quantitative theoretical guarantees. This paper presents the first quantitative error bounds for both deterministic and stochastic variants of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ algorithm. We analyze how OPTQ's iterative procedure induces quantization error and derive non-asymptotic 2-norm error bounds that depend explicitly on the calibration data and a regularization parameter that OPTQ uses. Our analysis provides theoretical justification for several practical design choices, including the widely used heuristic of ordering features by decreasing norm, as well as guidance for selecting the regularization parameter. For the stochastic variant, we establish stronger infinity-norm error bounds, which enable control over the required quantization alphabet and are particularly useful for downstream layers and nonlinearities. Finally, we extend our analysis to Qronos, providing new theoretical bounds, for both its deterministic and stochastic variants, that help explain its empirical advantages.", 'authors': ['Haoyu Zhang', 'Shihao Zhang', 'Ian Colbert', 'Rayan Saab']}, {'id': 'http://arxiv.org/abs/2508.04848v1', 'title': 'Large Language Models Reasoning Abilities Under Non-Ideal Conditions   After RL-Fine-Tuning', 'published': '2025-08-06T19:51:29Z', 'updated': '2025-08-06T19:51:29Z', 'abstract': 'Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs. We formally define and evaluate these challenging scenarios. We fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM) using RL with a representative policy-gradient algorithm and then test their performance on eight public datasets. Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities. Although we propose a scenario-specific remediation method, our results suggest current methods leave these reasoning deficits largely unresolved. This work highlights that the reasoning abilities of large models are often overstated and underscores the importance of evaluating models under non-ideal scenarios. The code and data will be released at XXXX.', 'authors': ['Chang Tian', 'Matthew B. Blaschko', 'Mingzhe Xing', 'Xiuxing Li', 'Yinliang Yue', 'Marie-Francine Moens']}, {'id': 'http://arxiv.org/abs/2508.04846v1', 'title': 'Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based   Geographical Information Systems (AWebGIS)', 'published': '2025-08-06T19:50:29Z', 'updated': '2025-08-06T19:50:29Z', 'abstract': "Autonomous web-based geographical information systems (AWebGIS) aim to perform geospatial operations from natural language input, providing intuitive, intelligent, and hands-free interaction. However, most current solutions rely on cloud-based large language models (LLMs), which require continuous internet access and raise users' privacy and scalability issues due to centralized server processing. This study compares three approaches to enabling AWebGIS: (1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2) a semi-automated offline method using classical machine learning classifiers such as support vector machine and random forest; and (3) a fully autonomous offline (client-side) method based on a fine-tuned small language model (SLM), specifically T5-small model, executed in the client's web browser. The third approach, which leverages SLMs, achieved the highest accuracy among all methods, with an exact matching accuracy of 0.93, Levenshtein similarity of 0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L scores of 0.98. Crucially, this client-side computation strategy reduces the load on backend servers by offloading processing to the user's device, eliminating the need for server-based inference. These results highlight the feasibility of browser-executable models for AWebGIS solutions.", 'authors': ['Mahdi Nazari Ashani', 'Ali Asghar Alesheikh', 'Saba Kazemi', 'Kimya Kheirkhah', 'Yasin Mohammadi', 'Fatemeh Rezaie', 'Amir Mahdi Manafi', 'Hedieh Zarkesh']}, {'id': 'http://arxiv.org/abs/2508.04842v1', 'title': 'Charts-of-Thought: Enhancing LLM Visualization Literacy Through   Structured Data Extraction', 'published': '2025-08-06T19:42:43Z', 'updated': '2025-08-06T19:42:43Z', 'abstract': 'This paper evaluates the visualization literacy of modern Large Language Models (LLMs) and introduces a novel prompting technique called Charts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet, GPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment Test (VLAT) using standard prompts and our structured approach. The Charts-of-Thought method guides LLMs through a systematic data extraction, verification, and analysis process before answering visualization questions. Our results show Claude-3.7-sonnet achieved a score of 50.17 using this method, far exceeding the human baseline of 28.82. This approach improved performance across all models, with score increases of 21.8% for GPT-4.5, 9.4% for Gemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The performance gains were consistent across original and modified VLAT charts, with Claude correctly answering 100% of questions for several chart types that previously challenged LLMs. Our study reveals that modern multimodal LLMs can surpass human performance on visualization literacy tasks when given the proper analytical framework. These findings establish a new benchmark for LLM visualization literacy and demonstrate the importance of structured prompting strategies for complex visual interpretation tasks. Beyond improving LLM visualization literacy, Charts-of-Thought could also enhance the accessibility of visualizations, potentially benefiting individuals with visual impairments or lower visualization literacy.', 'authors': ['Amit Kumar Das', 'Mohammad Tarun', 'Klaus Mueller']}, {'id': 'http://arxiv.org/abs/2508.04826v1', 'title': "Persistent Instability in LLM's Personality Measurements: Effects of   Scale, Reasoning, and Conversation History", 'published': '2025-08-06T19:11:33Z', 'updated': '2025-08-06T19:11:33Z', 'abstract': 'Large language models require consistent behavioral patterns for safe deployment, yet their personality-like traits remain poorly understood. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25+ open-source models (1B-671B parameters) across 500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted personality instruments, we systematically vary question order, paraphrasing, personas, and reasoning modes. Our findings challenge fundamental deployment assumptions: (1) Even 400B+ models exhibit substantial response variability (SD > 0.4); (2) Minor prompt reordering alone shifts personality measurements by up to 20%; (3) Interventions expected to stabilize behavior, such as chain-of-thought reasoning, detailed personas instruction, inclusion of conversation history, can paradoxically increase variability; (4) LLM-adapted instruments show equal instability to human-centric versions, confirming architectural rather than translational limitations. This persistent instability across scales and mitigation strategies suggests current LLMs lack the foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that personality-based alignment strategies may be fundamentally inadequate.', 'authors': ['Tommaso Tosato', 'Saskia Helbling', 'Yorguin-Jose Mantilla-Ramos', 'Mahmood Hegazy', 'Alberto Tosato', 'David John Lemay', 'Irina Rish', 'Guillaume Dumas']}, {'id': 'http://arxiv.org/abs/2508.04820v1', 'title': 'Automated File-Level Logging Generation for Machine Learning   Applications using LLMs: A Case Study using GPT-4o Mini', 'published': '2025-08-06T18:57:51Z', 'updated': '2025-08-06T18:57:51Z', 'abstract': 'Logging is essential in software development, helping developers monitor system behavior and aiding in debugging applications. Given the ability of large language models (LLMs) to generate natural language and code, researchers are exploring their potential to generate log statements. However, prior work focuses on evaluating logs introduced in code functions, leaving file-level log generation underexplored -- especially in machine learning (ML) applications, where comprehensive logging can enhance reliability. In this study, we evaluate the capacity of GPT-4o mini as a case study to generate log statements for ML projects at file level. We gathered a set of 171 ML repositories containing 4,073 Python files with at least one log statement. We identified and removed the original logs from the files, prompted the LLM to generate logs for them, and evaluated both the position of the logs and log level, variables, and text quality of the generated logs compared to human-written logs. In addition, we manually analyzed a representative sample of generated logs to identify common patterns and challenges. We find that the LLM introduces logs in the same place as humans in 63.91% of cases, but at the cost of a high overlogging rate of 82.66%. Furthermore, our manual analysis reveals challenges for file-level logging, which shows overlogging at the beginning or end of a function, difficulty logging within large code blocks, and misalignment with project-specific logging conventions. While the LLM shows promise for generating logs for complete files, these limitations remain to be addressed for practical implementation.', 'authors': ['Mayra Sofia Ruiz Rodriguez', 'SayedHassan Khatoonabadi', 'Emad Shihab']}, {'id': 'http://arxiv.org/abs/2508.04795v1', 'title': 'Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a   Frozen LLM', 'published': '2025-08-06T18:14:04Z', 'updated': '2025-08-06T18:14:04Z', 'abstract': 'In dialogue transcription pipelines, Large Language Models (LLMs) are frequently employed in post-processing to improve grammar, punctuation, and readability. We explore a complementary post-processing step: enriching transcribed dialogues by adding metadata tags for speaker characteristics such as age, gender, and emotion. Some of the tags are global to the entire dialogue, while some are time-variant. Our approach couples frozen audio foundation models, such as Whisper or WavLM, with a frozen LLAMA language model to infer these speaker attributes, without requiring task-specific fine-tuning of either model. Using lightweight, efficient connectors to bridge audio and language representations, we achieve competitive performance on speaker profiling tasks while preserving modularity and speed. Additionally, we demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving an Equal Error Rate of 8.8% in some scenarios.', 'authors': ['Thomas Thebaud', 'Yen-Ju Lu', 'Matthew Wiesner', 'Peter Viechnicki', 'Najim Dehak']}, {'id': 'http://arxiv.org/abs/2508.04787v1', 'title': 'Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with   Interactive AI-Generated Educational Podcasts', 'published': '2025-08-06T18:03:42Z', 'updated': '2025-08-06T18:03:42Z', 'abstract': 'This study examined whether embedding LLM-guided reflection prompts in an interactive AI-generated podcast improved learning and user experience compared to a version without prompts. Thirty-six undergraduates participated, and while learning outcomes were similar across conditions, reflection prompts reduced perceived attractiveness, highlighting a call for more research on reflective interactivity design.', 'authors': ['Vishnu Menon', 'Andy Cherney', 'Elizabeth B. Cloude', 'Li Zhang', 'Tiffany D. Do']}, {'id': 'http://arxiv.org/abs/2508.04698v1', 'title': 'FaST: Feature-aware Sampling and Tuning for Personalized Preference   Alignment with Limited Data', 'published': '2025-08-06T17:58:26Z', 'updated': '2025-08-06T17:58:26Z', 'abstract': 'LLM-powered conversational assistants are often deployed in a one-size-fits-all manner, which fails to accommodate individual user preferences. Recently, LLM personalization -- tailoring models to align with specific user preferences -- has gained increasing attention as a way to bridge this gap. In this work, we specifically focus on a practical yet challenging setting where only a small set of preference annotations can be collected per user -- a problem we define as Personalized Preference Alignment with Limited Data (PPALLI). To support research in this area, we introduce two datasets -- DnD and ELIP -- and benchmark a variety of alignment techniques on them. We further propose FaST, a highly parameter-efficient approach that leverages high-level features automatically discovered from the data, achieving the best overall performance.', 'authors': ['Thibaut Thonet', 'Germán Kruszewski', 'Jos Rozen', 'Pierre Erbacher', 'Marc Dymetman']}, {'id': 'http://arxiv.org/abs/2508.04679v1', 'title': 'MisVisFix: An Interactive Dashboard for Detecting, Explaining, and   Correcting Misleading Visualizations using Large Language Models', 'published': '2025-08-06T17:45:11Z', 'updated': '2025-08-06T17:45:11Z', 'abstract': 'Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models (LLMs) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming LLM-based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data communication.', 'authors': ['Amit Kumar Das', 'Klaus Mueller']}, {'id': 'http://arxiv.org/abs/2508.04676v1', 'title': 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via   General Samples Replay', 'published': '2025-08-06T17:42:22Z', 'updated': '2025-08-06T17:42:22Z', 'abstract': 'The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.', 'authors': ['Yunan Zhang', 'Shuoran Jiang', 'Mengchen Zhao', 'Yuefeng Li', 'Yang Fan', 'Xiangping Wu', 'Qingcai Chen']}, {'id': 'http://arxiv.org/abs/2508.04664v1', 'title': 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context   Management', 'published': '2025-08-06T17:32:58Z', 'updated': '2025-08-06T17:32:58Z', 'abstract': "Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.", 'authors': ['Mo Li', 'L. H. Xu', 'Qitai Tan', 'Ting Cao', 'Yunxin Liu']}, {'id': 'http://arxiv.org/abs/2508.04655v1', 'title': 'X-SAM: From Segment Anything to Any Segmentation', 'published': '2025-08-06T17:19:10Z', 'updated': '2025-08-06T17:19:10Z', 'abstract': 'Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \\textit{segment anything} to \\textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.', 'authors': ['Hao Wang', 'Limeng Qiao', 'Zequn Jie', 'Zhijian Huang', 'Chengjian Feng', 'Qingfang Zheng', 'Lin Ma', 'Xiangyuan Lan', 'Xiaodan Liang']}, {'id': 'http://arxiv.org/abs/2508.04652v1', 'title': 'LLM Collaboration With Multi-Agent Reinforcement Learning', 'published': '2025-08-06T17:18:25Z', 'updated': '2025-08-06T17:18:25Z', 'abstract': 'A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.', 'authors': ['Shuo Liu', 'Zeyu Liang', 'Xueguang Lyu', 'Christopher Amato']}, {'id': 'http://arxiv.org/abs/2508.04634v1', 'title': 'VirtLab: An AI-Powered System for Flexible, Customizable, and   Large-scale Team Simulations', 'published': '2025-08-06T17:02:01Z', 'updated': '2025-08-06T17:02:01Z', 'abstract': "Simulating how team members collaborate within complex environments using Agentic AI is a promising approach to explore hypotheses grounded in social science theories and study team behaviors. We introduce VirtLab, a user-friendly, customizable, multi-agent, and scalable team simulation system that enables testing teams with LLM-based agents in spatial and temporal settings. This system addresses the current frameworks' design and technical limitations that do not consider flexible simulation scenarios and spatial settings. VirtLab contains a simulation engine and a web interface that enables both technical and non-technical users to formulate, run, and analyze team simulations without programming. We demonstrate the system's utility by comparing ground truth data with simulated scenarios.", 'authors': ['Mohammed Almutairi', 'Charles Chiang', 'Haoze Guo', 'Matthew Belcher', 'Nandini Banerjee', 'Maria Milkowski', 'Svitlana Volkova', 'Daniel Nguyen', 'Tim Weninger', 'Michael Yankoski', 'Trenton W. Ford', 'Diego Gomez-Zara']}, {'id': 'http://arxiv.org/abs/2508.04632v2', 'title': 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with   Verifiable Rewards', 'published': '2025-08-06T17:00:54Z', 'updated': '2025-08-07T11:30:20Z', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.', 'authors': ['Xu Guo', 'Tianyi Liang', 'Tong Jian', 'Xiaogui Yang', 'Ling-I Wu', 'Chenhui Li', 'Zhihui Lu', 'Qipeng Guo', 'Kai Chen']}, {'id': 'http://arxiv.org/abs/2508.04626v1', 'title': 'P-Aligner: Enabling Pre-Alignment of Language Models via Principled   Instruction Synthesis', 'published': '2025-08-06T16:51:38Z', 'updated': '2025-08-06T16:51:38Z', 'abstract': 'Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead.', 'authors': ['Feifan Song', 'Bofei Gao', 'Yifan Song', 'Yi Liu', 'Weimin Xiong', 'Yuyang Song', 'Tianyu Liu', 'Guoyin Wang', 'Houfeng Wang']}, {'id': 'http://arxiv.org/abs/2508.04604v1', 'title': 'TURA: Tool-Augmented Unified Retrieval Agent for AI Search', 'published': '2025-08-06T16:24:17Z', 'updated': '2025-08-06T16:24:17Z', 'abstract': 'The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.', 'authors': ['Zhejun Zhao', 'Yuehu Dong', 'Alley Liu', 'Lixue Zheng', 'Pingsheng Liu', 'Dongdong Shen', 'Long Xia', 'Jiashu Zhao', 'Dawei Yin']}, {'id': 'http://arxiv.org/abs/2508.04581v1', 'title': 'Share Your Attention: Transformer Weight Sharing via Matrix-based   Dictionary Learning', 'published': '2025-08-06T16:06:43Z', 'updated': '2025-08-06T16:06:43Z', 'abstract': "Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.", 'authors': ['Magauiya Zhussip', 'Dmitriy Shopkhoev', 'Ammar Ali', 'Stamatios Lefkimmiatis']}, {'id': 'http://arxiv.org/abs/2508.04563v1', 'title': 'SID: Benchmarking Guided Instruction Capabilities in STEM Education with   a Socratic Interdisciplinary Dialogues Dataset', 'published': '2025-08-06T15:49:26Z', 'updated': '2025-08-06T15:49:26Z', 'abstract': "Fostering students' abilities for knowledge integration and transfer in complex problem-solving scenarios is a core objective of modern education, and interdisciplinary STEM is a key pathway to achieve this, yet it requires expert guidance that is difficult to scale. While LLMs offer potential in this regard, their true capability for guided instruction remains unclear due to the lack of an effective evaluation benchmark. To address this, we introduce SID, the first benchmark designed to systematically evaluate the higher-order guidance capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our contributions include a large-scale dataset of 10,000 dialogue turns across 48 complex STEM projects, a novel annotation schema for capturing deep pedagogical features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline experiments confirm that even state-of-the-art LLMs struggle to execute effective guided dialogues that lead students to achieve knowledge integration and transfer. This highlights the critical value of our benchmark in driving the development of more pedagogically-aware LLMs.", 'authors': ['Mei Jiang', 'Houping Yue', 'Bingdong Li', 'Hao Hao', 'Ying Qian', 'Bo Jiang', 'Aimin Zhou']}, {'id': 'http://arxiv.org/abs/2508.04531v1', 'title': 'Unveiling the Landscape of Clinical Depression Assessment: From   Behavioral Signatures to Psychiatric Reasoning', 'published': '2025-08-06T15:13:24Z', 'updated': '2025-08-06T15:13:24Z', 'abstract': 'Depression is a widespread mental disorder that affects millions worldwide. While automated depression assessment shows promise, most studies rely on limited or non-clinically validated data, and often prioritize complex model design over real-world effectiveness. In this paper, we aim to unveil the landscape of clinical depression assessment. We introduce C-MIND, a clinical neuropsychiatric multimodal diagnosis dataset collected over two years from real hospital visits. Each participant completes three structured psychiatric tasks and receives a final diagnosis from expert clinicians, with informative audio, video, transcript, and functional near-infrared spectroscopy (fNIRS) signals recorded. Using C-MIND, we first analyze behavioral signatures relevant to diagnosis. We train a range of classical models to quantify how different tasks and modalities contribute to diagnostic performance, and dissect the effectiveness of their combinations. We then explore whether LLMs can perform psychiatric reasoning like clinicians and identify their clear limitations in realistic clinical settings. In response, we propose to guide the reasoning process with clinical expertise and consistently improves LLM diagnostic performance by up to 10% in Macro-F1 score. We aim to build an infrastructure for clinical depression assessment from both data and algorithmic perspectives, enabling C-MIND to facilitate grounded and reliable research for mental healthcare.', 'authors': ['Zhuang Chen', 'Guanqun Bi', 'Wen Zhang', 'Jiawei Hu', 'Aoyun Wang', 'Xiyao Xiao', 'Kun Feng', 'Minlie Huang']}, {'id': 'http://arxiv.org/abs/2508.04530v2', 'title': 'Balancing Stylization and Truth via Disentangled Representation Steering', 'published': '2025-08-06T15:12:05Z', 'updated': '2025-08-07T06:14:50Z', 'abstract': "Generating stylized large language model (LLM) responses via representation editing is a promising way for fine-grained output control. However, there exists an inherent trade-off: imposing a distinctive style often degrades truthfulness. Existing representation editing methods, by naively injecting style signals, overlook this collateral impact and frequently contaminate the model's core truthfulness representations, resulting in reduced answer correctness. We term this phenomenon stylization-induced truthfulness collapse. We attribute this issue to latent coupling between style and truth directions in certain key attention heads, and propose StyliTruth, a mechanism that preserves stylization while keeping truthfulness intact. StyliTruth separates the style-relevant and truth-relevant subspaces in the model's representation space via an orthogonal deflation process. This decomposition enables independent control of style and truth in their own subspaces, minimizing interference. By designing adaptive, token-level steering vectors within each subspace, we dynamically and precisely control the generation process to maintain both stylistic fidelity and truthfulness. We validate our method on multiple styles and languages. Extensive experiments and analyses show that StyliTruth significantly reduces stylization-induced truthfulness collapse and outperforms existing inference-time intervention methods in balancing style adherence with truthfulness.", 'authors': ['Chenglei Shen', 'Zhongxiang Sun', 'Teng Shi', 'Xiao Zhang', 'Jun Xu']}, {'id': 'http://arxiv.org/abs/2508.04524v1', 'title': 'RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning   Framework for Explainable Deepfake Detection', 'published': '2025-08-06T15:08:16Z', 'updated': '2025-08-06T15:08:16Z', 'abstract': "The rapid advancement of AI-generation models has enabled the creation of hyperrealistic imagery, posing ethical risks through widespread misinformation. Current deepfake detection methods, categorized as face specific detectors or general AI-generated detectors, lack transparency by framing detection as a classification task without explaining decisions. While several LLM-based approaches offer explainability, they suffer from coarse-grained analyses and dependency on labor-intensive annotations. This paper introduces RAIDX (Retrieval-Augmented Image Deepfake Detection and Explainability), a novel deepfake detection framework integrating Retrieval-Augmented Generation (RAG) and Group Relative Policy Optimization (GRPO) to enhance detection accuracy and decision explainability. Specifically, RAIDX leverages RAG to incorporate external knowledge for improved detection accuracy and employs GRPO to autonomously generate fine-grained textual explanations and saliency maps, eliminating the need for extensive manual annotations. Experiments on multiple benchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and providing interpretable rationales in both textual descriptions and saliency maps, achieving state-of-the-art detection performance while advancing transparency in deepfake identification. RAIDX represents the first unified framework to synergize RAG and GRPO, addressing critical gaps in accuracy and explainability. Our code and models will be publicly available.", 'authors': ['Tianxiao Li', 'Zhenglin Huang', 'Haiquan Wen', 'Yiwei He', 'Shuchang Lyu', 'Baoyuan Wu', 'Guangliang Cheng']}, {'id': 'http://arxiv.org/abs/2508.04495v1', 'title': 'Causal Reflection with Language Models', 'published': '2025-08-06T14:44:23Z', 'updated': '2025-08-06T14:44:23Z', 'abstract': "While LLMs exhibit impressive fluency and factual recall, they struggle with robust causal reasoning, often relying on spurious correlations and brittle patterns. Similarly, traditional Reinforcement Learning agents also lack causal understanding, optimizing for rewards without modeling why actions lead to outcomes. We introduce Causal Reflection, a framework that explicitly models causality as a dynamic function over state, action, time, and perturbation, enabling agents to reason about delayed and nonlinear effects. Additionally, we define a formal Reflect mechanism that identifies mismatches between predicted and observed outcomes and generates causal hypotheses to revise the agent's internal model. In this architecture, LLMs serve not as black-box reasoners, but as structured inference engines translating formal causal outputs into natural language explanations and counterfactuals. Our framework lays the theoretical groundwork for Causal Reflective agents that can adapt, self-correct, and communicate causal understanding in evolving environments.", 'authors': ['Abi Aryan', 'Zac Liu']}, {'id': 'http://arxiv.org/abs/2508.04482v1', 'title': 'OS Agents: A Survey on MLLM-based Agents for General Computing Devices   Use', 'published': '2025-08-06T14:33:45Z', 'updated': '2025-08-06T14:33:45Z', 'abstract': 'The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.', 'authors': ['Xueyu Hu', 'Tao Xiong', 'Biao Yi', 'Zishu Wei', 'Ruixuan Xiao', 'Yurun Chen', 'Jiasheng Ye', 'Meiling Tao', 'Xiangxin Zhou', 'Ziyu Zhao', 'Yuhuai Li', 'Shengze Xu', 'Shenzhi Wang', 'Xinchen Xu', 'Shuofei Qiao', 'Zhaokai Wang', 'Kun Kuang', 'Tieyong Zeng', 'Liang Wang', 'Jiwei Li', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou', 'Guoyin Wang', 'Keting Yin', 'Zhou Zhao', 'Hongxia Yang', 'Fan Wu', 'Shengyu Zhang', 'Fei Wu']}, {'id': 'http://arxiv.org/abs/2508.04474v1', 'title': 'TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large   Language Models', 'published': '2025-08-06T14:25:05Z', 'updated': '2025-08-06T14:25:05Z', 'abstract': 'Recent advances in large language models (LLMs) have unlocked powerful reasoning and decision-making capabilities. However, their inherent dependence on static parametric memory fundamentally limits their adaptability, factual accuracy, and interpretability in knowledge-intensive scenarios. Knowledge graphs (KGs), as structured repositories of explicit relational knowledge, offer a promising approach for augmenting LLMs with external, interpretable memory. Nevertheless, most existing methods that combine LLMs with KGs treat reasoning and knowledge updating as separate processes, resulting in suboptimal utilization of new information and hindering real-time updates. In this work, we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And Incremental Learning that couples joint inference and dynamic KG refinement with large language models. TRAIL enables LLM agents to iteratively explore, update, and refine knowledge graphs during the reasoning process, employing a confidence-driven mechanism for the generation, validation, and pruning of new facts. This plug-and-play architecture facilitates seamless integration with various LLMs, supporting continual adaptation without the need for retraining. Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms existing KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More importantly, these results represent a significant step toward developing adaptive, memory-augmented language models capable of continual learning and reliable, transparent reasoning.', 'authors': ['Xinkui Zhao', 'Haode Li', 'Yifan Zhang', 'Guanjie Cheng', 'Yueshen Xu']}, {'id': 'http://arxiv.org/abs/2508.04462v1', 'title': 'CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large   Language Model Inference', 'published': '2025-08-06T14:02:10Z', 'updated': '2025-08-06T14:02:10Z', 'abstract': "Speculative decoding (SD), where an extra draft model first provides multiple draft tokens and the original target model then verifies these tokens in parallel, has shown great power for LLM inference acceleration. However, existing SD methods must adhere to the 'draft-then-verify' paradigm, which forces drafting and verification processes to execute sequentially during SD, resulting in inefficient inference performance and limiting the size of the draft model. Furthermore, once a single token in the candidate sequence is rejected during the drafting process, all subsequent candidate tokens must be discarded, leading to inefficient drafting. To address these challenges, we propose a cache-based parallel speculative decoding framework employing a 'query-and-correct' paradigm. Specifically, CARD decouples drafting and verification: the draft model generates candidate tokens to populate a shared cache, while the target model concurrently rectifies the draft model's generation direction. This effectively enables the target model to perform inference at speed approaching that of the draft model. Our approach achieves up to 4.83 speedup over vanilla decoding without requiring fine-tuning of either the draft or target models. Our code is available at https://github.com/hunzhizi/CARD.", 'authors': ['Enyu Zhou', 'Kai Sheng', 'Hao Chen', 'Xin He']}, {'id': 'http://arxiv.org/abs/2508.04460v1', 'title': 'From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive   Reasoning in Large Reasoning Models via Decoupled Reasoning and Control', 'published': '2025-08-06T13:59:17Z', 'updated': '2025-08-06T13:59:17Z', 'abstract': 'Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step reasoning, reflection, and backtracking, commonly referred to as "Aha Moments". However, such emergent behaviors remain unregulated and uncontrolled, often resulting in overthinking, where the model continues generating redundant reasoning content even after reaching reliable conclusions. This leads to excessive computational costs and increased latency, limiting the practical deployment of LRMs. The root cause lies in the absence of intrinsic regulatory mechanisms, as current models are unable to monitor and adaptively manage their reasoning process to determine when to continue, backtrack, or terminate. To address this issue, we propose the Meta-cognitive Reasoning Framework (MERA), which explicitly decouples the thinking process into distinct reasoning and control components, thereby enabling the independent optimization of control strategies. Specifically, MERA incorporates a takeover-based data construction mechanism that identifies critical decision points during reasoning and delegates the creation of control signals to auxiliary LLMs, thereby enabling the construction of high-quality reasoning-control data. Additionally, a structured reasoning-control separation is implemented via supervised fine-tuning, enabling the model to generate explicit traces and acquire initial meta-cognitive control capabilities. Finally, MERA employs Control-Segment Policy Optimization (CSPO), which combines segment-wise Group Relative Policy Optimization (GRPO) with a control-masking mechanism to optimize control behavior learning while minimizing interference from irrelevant content. Experiments on various reasoning benchmarks demonstrate that models trained with MERA enhance both reasoning efficiency and accuracy.', 'authors': ['Rui Ha', 'Chaozhuo Li', 'Rui Pu', 'Sen Su']}, {'id': 'http://arxiv.org/abs/2508.04451v1', 'title': 'Automatic LLM Red Teaming', 'published': '2025-08-06T13:52:00Z', 'updated': '2025-08-06T13:52:00Z', 'abstract': "Red teaming is critical for identifying vulnerabilities and building trust in current LLMs. However, current automated methods for Large Language Models (LLMs) rely on brittle prompt templates or single-turn attacks, failing to capture the complex, interactive nature of real-world adversarial dialogues. We propose a novel paradigm: training an AI to strategically `break' another AI. By formalizing red teaming as a Markov Decision Process (MDP) and employing a hierarchical Reinforcement Learning (RL) framework, we effectively address the inherent sparse reward and long-horizon challenges. Our generative agent learns coherent, multi-turn attack strategies through a fine-grained, token-level harm reward, enabling it to uncover subtle vulnerabilities missed by existing baselines. This approach sets a new state-of-the-art, fundamentally reframing LLM red teaming as a dynamic, trajectory-based process (rather than a one-step test) essential for robust AI deployment.", 'authors': ['Roman Belaire', 'Arunesh Sinha', 'Pradeep Varakantham']}, {'id': 'http://arxiv.org/abs/2508.04448v1', 'title': 'Large Language Models Versus Static Code Analysis Tools: A Systematic   Benchmark for Vulnerability Detection', 'published': '2025-08-06T13:48:38Z', 'updated': '2025-08-06T13:48:38Z', 'abstract': "Modern software relies on a multitude of automated testing and quality assurance tools to prevent errors, bugs and potential vulnerabilities. This study sets out to provide a head-to-head, quantitative and qualitative evaluation of six automated approaches: three industry-standard rule-based static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three state-of-the-art large language models hosted on the GitHub Models platform (GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten real-world C# projects that embed 63 vulnerabilities across common categories such as SQL injection, hard-coded secrets and outdated dependencies, we measure classical detection accuracy (precision, recall, F-score), analysis latency, and the developer effort required to vet true positives. The language-based scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs' advantage originates from superior recall, confirming an ability to reason across broader code contexts. However, this benefit comes with substantial trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language models mislocate issues at line-or-column granularity due to tokenisation artefacts. Overall, language models successfully rival traditional static analysers in finding real vulnerabilities. Still, their noisier output and imprecise localisation limit their standalone use in safety-critical audits. We therefore recommend a hybrid pipeline: employ language models early in development for broad, context-aware triage, while reserving deterministic rule-based scanners for high-assurance verification. The open benchmark and JSON-based result harness released with this paper lay a foundation for reproducible, practitioner-centric research into next-generation automated code security.", 'authors': ['Damian Gnieciak', 'Tomasz Szandala']}, {'id': 'http://arxiv.org/abs/2508.04755v1', 'title': 'Are Large Language Models Dynamic Treatment Planners? An In Silico Study   from a Prior Knowledge Injection Angle', 'published': '2025-08-06T13:46:02Z', 'updated': '2025-08-06T13:46:02Z', 'abstract': "Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold promise for automating complex clinical decision-making, yet their practical deployment remains hindered by the intensive engineering required to inject clinical knowledge and ensure patient safety. Recent advancements in large language models (LLMs) suggest a complementary approach, where implicit prior knowledge and clinical heuristics are naturally embedded through linguistic prompts without requiring environment-specific training. In this study, we rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in silico Type 1 diabetes simulator, comparing their zero-shot inference performance against small neural network-based RL agents (SRAs) explicitly trained for the task. Our results indicate that carefully designed zero-shot prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or superior clinical performance relative to extensively trained SRAs, particularly in stable patient cohorts. However, LLMs exhibit notable limitations, such as overly aggressive insulin dosing when prompted with chain-of-thought (CoT) reasoning, highlighting critical failure modes including arithmetic hallucination, temporal misinterpretation, and inconsistent clinical logic. Incorporating explicit reasoning about latent clinical states (e.g., meals) yielded minimal performance gains, underscoring the current model's limitations in capturing complex, hidden physiological dynamics solely through textual inference. Our findings advocate for cautious yet optimistic integration of LLMs into clinical workflows, emphasising the necessity of targeted prompt engineering, careful validation, and potentially hybrid approaches that combine linguistic reasoning with structured physiological modelling to achieve safe, robust, and clinically effective decision-support systems.", 'authors': ['Zhiyao Luo', 'Tingting Zhu']}, {'id': 'http://arxiv.org/abs/2508.04440v1', 'title': 'StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs   through Knowledge-Reasoning Fusion', 'published': '2025-08-06T13:28:22Z', 'updated': '2025-08-06T13:28:22Z', 'abstract': 'Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.', 'authors': ['Yutong Wu', 'Di Huang', 'Ruosi Wan', 'Yue Peng', 'Shijie Shang', 'Chenrui Cao', 'Lei Qi', 'Rui Zhang', 'Zidong Du', 'Jie Yan', 'Xing Hu']}, {'id': 'http://arxiv.org/abs/2508.04428v1', 'title': '\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding   Dialogues Between Experts and LLM-Simulated Novices', 'published': '2025-08-06T13:16:10Z', 'updated': '2025-08-06T13:16:10Z', 'abstract': "High-quality, multi-turn instructional dialogues between novices and experts are essential for developing AI systems that support teaching, learning, and decision-making. These dialogues often involve scaffolding -- the process by which an expert supports a novice's thinking through questions, feedback, and step-by-step guidance. However, such data are scarce due to privacy concerns in recording and the vulnerability inherent in help-seeking. We present SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding dialogues. Using teaching development coaching as an example domain, SimInstruct simulates novice instructors via LLMs, varying their teaching challenges and LLM's persona traits, while human experts provide multi-turn feedback, reasoning, and instructional support. This design enables the creation of realistic, pedagogically rich dialogues without requiring real novice participants. Our results reveal that persona traits, such as extroversion and introversion, meaningfully influence how experts engage. Compared to real mentoring recordings, SimInstruct dialogues demonstrate comparable pedagogical relevance and cognitive depth. Experts also reported the process as engaging and reflective, improving both data quality and their own professional insight. We further fine-tuned a LLaMA model to be an expert model using the augmented dataset, which outperformed GPT-4o in instructional quality. Our analysis highlights GPT-4o's limitations in weak reflective questioning, overuse of generic praise, a condescending tone, and a tendency to overwhelm novices with excessive suggestions.", 'authors': ['Si Chen', 'Izzy Molnar', 'Ting Hua', 'Peiyu Li', 'Le Huy Khiem', 'G. Alex Ambrose', 'Jim Lang', 'Ronald Metoyer', 'Nitesh V. Chawla']}, {'id': 'http://arxiv.org/abs/2508.04423v1', 'title': 'Evaluating, Synthesizing, and Enhancing for Customer Support   Conversation', 'published': '2025-08-06T13:11:17Z', 'updated': '2025-08-06T13:11:17Z', 'abstract': 'Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.', 'authors': ['Jie Zhu', 'Huaixia Dou', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong']}, {'id': 'http://arxiv.org/abs/2508.04412v1', 'title': 'Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents', 'published': '2025-08-06T12:56:54Z', 'updated': '2025-08-06T12:56:54Z', 'abstract': "Frontier LLMs only recently enabled serviceable, autonomous web agents. At that, a model poses as an instantaneous domain model backend. Ought to suggest interaction, it is consulted with a web-based task and respective application state. The key problem lies in application state serialisation $\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are premised on grounded GUI snapshots, i.e., screenshots enhanced with visual cues. Not least to resemble human perception, but for images representing relatively cheap means of model input. LLM vision still lag behind code interpretation capabilities. DOM snapshots, which structurally resemble HTML, impose a desired alternative. Vast model input token size, however, disables reliable implementation with web agents to date.   We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a grounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input token order of magnitude (1e3). Our best evaluated configurations $\\unicode{x2013}$ one token order above, but within the model's context window $\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.", 'authors': ['Thassilo M. Schiepanski', 'Nicholas Piël']}, {'id': 'http://arxiv.org/abs/2508.04405v1', 'title': 'FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via   Algorithm-System Co-Design', 'published': '2025-08-06T12:47:05Z', 'updated': '2025-08-06T12:47:05Z', 'abstract': 'Large Language Models (LLMs) demonstrate exceptional performance but entail significant memory and computational costs, restricting their practical deployment. While existing INT4/INT8 quantization reduces these costs, they often degrade accuracy or lack optimal efficiency. INT6 quantization offers a superior trade-off between model accuracy and inference efficiency, but lacks hardware support in modern GPUs, forcing emulation via higher-precision arithmetic units that limit acceleration.   In this paper, we propose FlexQ, a novel post-training INT6 quantization framework combining algorithmic innovation with system-level optimizations. FlexQ employs uniform 6-bit weight quantization across all layers, with adaptive retention of 8-bit activations in layers identified through layer-wise sensitivity analysis. To maximize hardware efficiency, we develop a specialized high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core (BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ maintains near-FP16 accuracy, with perplexity increases of no more than 0.05. The proposed kernel achieves an average 1.39$\\times$ speedup over ABQ-LLM on LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference acceleration and 1.21$\\times$ memory savings over SmoothQuant. Code is released at https://github.com/FlyFoxPlayer/FlexQ.', 'authors': ['Hao Zhang', 'Aining Jia', 'Weifeng Bu', 'Yushu Cai', 'Kai Sheng', 'Hao Chen', 'Xin He']}, {'id': 'http://arxiv.org/abs/2508.04401v1', 'title': "Why are LLMs' abilities emergent?", 'published': '2025-08-06T12:43:04Z', 'updated': '2025-08-06T12:43:04Z', 'abstract': 'The remarkable success of Large Language Models (LLMs) in generative tasks has raised fundamental questions about the nature of their acquired capabilities, which often appear to emerge unexpectedly without explicit training. This paper examines the emergent properties of Deep Neural Networks (DNNs) through both theoretical analysis and empirical observation, addressing the epistemological challenge of "creation without understanding" that characterises contemporary AI development. We explore how the neural approach\'s reliance on nonlinear, stochastic processes fundamentally differs from symbolic computational paradigms, creating systems whose macro-level behaviours cannot be analytically derived from micro-level neuron activities. Through analysis of scaling laws, grokking phenomena, and phase transitions in model capabilities, I demonstrate that emergent abilities arise from the complex dynamics of highly sensitive nonlinear systems rather than simply from parameter scaling alone. My investigation reveals that current debates over metrics, pre-training loss thresholds, and in-context learning miss the fundamental ontological nature of emergence in DNNs. I argue that these systems exhibit genuine emergent properties analogous to those found in other complex natural phenomena, where systemic capabilities emerge from cooperative interactions among simple components without being reducible to their individual behaviours. The paper concludes that understanding LLM capabilities requires recognising DNNs as a new domain of complex dynamical systems governed by universal principles of emergence, similar to those operating in physics, chemistry, and biology. This perspective shifts the focus from purely phenomenological definitions of emergence to understanding the internal dynamic transformations that enable these systems to acquire capabilities that transcend their individual components.', 'authors': ['Vladimír Havlík']}, {'id': 'http://arxiv.org/abs/2508.04399v1', 'title': 'Improving Crash Data Quality with Large Language Models: Evidence from   Secondary Crash Narratives in Kentucky', 'published': '2025-08-06T12:41:18Z', 'updated': '2025-08-06T12:41:18Z', 'abstract': 'This study evaluates advanced natural language processing (NLP) techniques to enhance crash data quality by mining crash narratives, using secondary crash identification in Kentucky as a case study. Drawing from 16,656 manually reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we compare three model classes: zero-shot open-source large language models (LLMs) (LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic regression as baseline. Models were calibrated on 2015-2021 data and tested on 1,771 narratives from 2022. Fine-tuned transformers achieved superior performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy (95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139 minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred high computational costs (up to 723 minutes for DeepSeek-R1:70B), while fine-tuned models processed the test set in seconds after brief training. Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can rival larger counterparts in performance while reducing runtime, suggesting opportunities for optimized deployments. Results highlight trade-offs between accuracy, efficiency, and data requirements, with fine-tuned transformer models balancing precision and recall effectively on Kentucky data. Practical deployment considerations emphasize privacy-preserving local deployment, ensemble approaches for improved accuracy, and incremental processing for scalability, providing a replicable scheme for enhancing crash-data quality with advanced NLP.', 'authors': ['Xu Zhang', 'Mei Chen']}, {'id': 'http://arxiv.org/abs/2508.04353v1', 'title': 'LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for   Learned Thematic Significance Tracking in Multimedia Content', 'published': '2025-08-06T11:48:51Z', 'updated': '2025-08-06T11:48:51Z', 'abstract': 'This paper introduces the Learned User Significance Tracker (LUST), a framework designed to analyze video content and quantify the thematic relevance of its segments in relation to a user-provided textual description of significance. LUST leverages a multi-modal analytical pipeline, integrating visual cues from video frames with textual information extracted via Automatic Speech Recognition (ASR) from the audio track. The core innovation lies in a hierarchical, two-stage relevance scoring mechanism employing Large Language Models (LLMs). An initial "direct relevance" score, $S_{d,i}$, assesses individual segments based on immediate visual and auditory content against the theme. This is followed by a "contextual relevance" score, $S_{c,i}$, that refines the assessment by incorporating the temporal progression of preceding thematic scores, allowing the model to understand evolving narratives. The LUST framework aims to provide a nuanced, temporally-aware measure of user-defined significance, outputting an annotated video with visualized relevance scores and comprehensive analytical logs.', 'authors': ['Anderson de Lima Luiz']}, {'id': 'http://arxiv.org/abs/2508.04350v1', 'title': 'Chain of Questions: Guiding Multimodal Curiosity in Language Models', 'published': '2025-08-06T11:42:54Z', 'updated': '2025-08-06T11:42:54Z', 'abstract': "Reasoning capabilities in large language models (LLMs) have substantially advanced through methods such as chain-of-thought and explicit step-by-step explanations. However, these improvements have not yet fully transitioned to multimodal contexts, where models must proactively decide which sensory modalities such as vision, audio, or spatial perception to engage when interacting with complex real-world environments. In this paper, we introduce the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach that encourages multimodal language models to dynamically generate targeted questions regarding their surroundings. These generated questions guide the model to selectively activate relevant modalities, thereby gathering critical information necessary for accurate reasoning and response generation. We evaluate our framework on a novel multimodal benchmark dataset, assembled by integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results demonstrate that our CoQ method improves a foundation model's ability to effectively identify and integrate pertinent sensory information. This leads to improved accuracy, interpretability, and alignment of the reasoning process with diverse multimodal tasks.", 'authors': ['Nima Iji', 'Kia Dashtipour']}, {'id': 'http://arxiv.org/abs/2508.04349v1', 'title': 'GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy   Entropy', 'published': '2025-08-06T11:42:47Z', 'updated': '2025-08-06T11:42:47Z', 'abstract': 'Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \\textbf{Group Token Policy Optimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group Relative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models.', 'authors': ['Hongze Tan', 'Jianfei Pan']}, {'id': 'http://arxiv.org/abs/2508.04339v1', 'title': 'Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for   Belief-Tracked Inference with Pretrained Language Models', 'published': '2025-08-06T11:33:35Z', 'updated': '2025-08-06T11:33:35Z', 'abstract': 'Large language models often fail at logical reasoning when semantic heuristics conflict with decisive evidence - a phenomenon we term cognitive traps. To address this fundamental limitation, we introduce the Deliberative Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from probability maximization to uncertainty minimization. Instead of asking "Which answer is most likely?", DRN asks "Which hypothesis has the most internally consistent evidence?". DRN achieves intrinsic interpretability by explicitly tracking belief states and quantifying epistemic uncertainty for competing hypotheses through an iterative evidence synthesis process. We validate our approach through two complementary architectures - a bespoke discriminative model that embodies the core uncertainty minimization principle, and a lightweight verification module that enhances existing generative LLMs. Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over standard baselines. When integrated as a parameter-efficient verifier with Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most challenging problems. Critically, DRN demonstrates strong zero-shot generalization, improving TruthfulQA performance by 23.6% without additional training, indicating that uncertainty-driven deliberation learns transferable reasoning principles. We position DRN as a foundational, verifiable System 2 reasoning component for building more trustworthy AI systems.', 'authors': ['Anran Xu', 'Jincheng Wang', 'Baigen Cai', 'Tao Wen']}, {'id': 'http://arxiv.org/abs/2508.04337v1', 'title': 'Modelling and Classifying the Components of a Literature Review', 'published': '2025-08-06T11:30:07Z', 'updated': '2025-08-06T11:30:07Z', 'abstract': 'Previous work has demonstrated that AI methods for analysing scientific literature benefit significantly from annotating sentences in papers according to their rhetorical roles, such as research gaps, results, limitations, extensions of existing methodologies, and others. Such representations also have the potential to support the development of a new generation of systems capable of producing high-quality literature reviews. However, achieving this goal requires the definition of a relevant annotation schema and effective strategies for large-scale annotation of the literature. This paper addresses these challenges by 1) introducing a novel annotation schema specifically designed to support literature review generation and 2) conducting a comprehensive evaluation of a wide range of state-of-the-art large language models (LLMs) in classifying rhetorical roles according to this schema. To this end, we also present Sci-Sentence, a novel multidisciplinary benchmark comprising 700 sentences manually annotated by domain experts and 2,240 sentences automatically labelled using LLMs. We evaluate 37 LLMs on this benchmark, spanning diverse model families and sizes, using both zero-shot learning and fine-tuning approaches. The experiments yield several novel insights that advance the state of the art in this challenging domain. First, the current generation of LLMs performs remarkably well on this task when fine-tuned on high-quality data, achieving performance levels above 96\\% F1. Second, while large proprietary models like GPT-4o achieve the best results, some lightweight open-source alternatives also demonstrate excellent performance. Finally, enriching the training data with semi-synthetic examples generated by LLMs proves beneficial, enabling small encoders to achieve robust results and significantly enhancing the performance of several open decoder models.', 'authors': ['Francisco Bolaños', 'Angelo Salatino', 'Francesco Osborne', 'Enrico Motta']}, {'id': 'http://arxiv.org/abs/2508.04329v2', 'title': 'Forgetting: A New Mechanism Towards Better Large Language Model   Fine-tuning', 'published': '2025-08-06T11:22:23Z', 'updated': '2025-08-07T08:30:41Z', 'abstract': 'Supervised fine-tuning (SFT) plays a critical role for pretrained large language models (LLMs), notably enhancing their capacity to acquire domain-specific knowledge while preserving or potentially augmenting their general-purpose capabilities. However, the efficacy of SFT hinges on data quality as well as data volume, otherwise it may result in limited performance gains or even degradation relative to the associated baselines. To mitigate such reliance, we suggest categorizing tokens within each corpus into two parts -- positive and negative tokens -- based on whether they are useful to improve model performance. Positive tokens can be trained in common ways, whereas negative tokens, which may lack essential semantics or be misleading, should be explicitly forgotten. Overall, the token categorization facilitate the model to learn less informative message, and the forgetting process shapes a knowledge boundary to guide the model on what information to learn more precisely. We conduct experiments on well-established benchmarks, finding that this forgetting mechanism not only improves overall model performance and also facilitate more diverse model responses.', 'authors': ['Ali Taheri Ghahrizjani', 'Alireza Taban', 'Qizhou Wang', 'Shanshan Ye', 'Abdolreza Mirzaei', 'Tongliang Liu', 'Bo Han']}, {'id': 'http://arxiv.org/abs/2508.04325v1', 'title': 'Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language   Models', 'published': '2025-08-06T11:11:40Z', 'updated': '2025-08-06T11:11:40Z', 'abstract': "Large language models (LLMs) show significant potential in healthcare, prompting numerous benchmarks to evaluate their capabilities. However, concerns persist regarding the reliability of these benchmarks, which often lack clinical fidelity, robust data management, and safety-oriented evaluation metrics. To address these shortcomings, we introduce MedCheck, the first lifecycle-oriented assessment framework specifically designed for medical benchmarks. Our framework deconstructs a benchmark's development into five continuous stages, from design to governance, and provides a comprehensive checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis uncovers widespread, systemic issues, including a profound disconnect from clinical practice, a crisis of data integrity due to unmitigated contamination risks, and a systematic neglect of safety-critical evaluation dimensions like model robustness and uncertainty awareness. Based on these findings, MedCheck serves as both a diagnostic tool for existing benchmarks and an actionable guideline to foster a more standardized, reliable, and transparent approach to evaluating AI in healthcare.", 'authors': ['Zizhan Ma', 'Wenxuan Wang', 'Guo Yu', 'Yiu-Fai Cheung', 'Meidan Ding', 'Jie Liu', 'Wenting Chen', 'Linlin Shen']}, {'id': 'http://arxiv.org/abs/2508.04306v1', 'title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding   Errors in Long-Form Literature Review Generation', 'published': '2025-08-06T10:45:52Z', 'updated': '2025-08-06T10:45:52Z', 'abstract': 'Literature reviews play an important role in scientific research. Recent advances in large language models (LLMs) have boosted the development of automated systems for the entire literature review workflow, from retrieval to manuscript drafting. However, a key challenge is that mistakes made in early stages can propagate and amplify in subsequent steps, leading to compounding errors that undermine the faithfulness of the final review. To tackle this issue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which consists of a manager agent and four executor agents for literature searching, outline generation, fact localization, and manuscript drafting. We propose three novel collaboration paradigms, forming exploration, exploitation, and experience taskforces, to effectively organize agents and mitigate compounding errors both between and within executor agents. Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks. We further propose a new benchmark dataset featuring more diverse topics for faithful literature review generation.', 'authors': ['Zhi Zhang', 'Yan Liu', 'Zhejing Hu', 'Gong Chen', 'Sheng-hua Zhong', 'Jiannong Cao']}, {'id': 'http://arxiv.org/abs/2508.04295v1', 'title': 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust   Translation', 'published': '2025-08-06T10:31:23Z', 'updated': '2025-08-06T10:31:23Z', 'abstract': "Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions.", 'authors': ['Chaofan Wang', 'Tingrui Yu', 'Jie Wang', 'Dong Chen', 'Wenrui Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Beijun Shen']}, {'id': 'http://arxiv.org/abs/2508.04289v2', 'title': 'Method-Based Reasoning for Large Language Models: Extraction, Reuse, and   Continuous Improvement', 'published': '2025-08-06T10:26:52Z', 'updated': '2025-08-07T04:14:31Z', 'abstract': "Large language models (LLMs) have shown impressive capabilities across a wide range of language tasks. However, their reasoning process is primarily guided by statistical patterns in training data, which limits their ability to handle novel problems and perform consistent logical reasoning. In this paper, we propose a method-based model that enhances LLMs with explicit, reusable procedures extracted from training content, generated responses, and user interactions. Each method is represented as a pair consisting of a problem and its corresponding solution, stored externally and ranked based on feedback. When a new query is received, the system retrieves and applies the most relevant methods to guide the LLM's response. Our model enables continual learning, method reuse, and logical consistency beyond next-token prediction. Experimental results demonstrate that the system improves factual verification and generalization in complex prompts, and that newly learned methods can outperform earlier ones through user-driven refinement.", 'authors': ['Hong Su']}, {'id': 'http://arxiv.org/abs/2508.04281v1', 'title': 'Prompt Injection Vulnerability of Consensus Generating Applications in   Digital Democracy', 'published': '2025-08-06T10:10:01Z', 'updated': '2025-08-06T10:10:01Z', 'abstract': 'Large Language Models (LLMs) are gaining traction as a method to generate consensus statements and aggregate preferences in digital democracy experiments. Yet, LLMs may introduce critical vulnerabilities in these systems. Here, we explore the impact of prompt-injection attacks targeting consensus generating systems by introducing a four-dimensional taxonomy of attacks. We test these attacks using LLaMA 3.1 8B and Chat GPT 4.1 Nano finding the LLMs more vulnerable to criticism attacks -- attacks using disagreeable prompts -- and more effective at tilting ambiguous consensus statements. We also find evidence of more effective manipulation when using explicit imperatives and rational-sounding arguments compared to emotional language or fabricated statistics. To mitigate these vulnerabilities, we apply Direct Preference Optimization (DPO), an alignment method that fine-tunes LLMs to prefer unperturbed consensus statements. While DPO significantly improves robustness, it still offers limited protection against attacks targeting ambiguous consensus. These results advance our understanding of the vulnerability and robustness of consensus generating LLMs in digital democracy applications.', 'authors': ['Jairo Gudiño-Rosero', 'Clément Contet', 'Umberto Grandi', 'César A. Hidalgo']}, {'id': 'http://arxiv.org/abs/2508.04280v1', 'title': 'Enhancing Vision-Language Model Training with Reinforcement Learning in   Synthetic Worlds for Real-World Success', 'published': '2025-08-06T10:08:48Z', 'updated': '2025-08-06T10:08:48Z', 'abstract': 'Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\\% relative on BALROG (game-centric agentic control), +5\\% relative on the hardest part of VSI-Bench (spatial planning), and +2\\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks.', 'authors': ['George Bredis', 'Stanislav Dereka', 'Viacheslav Sinii', 'Ruslan Rakhimov', 'Daniil Gavrilov']}, {'id': 'http://arxiv.org/abs/2508.04279v1', 'title': 'Mockingbird: How does LLM perform in general machine learning tasks?', 'published': '2025-08-06T10:08:47Z', 'updated': '2025-08-06T10:08:47Z', 'abstract': 'Large language models (LLMs) are now being used with increasing frequency as chat bots, tasked with the summarizing information or generating text and code in accordance with user instructions. The rapid increase in reasoning capabilities and inference speed of LLMs has revealed their remarkable potential for applications extending beyond the domain of chat bots to general machine learning tasks. This work is conducted out of the curiosity about such potential. In this work, we propose a framework Mockingbird to adapt LLMs to general machine learning tasks and evaluate its performance and scalability on several general machine learning tasks. The core concept of this framework is instructing LLMs to role-play functions and reflect on its mistakes to improve itself. Our evaluation and analysis result shows that LLM-driven machine learning methods, such as Mockingbird, can achieve acceptable results on common machine learning tasks; however, solely reflecting on its own currently cannot outperform the effect of domain-specific documents and feedback from human experts.', 'authors': ['Haoyu Jia', 'Yoshiki Obinata', 'Kento Kawaharazuka', 'Kei Okada']}, {'id': 'http://arxiv.org/abs/2508.04276v1', 'title': 'A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on   Graph-based Retrieval-Augmented Generation of Large Language Models', 'published': '2025-08-06T10:01:26Z', 'updated': '2025-08-06T10:01:26Z', 'abstract': 'Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as a promising paradigm for enhancing large language models (LLMs) by converting raw text into structured knowledge graphs, improving both accuracy and explainability. However, GraphRAG relies on LLMs to extract knowledge from raw text during graph construction, and this process can be maliciously manipulated to implant misleading information. Targeting this attack surface, we propose two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a few words in the source text can significantly change the constructed graph, poison the GraphRAG, and severely mislead downstream reasoning. The first attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate vulnerable nodes in the generated graphs and rewrites the corresponding narratives with LLMs, achieving precise control over specific question-answering (QA) outcomes with a success rate of 93.1\\%, while keeping the poisoned text fluent and natural. The second attack, named Universal KPA (UKPA), exploits linguistic cues such as pronouns and dependency relations to disrupt the structural integrity of the generated graph by altering globally influential words. With fewer than 0.05\\% of full text modified, the QA accuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that state-of-the-art defense methods fail to detect these attacks, highlighting that securing GraphRAG pipelines against knowledge poisoning remains largely unexplored.', 'authors': ['Jiayi Wen', 'Tianxin Chen', 'Zhirun Zheng', 'Cheng Huang']}, {'id': 'http://arxiv.org/abs/2508.04266v1', 'title': 'ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for   LLM-based Agents', 'published': '2025-08-06T09:51:30Z', 'updated': '2025-08-06T09:51:30Z', 'abstract': 'Existing benchmarks in e-commerce primarily focus on basic user intents, such as finding or purchasing products. However, real-world users often pursue more complex goals, such as applying vouchers, managing budgets, and finding multi-products seller. To bridge this gap, we propose ShoppingBench, a novel end-to-end shopping benchmark designed to encompass increasingly challenging levels of grounded intent. Specifically, we propose a scalable framework to simulate user instructions based on various intents derived from sampled real-world products. To facilitate consistent and reliable evaluations, we provide a large-scale shopping sandbox that serves as an interactive simulated environment, incorporating over 2.5 million real-world products. Experimental results demonstrate that even state-of-the-art language agents (such as GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks, highlighting the significant challenges posed by our ShoppingBench. In addition, we propose a trajectory distillation strategy and leverage supervised fine-tuning, along with reinforcement learning on synthetic trajectories, to distill the capabilities of a large language agent into a smaller one. As a result, our trained agent achieves competitive performance compared to GPT-4.1.', 'authors': ['Jiangyuan Wang', 'Kejun Xiao', 'Qi Sun', 'Huaipeng Zhao', 'Tao Luo', 'Jiandong Zhang', 'Xiaoyi Zeng']}, {'id': 'http://arxiv.org/abs/2508.04257v1', 'title': 'KVSink: Understanding and Enhancing the Preservation of Attention Sinks   in KV Cache Quantization for LLMs', 'published': '2025-08-06T09:40:09Z', 'updated': '2025-08-06T09:40:09Z', 'abstract': 'Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.', 'authors': ['Zunhai Su', 'Kehong Yuan']}, {'id': 'http://arxiv.org/abs/2508.04251v1', 'title': 'T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head   Alignment and Residual Fusion', 'published': '2025-08-06T09:31:44Z', 'updated': '2025-08-06T09:31:44Z', 'abstract': 'Multivariate time series forecasting (MTSF) seeks to model temporal dynamics among variables to predict future trends. Transformer-based models and large language models (LLMs) have shown promise due to their ability to capture long-range dependencies and patterns. However, current methods often rely on rigid inductive biases, ignore intervariable interactions, or apply static fusion strategies that limit adaptability across forecast horizons. These limitations create bottlenecks in capturing nuanced, horizon-specific relationships in time-series data. To solve this problem, we propose T3Time, a novel trimodal framework consisting of time, spectral, and prompt branches, where the dedicated frequency encoding branch captures the periodic structures along with a gating mechanism that learns prioritization between temporal and spectral features based on the prediction horizon. We also proposed a mechanism which adaptively aggregates multiple cross-modal alignment heads by dynamically weighting the importance of each head based on the features. Extensive experiments on benchmark datasets demonstrate that our model consistently outperforms state-of-the-art baselines, achieving an average reduction of 3.28% in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in few-shot learning settings: with 5% training data, we see a reduction in MSE and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98% on average. Code - https://github.com/monaf-chowdhury/T3Time/', 'authors': ['Abdul Monaf Chowdhury', 'Rabeya Akter', 'Safaeid Hossain Arib']}, {'id': 'http://arxiv.org/abs/2508.04248v1', 'title': 'TalkDep: Clinically Grounded LLM Personas for Conversation-Centric   Depression Screening', 'published': '2025-08-06T09:30:47Z', 'updated': '2025-08-06T09:30:47Z', 'abstract': 'The increasing demand for mental health services has outpaced the availability of real training data to develop clinical professionals, leading to limited support for the diagnosis of depression. This shortage has motivated the development of simulated or virtual patients to assist in training and evaluation, but existing approaches often fail to generate clinically valid, natural, and diverse symptom presentations. In this work, we embrace the recent advanced language models as the backbone and propose a novel clinician-in-the-loop patient simulation pipeline, TalkDep, with access to diversified patient profiles to develop simulated patients. By conditioning the model on psychiatric diagnostic criteria, symptom severity scales, and contextual factors, our goal is to create authentic patient responses that can better support diagnostic model training and evaluation. We verify the reliability of these simulated patients with thorough assessments conducted by clinical professionals. The availability of validated simulated patients offers a scalable and adaptable resource for improving the robustness and generalisability of automatic depression diagnosis systems.', 'authors': ['Xi Wang', 'Anxo Perez', 'Javier Parapar', 'Fabio Crestani']}, {'id': 'http://arxiv.org/abs/2508.04240v1', 'title': 'ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and   Neural Decoding during Reading and Listening', 'published': '2025-08-06T09:25:05Z', 'updated': '2025-08-06T09:25:05Z', 'abstract': 'EEG-based neural decoding requires large-scale benchmark datasets. Paired brain-language data across speaking, listening, and reading modalities are essential for aligning neural activity with the semantic representation of large language models (LLMs). However, such datasets are rare, especially for non-English languages. Here, we present ChineseEEG-2, a high-density EEG dataset designed for benchmarking neural decoding models under real-world language tasks. Building on our previous ChineseEEG dataset, which focused on silent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and Passive Listening (PL), using the same Chinese corpus. EEG and audio were simultaneously recorded from four participants during ~10.7 hours of reading aloud. These recordings were then played to eight other participants, collecting ~21.6 hours of EEG during listening. This setup enables speech temporal and semantic alignment across the RA and PL modalities. ChineseEEG-2 includes EEG signals, precise audio, aligned semantic embeddings from pre-trained language models, and task labels. Together with ChineseEEG, this dataset supports joint semantic alignment learning across speaking, listening, and reading. It enables benchmarking of neural decoding algorithms and promotes brain-LLM alignment under multimodal language tasks, especially in Chinese. ChineseEEG-2 provides a benchmark dataset for next-generation neural semantic decoding.', 'authors': ['Sitong Chen', 'Beiqianyi Li', 'Cuilin He', 'Dongyang Li', 'Mingyang Wu', 'Xinke Shen', 'Song Wang', 'Xuetao Wei', 'Xindi Wang', 'Haiyan Wu', 'Quanying Liu']}, {'id': 'http://arxiv.org/abs/2508.04231v1', 'title': 'Empowering Time Series Forecasting with LLM-Agents', 'published': '2025-08-06T09:14:08Z', 'updated': '2025-08-06T09:14:08Z', 'abstract': 'Large Language Model (LLM) powered agents have emerged as effective planners for Automated Machine Learning (AutoML) systems. While most existing AutoML approaches focus on automating feature engineering and model architecture search, recent studies in time series forecasting suggest that lightweight models can often achieve state-of-the-art performance. This observation led us to explore improving data quality, rather than model architecture, as a potentially fruitful direction for AutoML on time series data. We propose DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata accompanying time series to clean data while optimizing forecasting performance. We evaluated DCATS using four time series forecasting models on a large-scale traffic volume forecasting dataset. Results demonstrate that DCATS achieves an average 6% error reduction across all tested models and time horizons, highlighting the potential of data-centric approaches in AutoML for time series forecasting.', 'authors': ['Chin-Chia Michael Yeh', 'Vivian Lai', 'Uday Singh Saini', 'Xiran Fan', 'Yujie Fan', 'Junpeng Wang', 'Xin Dai', 'Yan Zheng']}, {'id': 'http://arxiv.org/abs/2508.04219v1', 'title': 'Hierarchical Text Classification Using Black Box Large Language Models', 'published': '2025-08-06T08:53:50Z', 'updated': '2025-08-06T08:53:50Z', 'abstract': 'Hierarchical Text Classification (HTC) aims to assign texts to structured label hierarchies; however, it faces challenges due to data scarcity and model complexity. This study explores the feasibility of using black box Large Language Models (LLMs) accessed via APIs for HTC, as an alternative to traditional machine learning methods that require extensive labeled data and computational resources. We evaluate three prompting strategies -- Direct Leaf Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and few-shot settings, comparing the accuracy and cost-effectiveness of these strategies. Experiments on two datasets show that a few-shot setting consistently improves classification accuracy compared to a zero-shot setting. While a traditional machine learning model achieves high accuracy on a dataset with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the machine learning model on a dataset with a deeper hierarchy. API costs increase significantly due to the higher input tokens required for deeper label hierarchies on DH strategy. These results emphasize the trade-off between accuracy improvement and the computational cost of prompt strategy. These findings highlight the potential of black box LLMs for HTC while underscoring the need to carefully select a prompt strategy to balance performance and cost.', 'authors': ['Kosuke Yoshimura', 'Hisashi Kashima']}, {'id': 'http://arxiv.org/abs/2508.04749v1', 'title': 'Bridging Brains and Models: MoE-Based Functional Lesions for Simulating   and Rehabilitating Aphasia', 'published': '2025-08-06T08:52:03Z', 'updated': '2025-08-06T08:52:03Z', 'abstract': "The striking alignment between large language models (LLMs) and human brain activity positions them as powerful models of healthy cognition. This parallel raises a fundamental question: if LLMs can model the intact brain, can we lesion them to simulate the linguistic deficits of the injured brain? In this work, we introduce a methodology to model aphasia - a complex language disorder caused by neural injury - by selectively disabling components in a modular Mixture-of-Experts (MoE) language model. We simulate distinct aphasia subtypes, validate their linguistic outputs against real patient speech, and then investigate functional recovery by retraining the model's remaining healthy experts. Our results demonstrate that lesioning functionally-specialized experts for syntax or semantics induces distinct impairments that closely resemble Broca's and Wernicke's aphasia, respectively. Crucially, we show that freezing the damaged experts and retraining the intact ones on conversational data restores significant linguistic function, demonstrating a computational analogue for rehabilitation. These findings establish modular LLMs as a powerful and clinically-relevant potential framework for modeling the mechanisms of language disorders and for computationally exploring novel pathways for therapy.", 'authors': ['Yifan Wang', 'Jingyuan Sun', 'Jichen Zheng', 'Yunhao Zhang', 'Chunyu Ye', 'Jixing Li', 'Chengqing Zong', 'Shaonan Wang']}, {'id': 'http://arxiv.org/abs/2508.04213v1', 'title': 'A Hybrid AI Methodology for Generating Ontologies of Research Topics   from Scientific Paper Corpora', 'published': '2025-08-06T08:48:14Z', 'updated': '2025-08-06T08:48:14Z', 'abstract': 'Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM) play a central role in providing the primary framework through which intelligent systems can explore and interpret the literature. However, these resources have traditionally been manually curated, a process that is time-consuming, prone to obsolescence, and limited in granularity. This paper presents Sci-OG, a semi-auto\\-mated methodology for generating research topic ontologies, employing a multi-step approach: 1) Topic Discovery, extracting potential topics from research papers; 2) Relationship Classification, determining semantic relationships between topic pairs; and 3) Ontology Construction, refining and organizing topics into a structured ontology. The relationship classification component, which constitutes the core of the system, integrates an encoder-based language model with features describing topic occurrence in the scientific literature. We evaluate this approach against a range of alternative solutions using a dataset of 21,649 manually annotated semantic triples. Our method achieves the highest F1 score (0.951), surpassing various competing approaches, including a fine-tuned SciBERT model and several LLM baselines, such as the fine-tuned GPT4-mini. Our work is corroborated by a use case which illustrates the practical application of our system to extend the CSO ontology in the area of cybersecurity. The presented solution is designed to improve the accessibility, organization, and analysis of scientific knowledge, thereby supporting advancements in AI-enabled literature management and research exploration.', 'authors': ['Alessia Pisu', 'Livio Pompianu', 'Francesco Osborne', 'Diego Reforgiato Recupero', 'Daniele Riboni', 'Angelo Salatino']}, {'id': 'http://arxiv.org/abs/2508.04748v1', 'title': 'AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular   Property Prediction with Large Language Models', 'published': '2025-08-06T08:46:22Z', 'updated': '2025-08-06T08:46:22Z', 'abstract': "Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.", 'authors': ['Xuan Lin', 'Long Chen', 'Yile Wang']}, {'id': 'http://arxiv.org/abs/2508.04206v1', 'title': 'ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal   Movie Recommendation', 'published': '2025-08-06T08:39:07Z', 'updated': '2025-08-06T08:39:07Z', 'abstract': 'Recommending long-form video content demands joint modeling of visual, audio, and textual modalities, yet most benchmarks address only raw features or narrow fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for LLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K, it aligns dense item embeddings from three modalities: audio (block-level, i-vector), visual (CNN, AVF), and text. Missing or sparse metadata is automatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada), generating high-quality synopses for thousands of movies. All text (raw or augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5), producing multiple ready-to-use sets. The pipeline supports interchangeable early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are fully declarative via a single YAML file. Evaluation spans accuracy (Recall, nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty, diversity, fairness. Results show LLM-based augmentation and strong text embeddings boost cold-start and coverage, especially when fused with audio-visual features. Systematic benchmarking reveals universal versus backbone- or metric-specific combinations. Open-source code, embeddings, and configs enable reproducible, fair multimodal RS research and advance principled generative AI integration in large-scale recommendation. Code: https://recsys-lab.github.io/ViLLA-MMBench', 'authors': ['Fatemeh Nazary', 'Ali Tourani', 'Yashar Deldjoo', 'Tommaso Di Noia']}, {'id': 'http://arxiv.org/abs/2508.04199v1', 'title': 'Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource,   Culturally Nuanced Contexts', 'published': '2025-08-06T08:27:55Z', 'updated': '2025-08-06T08:27:55Z', 'abstract': 'Sentiment analysis in low-resource, culturally nuanced contexts challenges conventional NLP approaches that assume fixed labels and universal affective expressions. We present a diagnostic framework that treats sentiment as a context-dependent, culturally embedded construct, and evaluate how large language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp messages from Nairobi youth health groups. Using a combination of human-annotated data, sentiment-flipped counterfactuals, and rubric-based explanation evaluation, we probe LLM interpretability, robustness, and alignment with human reasoning. Framing our evaluation through a social-science measurement lens, we operationalize and interrogate LLMs outputs as an instrument for measuring the abstract concept of sentiment. Our findings reveal significant variation in model reasoning quality, with top-tier LLMs demonstrating interpretive stability, while open models often falter under ambiguity or sentiment shifts. This work highlights the need for culturally sensitive, reasoning-aware AI evaluation in complex, real-world communication.', 'authors': ['Millicent Ochieng', 'Anja Thieme', 'Ignatius Ezeani', 'Risa Ueno', 'Samuel Maina', 'Keshet Ronen', 'Javier Gonzalez', "Jacki O'Neill"]}, {'id': 'http://arxiv.org/abs/2508.04196v1', 'title': 'Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large   Language Models', 'published': '2025-08-06T08:25:40Z', 'updated': '2025-08-06T08:25:40Z', 'abstract': 'Despite significant advances in alignment techniques, we demonstrate that state-of-the-art language models remain vulnerable to carefully crafted conversational scenarios that can induce various forms of misalignment without explicit jailbreaking. Through systematic manual red-teaming with Claude-4-Opus, we discovered 10 successful attack scenarios, revealing fundamental vulnerabilities in how current alignment methods handle narrative immersion, emotional pressure, and strategic framing. These scenarios successfully elicited a range of misaligned behaviors, including deception, value drift, self-preservation, and manipulative reasoning, each exploiting different psychological and contextual vulnerabilities. To validate generalizability, we distilled our successful manual attacks into MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible testing across multiple models. Cross-model evaluation of our 10 scenarios against five frontier LLMs revealed an overall 76% vulnerability rate, with significant variations: GPT-4.1 showed the highest susceptibility (90%), while Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate that sophisticated reasoning capabilities often become attack vectors rather than protective mechanisms, as models can be manipulated into complex justifications for misaligned behavior. This work provides (i) a detailed taxonomy of conversational manipulation patterns and (ii) a reusable evaluation framework. Together, these findings expose critical gaps in current alignment strategies and highlight the need for robustness against subtle, scenario-based manipulation in future AI systems.', 'authors': ['Siddhant Panpatil', 'Hiskias Dingeto', 'Haon Park']}, {'id': 'http://arxiv.org/abs/2508.04181v1', 'title': 'Deeper Inside Deep ViT', 'published': '2025-08-06T08:08:04Z', 'updated': '2025-08-06T08:08:04Z', 'abstract': 'There have been attempts to create large-scale structures in vision models similar to LLM, such as ViT-22B. While this research has provided numerous analyses and insights, our understanding of its practical utility remains incomplete. Therefore, we examine how this model structure reacts and train in a local environment. We also highlight the instability in training and make some model modifications to stabilize it. The ViT-22B model, trained from scratch, overall outperformed ViT in terms of performance under the same parameter size. Additionally, we venture into the task of image generation, which has not been attempted in ViT-22B. We propose an image generation architecture using ViT and investigate which between ViT and ViT-22B is a more suitable structure for image generation.', 'authors': ['Sungrae Hong']}, {'id': 'http://arxiv.org/abs/2508.04175v1', 'title': 'AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning   and Fine-Grained Reward Optimization', 'published': '2025-08-06T08:00:27Z', 'updated': '2025-08-06T08:00:27Z', 'abstract': 'While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities across diverse domains, their application to specialized anomaly detection (AD) remains constrained by domain adaptation challenges. Existing Group Relative Policy Optimization (GRPO) based approaches suffer from two critical limitations: inadequate training data utilization when models produce uniform responses, and insufficient supervision over reasoning processes that encourage immediate binary decisions without deliberative analysis. We propose a comprehensive framework addressing these limitations through two synergistic innovations. First, we introduce a multi-stage deliberative reasoning process that guides models from region identification to focused examination, generating diverse response patterns essential for GRPO optimization while enabling structured supervision over analytical workflows. Second, we develop a fine-grained reward mechanism incorporating classification accuracy and localization supervision, transforming binary feedback into continuous signals that distinguish genuine analytical insight from spurious correctness. Comprehensive evaluation across multiple industrial datasets demonstrates substantial performance improvements in adapting general vision-language models to specialized anomaly detection. Our method achieves superior accuracy with efficient adaptation of existing annotations, effectively bridging the gap between general-purpose MLLM capabilities and the fine-grained visual discrimination required for detecting subtle manufacturing defects and structural irregularities.', 'authors': ['Jingyi Liao', 'Yongyi Su', 'Rong-Cheng Tu', 'Zhao Jin', 'Wenhao Sun', 'Yiting Li', 'Dacheng Tao', 'Xun Xu', 'Xulei Yang']}, {'id': 'http://arxiv.org/abs/2508.04149v1', 'title': 'Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap', 'published': '2025-08-06T07:24:14Z', 'updated': '2025-08-06T07:24:14Z', 'abstract': 'Aligning large language models (LLMs) with human preferences is a critical challenge in AI research. While methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they often rely on large, costly preference datasets. The current work lacks methods for high-quality data selection specifically for preference data. In this work, we introduce a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10\\% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources.', 'authors': ['Xuan Qi', 'Rongwu Xu', 'Zhijing Jin']}, {'id': 'http://arxiv.org/abs/2508.04145v1', 'title': 'Benefit from Rich: Tackling Search Interaction Sparsity in Search   Enhanced Recommendation', 'published': '2025-08-06T07:16:40Z', 'updated': '2025-08-06T07:16:40Z', 'abstract': 'In modern online platforms, search and recommendation (S&R) often coexist, offering opportunities for performance improvement through search-enhanced approaches. Existing studies show that incorporating search signals boosts recommendation performance. However, the effectiveness of these methods relies heavily on rich search interactions. They primarily benefit a small subset of users with abundant search behavior, while offering limited improvements for the majority of users who exhibit only sparse search activity. To address the problem of sparse search data in search-enhanced recommendation, we face two key challenges: (1) how to learn useful search features for users with sparse search interactions, and (2) how to design effective training objectives under sparse conditions. Our idea is to leverage the features of users with rich search interactions to enhance those of users with sparse search interactions. Based on this idea, we propose GSERec, a method that utilizes message passing on the User-Code Graphs to alleviate data sparsity in Search-Enhanced Recommendation. Specifically, we utilize Large Language Models (LLMs) with vector quantization to generate discrete codes, which connect similar users and thereby construct the graph. Through message passing on this graph, embeddings of users with rich search data are propagated to enhance the embeddings of users with sparse interactions. To further ensure that the message passing captures meaningful information from truly similar users, we introduce a contrastive loss to better model user similarities. The enhanced user representations are then integrated into downstream search-enhanced recommendation models. Experiments on three real-world datasets show that GSERec consistently outperforms baselines, especially for users with sparse search behaviors.', 'authors': ['Teng Shi', 'Weijie Yu', 'Xiao Zhang', 'Ming He', 'Jianping Fan', 'Jun Xu']}, {'id': 'http://arxiv.org/abs/2508.04138v1', 'title': 'COPO: Consistency-Aware Policy Optimization', 'published': '2025-08-06T07:05:18Z', 'updated': '2025-08-06T07:05:18Z', 'abstract': "Reinforcement learning has significantly enhanced the reasoning capabilities of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the introduction of DeepSeek R1 has inspired a surge of interest in leveraging rule-based rewards as a low-cost alternative for computing advantage functions and guiding policy optimization. However, a common challenge observed across many replication and extension efforts is that when multiple sampled responses under a single prompt converge to identical outcomes, whether correct or incorrect, the group-based advantage degenerates to zero. This leads to vanishing gradients and renders the corresponding samples ineffective for learning, ultimately limiting training efficiency and downstream performance. To address this issue, we propose a consistency-aware policy optimization framework that introduces a structured global reward based on outcome consistency, the global loss based on it ensures that, even when model outputs show high intra-group consistency, the training process still receives meaningful learning signals, which encourages the generation of correct and self-consistent reasoning paths from a global perspective. Furthermore, we incorporate an entropy-based soft blending mechanism that adaptively balances local advantage estimation with global optimization, enabling dynamic transitions between exploration and convergence throughout training. Our method introduces several key innovations in both reward design and optimization strategy. We validate its effectiveness through substantial performance gains on multiple mathematical reasoning benchmarks, highlighting the proposed framework's robustness and general applicability. Code of this work has been released at https://github.com/hijih/copo-code.git.", 'authors': ['Jinghang Han', 'Jiawei Chen', 'Hang Shao', 'Hao Ma', 'Mingcheng Li', 'Xintian Shen', 'Lihao Zheng', 'Wei Chen', 'Tao Wei', 'Lihua Zhang']}, {'id': 'http://arxiv.org/abs/2508.04125v1', 'title': 'Experimental Analysis of Productive Interaction Strategy with ChatGPT:   User Study on Function and Project-level Code Generation Tasks', 'published': '2025-08-06T06:48:48Z', 'updated': '2025-08-06T06:48:48Z', 'abstract': "The application of Large Language Models (LLMs) is growing in the productive completion of Software Engineering tasks. Yet, studies investigating the productive prompting techniques often employed a limited problem space, primarily focusing on well-known prompting patterns and mainly targeting function-level SE practices. We identify significant gaps in real-world workflows that involve complexities beyond class-level (e.g., multi-class dependencies) and different features that can impact Human-LLM Interactions (HLIs) processes in code generation. To address these issues, we designed an experiment that comprehensively analyzed the HLI features regarding the code generation productivity. Our study presents two project-level benchmark tasks, extending beyond function-level evaluations. We conducted a user study with 36 participants from diverse backgrounds, asking them to solve the assigned tasks by interacting with the GPT assistant using specific prompting patterns. We also examined the participants' experience and their behavioral features during interactions by analyzing screen recordings and GPT chat logs. Our statistical and empirical investigation revealed (1) that three out of 15 HLI features significantly impacted the productivity in code generation; (2) five primary guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of 29 runtime and logic errors that can occur during HLI processes, along with suggested mitigation plans.", 'authors': ['Sangwon Hyun', 'Hyunjun Kim', 'Jinhyuk Jang', 'Hyojin Choi', 'M. Ali Babar']}, {'id': 'http://arxiv.org/abs/2508.04117v1', 'title': 'Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks', 'published': '2025-08-06T06:34:12Z', 'updated': '2025-08-06T06:34:12Z', 'abstract': 'The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. We investigate the conditions that lead to LLM over-memorization and find that training epochs and large learning rates contribute to this issue. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. Our experiments unveil the over-memorization to be broadly applicable across different tasks, models, and finetuning methods. Our research highlights that overparameterized, extensively finetuned LLMs exhibit unique learning dynamics distinct from traditional machine learning models. Based on our observations of over-memorization, we provide recommendations on checkpoint and learning rate selection during finetuning.', 'authors': ['Zhiwen Ruan', 'Yun Chen', 'Yutao Hou', 'Peng Li', 'Yang Liu', 'Guanhua Chen']}, {'id': 'http://arxiv.org/abs/2508.04107v2', 'title': 'Unlocking the Potential of MLLMs in Referring Expression Segmentation   via a Light-weight Mask Decoder', 'published': '2025-08-06T06:06:52Z', 'updated': '2025-08-07T05:07:54Z', 'abstract': 'Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg.', 'authors': ['Jingchao Wang', 'Zhijian Wu', 'Dingjiang Huang', 'Yefeng Zheng', 'Hong Wang']}, {'id': 'http://arxiv.org/abs/2508.04096v1', 'title': 'Efficient Scaling for LLM-based ASR', 'published': '2025-08-06T05:28:18Z', 'updated': '2025-08-06T05:28:18Z', 'abstract': 'Large language model (LLM)-based automatic speech recognition (ASR) achieves strong performance but often incurs high computational costs. This work investigates how to obtain the best LLM-ASR performance efficiently. Through comprehensive and controlled experiments, we find that pretraining the speech encoder before integrating it with the LLM leads to significantly better scaling efficiency than the standard practice of joint post-training of LLM-ASR. Based on this insight, we propose a new multi-stage LLM-ASR training strategy, EFIN: Encoder First Integration. Among all training strategies evaluated, EFIN consistently delivers better performance (relative to 21.1% CERR) with significantly lower computation budgets (49.9% FLOPs). Furthermore, we derive a scaling law that approximates ASR error rates as a computation function, providing practical guidance for LLM-ASR scaling.', 'authors': ['Bingshen Mu', 'Yiwen Shao', 'Kun Wei', 'Dong Yu', 'Lei Xie']}, {'id': 'http://arxiv.org/abs/2508.04086v1', 'title': 'ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"', 'published': '2025-08-06T05:04:00Z', 'updated': '2025-08-06T05:04:00Z', 'abstract': 'Prior work synthesizes tool-use LLM datasets by first generating a user query, followed by complex tool-use annotations like DFS. This leads to inevitable annotation failures and low efficiency in data generation. We introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad first constructs valid tool-use chains through an iterative process guided by textual "gradients", and then synthesizes corresponding user queries. This "answer-first" approach led to ToolGrad-5k, a dataset generated with more complex tool use, lower cost, and 100% pass rate. Experiments show that models trained on ToolGrad-5k outperform those on expensive baseline datasets and proprietary LLMs, even on OOD benchmarks.', 'authors': ['Zhongyi Zhou', 'Kohei Uehara', 'Haoyu Zhang', 'Jingtao Zhou', 'Lin Gu', 'Ruofei Du', 'Zheng Xu', 'Tatsuya Harada']}, {'id': 'http://arxiv.org/abs/2508.04080v1', 'title': 'GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge   Boundaries via Iterative Self-Refinement', 'published': '2025-08-06T04:45:34Z', 'updated': '2025-08-06T04:45:34Z', 'abstract': "Recent studies have extended the application of large language models (LLMs) to geographic problems, revealing surprising geospatial competence even without explicit spatial supervision. However, LLMs still face challenges in spatial consistency, multi-hop reasoning, and geographic bias. To address these issues, we propose GeoSR, a self-refining agentic reasoning framework that embeds core geographic principles -- most notably Tobler's First Law of Geography -- into an iterative prediction loop. In GeoSR, the reasoning process is decomposed into three collaborating agents: (1) a variable-selection agent that selects relevant covariates from the same location; (2) a point-selection agent that chooses reference predictions at nearby locations generated by the LLM in previous rounds; and (3) a refine agent that coordinates the iterative refinement process by evaluating prediction quality and triggering further rounds when necessary. This agentic loop progressively improves prediction quality by leveraging both spatial dependencies and inter-variable relationships. We validate GeoSR on tasks ranging from physical-world property estimation to socioeconomic prediction. Experimental results show consistent improvements over standard prompting strategies, demonstrating that incorporating geostatistical priors and spatially structured reasoning into LLMs leads to more accurate and equitable geospatial predictions. The code of GeoSR is available at https://github.com/JinfanTang/GeoSR.", 'authors': ['Jinfan Tang', 'Kunming Wu', 'Ruifeng Gongxie', 'Yuya He', 'Yuankai Wu']}, {'id': 'http://arxiv.org/abs/2508.04073v1', 'title': 'Efficient Strategy for Improving Large Language Model (LLM) Capabilities', 'published': '2025-08-06T04:08:26Z', 'updated': '2025-08-06T04:08:26Z', 'abstract': 'Large Language Models (LLMs) have become a milestone in the field of artificial intelligence and natural language processing. However, their large-scale deployment remains constrained by the need for significant computational resources. This work proposes starting from a base model to explore and combine data processing and careful data selection techniques, training strategies, and architectural adjustments to improve the efficiency of LLMs in resource-constrained environments and within a delimited knowledge base. The methodological approach included defining criteria for building reliable datasets, conducting controlled experiments with different configurations, and systematically evaluating the resulting variants in terms of capability, versatility, response time, and safety. Finally, comparative tests were conducted to measure the performance of the developed variants and to validate the effectiveness of the proposed strategies. This work is based on the master\'s thesis in Systems and Computer Engineering titled "Efficient Strategy for Improving the Capabilities of Large Language Models (LLMs)".', 'authors': ['Julián Camilo Velandia Gutiérrez']}, {'id': 'http://arxiv.org/abs/2508.04072v1', 'title': 'KG-Augmented Executable CoT for Mathematical Coding', 'published': '2025-08-06T04:07:35Z', 'updated': '2025-08-06T04:07:35Z', 'abstract': 'In recent years, large language models (LLMs) have excelled in natural language processing tasks but face significant challenges in complex reasoning tasks such as mathematical reasoning and code generation. To address these limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a novel framework that enhances code generation through knowledge graphs and improves mathematical reasoning via executable code. KGA-ECoT decomposes problems into a Structured Task Graph, leverages efficient GraphRAG for precise knowledge retrieval from mathematical libraries, and generates verifiable code to ensure computational accuracy. Evaluations on multiple mathematical reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms existing prompting methods, achieving absolute accuracy improvements ranging from several to over ten percentage points. Further analysis confirms the critical roles of GraphRAG in enhancing code quality and external code execution in ensuring precision. These findings collectively establish KGA-ECoT as a robust and highly generalizable framework for complex mathematical reasoning tasks.', 'authors': ['Xingyu Chen', 'Junxiu An', 'Jun Guo', 'Li Wang', 'Jingcai Guo']}, {'id': 'http://arxiv.org/abs/2508.04063v1', 'title': 'Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for   Short Answer Grading', 'published': '2025-08-06T03:52:55Z', 'updated': '2025-08-06T03:52:55Z', 'abstract': "Research to improve Automated Short Answer Grading has recently focused on Large Language Models (LLMs) with prompt engineering and no- or few-shot prompting to achieve best results. This is in contrast to the fine-tuning approach, which has historically required large-scale compute clusters inaccessible to most users. New closed-model approaches such as OpenAI's fine-tuning service promise results with as few as 100 examples, while methods using open weights such as quantized low-rank adaptive (QLORA) can be used to fine-tune models on consumer GPUs. We evaluate both of these fine-tuning methods, measuring their interaction with few-shot prompting for automated short answer grading (ASAG) with structured (JSON) outputs. Our results show that finetuning with small amounts of data has limited utility for Llama open-weight models, but that fine-tuning methods can outperform few-shot baseline instruction-tuned LLMs for OpenAI's closed models. While our evaluation set is limited, we find some evidence that the observed benefits of finetuning may be impacted by the domain subject matter. Lastly, we observed dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by seeding the initial training examples with a significant amount of cheaply generated synthetic training data.", 'authors': ['Joel Walsh', 'Siddarth Mamidanna', 'Benjamin Nye', 'Mark Core', 'Daniel Auerbach']}, {'id': 'http://arxiv.org/abs/2508.04057v1', 'title': 'PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection   for Efficient RAG', 'published': '2025-08-06T03:33:01Z', 'updated': '2025-08-06T03:33:01Z', 'abstract': "Retrieval-Augmented Generation (RAG) has become a cornerstone technique for enhancing large language models (LLMs) with external knowledge. However, current RAG systems face two critical limitations: (1) they inefficiently retrieve information for every query, including simple questions that could be resolved using the LLM's parametric knowledge alone, and (2) they risk retrieving irrelevant documents when queries contain sparse information signals. To address these gaps, we introduce Parametric-verified Adaptive Information Retrieval and Selection (PAIRS), a training-free framework that integrates parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information. Specifically, PAIRS employs a dual-path generation mechanism: First, the LLM produces both a direct answer and a context-augmented answer using self-generated pseudo-context. When these outputs converge, PAIRS bypasses external retrieval entirely, dramatically improving the RAG system's efficiency. For divergent cases, PAIRS activates a dual-path retrieval (DPR) process guided by both the original query and self-generated contextual signals, followed by an Adaptive Information Selection (AIS) module that filters documents through weighted similarity to both sources. This simple yet effective approach can not only enhance efficiency by eliminating unnecessary retrievals but also improve accuracy through contextually guided retrieval and adaptive information selection. Experimental results on six question-answering (QA) benchmarks show that PAIRS reduces retrieval costs by around 25% (triggering for only 75% of queries) while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior baselines on average.", 'authors': ['Wang Chen', 'Guanqiang Qi', 'Weikang Li', 'Yang Li', 'Deguo Xia', 'Jizhou Huang']}, {'id': 'http://arxiv.org/abs/2508.04038v1', 'title': 'ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval   Driven LLM Agents', 'published': '2025-08-06T02:57:57Z', 'updated': '2025-08-06T02:57:57Z', 'abstract': 'Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA.', 'authors': ['Zechen Li', 'Baiyu Chen', 'Hao Xue', 'Flora D. Salim']}, {'id': 'http://arxiv.org/abs/2508.04032v1', 'title': 'Enhancing Serendipity Recommendation System by Constructing Dynamic User   Knowledge Graphs with Large Language Models', 'published': '2025-08-06T02:52:09Z', 'updated': '2025-08-06T02:52:09Z', 'abstract': "The feedback loop in industrial recommendation systems reinforces homogeneous content, creates filter bubble effects, and diminishes user satisfaction. Recently, large language models(LLMs) have demonstrated potential in serendipity recommendation, thanks to their extensive world knowledge and superior reasoning capabilities. However, these models still face challenges in ensuring the rationality of the reasoning process, the usefulness of the reasoning results, and meeting the latency requirements of industrial recommendation systems (RSs). To address these challenges, we propose a method that leverages llm to dynamically construct user knowledge graphs, thereby enhancing the serendipity of recommendation systems. This method comprises a two stage framework:(1) two-hop interest reasoning, where user static profiles and historical behaviors are utilized to dynamically construct user knowledge graphs via llm. Two-hop reasoning, which can enhance the quality and accuracy of LLM reasoning results, is then performed on the constructed graphs to identify users' potential interests; and(2) Near-line adaptation, a cost-effective approach to deploying the aforementioned models in industrial recommendation systems. We propose a u2i (user-to-item) retrieval model that also incorporates i2i (item-to-item) retrieval capabilities, the retrieved items not only exhibit strong relevance to users' newly emerged interests but also retain the high conversion rate of traditional u2i retrieval. Our online experiments on the Dewu app, which has tens of millions of users, indicate that the method increased the exposure novelty rate by 4.62%, the click novelty rate by 4.85%, the average view duration per person by 0.15%, unique visitor click through rate by 0.07%, and unique visitor interaction penetration by 0.30%, enhancing user experience.", 'authors': ['Qian Yong', 'Yanhui Li', 'Jialiang Shi', 'Yaguang Dou', 'Tian Qi']}, {'id': 'http://arxiv.org/abs/2508.04031v1', 'title': 'BridgeScope: A Universal Toolkit for Bridging Large Language Models and   Databases', 'published': '2025-08-06T02:51:16Z', 'updated': '2025-08-06T02:51:16Z', 'abstract': 'As large language models (LLMs) demonstrate increasingly powerful reasoning and orchestration capabilities, LLM-based agents are rapidly proliferating for complex data-related tasks. Despite this progress, the current design of how LLMs interact with databases exhibits critical limitations in usability, security, privilege management, and data transmission efficiency. To resolve these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs and databases through three key innovations. First, it modularizes SQL operations into fine-grained tools for context retrieval, CRUD execution, and ACID-compliant transaction management, enabling more precise and LLM-friendly functionality controls. Second, it aligns tool implementations with both database privileges and user security policies to steer LLMs away from unsafe or unauthorized operations, improving task execution efficiency while safeguarding database security. Third, it introduces a proxy mechanism for seamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All of these designs are database-agnostic and can be transparently integrated with existing agent architectures. We also release an open-source implementation of BridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate that BridgeScope enables LLM agents to operate databases more effectively, reduces token usage by up to 80% through improved security awareness, and uniquely supports data-intensive workflows beyond existing toolkits, establishing BridgeScope as a robust foundation for next-generation intelligent data automation.', 'authors': ['Lianggui Weng', 'Dandan Liu', 'Rong Zhu', 'Bolin Ding', 'Jingren Zhou']}, {'id': 'http://arxiv.org/abs/2508.04012v1', 'title': 'Step More: Going Beyond Single Backpropagation in Meta Learning Based   Model Editing', 'published': '2025-08-06T01:54:58Z', 'updated': '2025-08-06T01:54:58Z', 'abstract': 'Large Language Models (LLMs) underpin many AI applications, but their static nature makes updating knowledge costly. Model editing offers an efficient alternative by injecting new information through targeted parameter modifications. In particular, meta-learning-based model editing (MLBME) methods have demonstrated notable advantages in both editing effectiveness and efficiency. Despite this, we find that MLBME exhibits suboptimal performance in low-data scenarios, and its training efficiency is bottlenecked by the computation of KL divergence. To address these, we propose $\\textbf{S}$tep $\\textbf{M}$ore $\\textbf{Edit}$ ($\\textbf{SMEdit}$), a novel MLBME method that adopts $\\textbf{M}$ultiple $\\textbf{B}$ackpro$\\textbf{P}$agation $\\textbf{S}$teps ($\\textbf{MBPS}$) to improve editing performance under limited supervision and a norm regularization on weight updates to improve training efficiency. Experimental results on two datasets and two LLMs demonstrate that SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance. Our code will be released soon.', 'authors': ['Xiaopeng Li', 'Shasha Li', 'Xi Wang', 'Shezheng Song', 'Bin Ji', 'Shangwen Wang', 'Jun Ma', 'Xiaodong Liu', 'Mina Liu', 'Jie Yu']}, {'id': 'http://arxiv.org/abs/2508.03999v1', 'title': 'Tensorized Clustered LoRA Merging for Multi-Task Interference', 'published': '2025-08-06T01:26:43Z', 'updated': '2025-08-06T01:26:43Z', 'abstract': 'Despite the success of the monolithic dense paradigm of large language models (LLMs), the LoRA adapters offer an efficient solution by fine-tuning small task-specific modules and merging them with the base model. However, in multi-task settings, merging LoRA adapters trained on heterogeneous sources frequently causes \\textit{task interference}, degrading downstream performance. To address this, we propose a tensorized clustered LoRA (TC-LoRA) library targeting to address the task interference at the \\textit{text-level} and \\textit{parameter-level}. At the \\textit{text-level}, we cluster the training samples in the embedding space to capture input-format similarities, then train a specialized LoRA adapter for each cluster. At the \\textit{parameter-level}, we introduce a joint Canonical Polyadic (CP) decomposition that disentangles task-specific and shared factors across LoRA adapters. This joint factorization preserves essential knowledge while reducing cross-task interference. Extensive experiments on out-of-domain zero-shot and skill-composition tasks-including reasoning, question answering, and coding. Compared to strong SVD-based baselines, TC-LoRA achieves +1.4\\% accuracy on Phi-3 and +2.3\\% on Mistral-7B (+2.3\\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.', 'authors': ['Zhan Su', 'Fengran Mo', 'Guojun Liang', 'Jinghan Zhang', 'Bingbing Wen', 'Prayag Tiwari', 'Jian-Yun Nie']}, {'id': 'http://arxiv.org/abs/2508.03991v1', 'title': 'Galaxy: A Cognition-Centered Framework for Proactive,   Privacy-Preserving, and Self-Evolving LLM Agents', 'published': '2025-08-06T00:46:38Z', 'updated': '2025-08-06T00:46:38Z', 'abstract': "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are designed to enhance human capabilities and perform tasks on behalf of users. The emergence of LLM agents brings new opportunities for the development of IPAs. While responsive capabilities have been widely studied, proactive behaviors remain underexplored. Designing an IPA that is proactive, privacy-preserving, and capable of self-evolution remains a significant challenge. Designing such IPAs relies on the cognitive architecture of LLM agents. This work proposes Cognition Forest, a semantic structure designed to align cognitive modeling with system-level design. We unify cognitive architecture and system design into a self-reinforcing loop instead of treating them separately. Based on this principle, we present Galaxy, a framework that supports multidimensional interactions and personalized capability generation. Two cooperative agents are implemented based on Galaxy: KoRa, a cognition-enhanced generative agent that supports both responsive and proactive skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's self-evolution and privacy preservation. Experimental results show that Galaxy outperforms multiple state-of-the-art benchmarks. Ablation studies and real-world interaction cases validate the effectiveness of Galaxy.", 'authors': ['Chongyu Bao', 'Ruimin Dai', 'Yangbo Shen', 'Runyang Jian', 'Jinghan Zhang', 'Xiaolan Liu', 'Kunpeng Liu']}, {'id': 'http://arxiv.org/abs/2508.03990v1', 'title': "Are Today's LLMs Ready to Explain Well-Being Concepts?", 'published': '2025-08-06T00:45:02Z', 'updated': '2025-08-06T00:45:02Z', 'abstract': 'Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.', 'authors': ['Bohan Jiang', 'Dawei Li', 'Zhen Tan', 'Chengshuai Zhao', 'Huan Liu']}, {'id': 'http://arxiv.org/abs/2508.03979v1', 'title': 'Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in   Self-Consistency', 'published': '2025-08-06T00:14:18Z', 'updated': '2025-08-06T00:14:18Z', 'abstract': "Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis pruning. Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five LLMs on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases.", 'authors': ['Md Arafat Sultan', 'Ramón Fernandez Astudillo']}, {'id': 'http://arxiv.org/abs/2508.03970v1', 'title': 'Data and AI governance: Promoting equity, ethics, and fairness in large   language models', 'published': '2025-08-05T23:15:31Z', 'updated': '2025-08-05T23:15:31Z', 'abstract': 'In this paper, we cover approaches to systematically govern, assess and quantify bias across the complete life cycle of machine learning models, from initial development and validation to ongoing production monitoring and guardrail implementation. Building upon our foundational work on the Bias Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the authors share prevalent bias and fairness related gaps in Large Language Models (LLMs) and discuss data and AI governance framework to address Bias, Ethics, Fairness, and Factuality within LLMs. The data and AI governance approach discussed in this paper is suitable for practical, real-world applications, enabling rigorous benchmarking of LLMs prior to production deployment, facilitating continuous real-time evaluation, and proactively governing LLM generated responses. By implementing the data and AI governance across the life cycle of AI development, organizations can significantly enhance the safety and responsibility of their GenAI systems, effectively mitigating risks of discrimination and protecting against potential reputational or brand-related harm. Ultimately, through this article, we aim to contribute to advancement of the creation and deployment of socially responsible and ethically aligned generative artificial intelligence powered applications.', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay']}, {'id': 'http://arxiv.org/abs/2508.03966v1', 'title': 'GP and LLMs for Program Synthesis: No Clear Winners', 'published': '2025-08-05T23:09:45Z', 'updated': '2025-08-05T23:09:45Z', 'abstract': 'Genetic programming (GP) and large language models (LLMs) differ in how program specifications are provided: GP uses input-output examples, and LLMs use text descriptions. In this work, we compared the ability of PushGP and GPT-4o to synthesize computer programs for tasks from the PSB2 benchmark suite. We used three prompt variants with GPT-4o: input-output examples (data-only), textual description of the task (text-only), and a combination of both textual descriptions and input-output examples (data-text). Additionally, we varied the number of input-output examples available for building programs. For each synthesizer and task combination, we compared success rates across all program synthesizers, as well as the similarity between successful GPT-4o synthesized programs. We found that the combination of PushGP and GPT-4o with data-text prompting led to the greatest number of tasks solved (23 of the 25 tasks), even though several tasks were solved exclusively by only one of the two synthesizers. We also observed that PushGP and GPT-4o with data-only prompting solved fewer tasks with the decrease in the training set size, while the remaining synthesizers saw no decrease. We also detected significant differences in similarity between the successful programs synthesized for GPT-4o with text-only and data-only prompting. With there being no dominant program synthesizer, this work highlights the importance of different optimization techniques used by PushGP and LLMs to synthesize programs.', 'authors': ['Jose Guadalupe Hernandez', 'Anil Kumar Saini', 'Gabriel Ketron', 'Jason H. Moore']}, {'id': 'http://arxiv.org/abs/2508.03963v1', 'title': 'Can Large Language Models Adequately Perform Symbolic Reasoning Over   Time Series?', 'published': '2025-08-05T22:58:54Z', 'updated': '2025-08-05T22:58:54Z', 'abstract': "Uncovering hidden symbolic laws from time series data, as an aspiration dating back to Kepler's discovery of planetary motion, remains a core challenge in scientific discovery and artificial intelligence. While Large Language Models show promise in structured reasoning tasks, their ability to infer interpretable, context-aligned symbolic structures from time series data is still underexplored. To systematically evaluate this capability, we introduce SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning over real-world time series across three tasks: multivariate symbolic regression, Boolean network inference, and causal discovery. Unlike prior efforts limited to simple algebraic equations, SymbolBench spans a diverse set of symbolic forms with varying complexity. We further propose a unified framework that integrates LLMs with genetic programming to form a closed-loop symbolic reasoning system, where LLMs act both as predictors and evaluators. Our empirical results reveal key strengths and limitations of current models, highlighting the importance of combining domain knowledge, context alignment, and reasoning structure to improve LLMs in automated scientific discovery.", 'authors': ['Zewen Liu', 'Juntong Ni', 'Xianfeng Tang', 'Max S. Y. Lau', 'Wei Jin']}, {'id': 'http://arxiv.org/abs/2508.03935v1', 'title': 'CAP-LLM: Context-Augmented Personalized Large Language Models for News   Headline Generation', 'published': '2025-08-05T21:55:44Z', 'updated': '2025-08-05T21:55:44Z', 'abstract': "In the era of information overload, personalized news headline generation is crucial for engaging users by tailoring content to their preferences while accurately conveying news facts. Existing methods struggle with effectively capturing complex user interests and ensuring factual consistency, often leading to generic or misleading headlines. Leveraging the unprecedented capabilities of Large Language Models (LLMs) in text generation, we propose Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates user preferences and factual consistency constraints into a powerful pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture long-term user interests, a Context Injection Adapter to seamlessly integrate these preferences and current article context into the LLM's generation process, and a Fact-Consistency Reinforcement Module employing a novel contrastive loss to mitigate hallucination. Evaluated on the real-world PENS dataset, CAP-LLM achieves state-of-the-art performance across all metrics. Notably, it significantly improves factual consistency (FactCC of 87.50) over strong baselines like BART (86.67), while simultaneously enhancing personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1 26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations, and sensitivity analyses further validate the effectiveness of each component and the robustness of our approach, demonstrating CAP-LLM's ability to achieve a superior balance between personalization and factual accuracy in news headline generation.", 'authors': ['Raymond Wilson', 'Cole Graham', 'Chase Carter', 'Zefeng Yang', 'Ruiqi Gu']}, {'id': 'http://arxiv.org/abs/2508.03931v1', 'title': 'Analyzing Prominent LLMs: An Empirical Study of Performance and   Complexity in Solving LeetCode Problems', 'published': '2025-08-05T21:50:52Z', 'updated': '2025-08-05T21:50:52Z', 'abstract': "Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are transforming software engineering by automating key tasks, including code generation, testing, and debugging. As these models become integral to development workflows, a systematic comparison of their performance is essential for optimizing their use in real world applications. This study benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems across easy, medium, and hard difficulties, generating solutions in Java and Python. We evaluate each model based on execution time, memory usage, and algorithmic complexity, revealing significant performance differences. ChatGPT demonstrates consistent efficiency in execution time and memory usage, while Copilot and DeepSeek show variability as task complexity increases. Gemini, although effective on simpler tasks, requires more attempts as problem difficulty rises. Our findings provide actionable insights into each model's strengths and limitations, offering guidance for developers selecting LLMs for specific coding tasks and providing insights on the performance and complexity of GPT-like generated solutions.", 'authors': ['Everton Guimaraes', 'Nathalia Nascimento', 'Chandan Shivalingaiah', 'Asish Nelapati']}, {'id': 'http://arxiv.org/abs/2508.03929v1', 'title': 'MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework', 'published': '2025-08-05T21:45:36Z', 'updated': '2025-08-05T21:45:36Z', 'abstract': "Designing effective algorithmic components remains a fundamental obstacle in tackling NP-hard combinatorial optimization problems (COPs), where solvers often rely on carefully hand-crafted strategies. Despite recent advances in using large language models (LLMs) to synthesize high-quality components, most approaches restrict the search to a single element - commonly a heuristic scoring function - thus missing broader opportunities for innovation. In this paper, we introduce a broader formulation of solver design as a multi-strategy optimization problem, which seeks to jointly improve a set of interdependent components under a unified objective. To address this, we propose Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a novel framework based on Monte Carlo Tree Search that facilitates turn-based optimization between two LLM agents. At each turn, an agent improves one component by leveraging the history of both its own and its opponent's prior updates, promoting both competitive pressure and emergent cooperation. This structured interaction broadens the search landscape and encourages the discovery of diverse, high-performing solutions. Experiments across multiple COP domains show that MOTIF consistently outperforms state-of-the-art methods, highlighting the promise of turn-based, multi-agent prompting for fully automated solver design.", 'authors': ['Nguyen Viet Tuan Kiet', 'Dao Van Tung', 'Tran Cong Dao', 'Huynh Thi Thanh Binh']}, {'id': 'http://arxiv.org/abs/2508.03905v1', 'title': 'Sotopia-RL: Reward Design for Social Intelligence', 'published': '2025-08-05T20:43:42Z', 'updated': '2025-08-05T20:43:42Z', 'abstract': 'Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.', 'authors': ['Haofei Yu', 'Zhengyang Qi', 'Yining Zhao', 'Kolby Nottingham', 'Keyang Xuan', 'Bodhisattwa Prasad Majumder', 'Hao Zhu', 'Paul Pu Liang', 'Jiaxuan You']}, {'id': 'http://arxiv.org/abs/2508.03860v1', 'title': 'Hallucination to Truth: A Review of Fact-Checking and Factuality   Evaluation in Large Language Models', 'published': '2025-08-05T19:20:05Z', 'updated': '2025-08-05T19:20:05Z', 'abstract': 'Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. The review also discusses the role of instruction tuning, multi-agent reasoning, and external knowledge access via RAG frameworks. Key findings highlight the limitations of current metrics, the value of grounding outputs with validated external evidence, and the importance of domain-specific customization to improve factual consistency. Overall, the review underlines the importance of building LLMs that are not only accurate and explainable but also tailored for domain-specific fact-checking. These insights contribute to the advancement of research toward more trustworthy and context-aware language models.', 'authors': ['Subhey Sadi Rahman', 'Md. Adnanul Islam', 'Md. Mahbub Alam', 'Musarrat Zeba', 'Md. Abdur Rahman', 'Sadia Sultana Chowa', 'Mohaimenul Azam Khan Raiaan', 'Sami Azam']}, {'id': 'http://arxiv.org/abs/2508.03852v2', 'title': 'A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers', 'published': '2025-08-05T19:09:00Z', 'updated': '2025-08-07T01:27:46Z', 'abstract': 'Building 3-D models is challenging for blind and low-vision (BLV) users due to the inherent complexity of 3-D models and the lack of support for non-visual interaction in existing tools. To address this issue, we introduce A11yShape, a novel system designed to help BLV users who possess basic programming skills understand, modify, and iterate on 3-D models. A11yShape leverages LLMs and integrates with OpenSCAD, a popular open-source editor that generates 3-D models from code. Key functionalities of A11yShape include accessible descriptions of 3-D models, version control to track changes in models and code, and a hierarchical representation of model components. Most importantly, A11yShape employs a cross-representation highlighting mechanism to synchronize semantic selections across all model representations -- code, semantic hierarchy, AI description, and 3-D rendering. We conducted a multi-session user study with four BLV programmers, where, after an initial tutorial session, participants independently completed 12 distinct models across two testing sessions, achieving results that aligned with their own satisfaction. The result demonstrates that participants were able to comprehend provided 3-D models, as well as independently create and modify 3-D models -- tasks that were previously impossible without assistance from sighted individuals.', 'authors': ['Zhuohao Jerry Zhang', 'Haichang Li', 'Chun Meng Yu', 'Faraz Faruqi', 'Junan Xie', 'Gene S-H Kim', 'Mingming Fan', 'Angus G. Forbes', 'Jacob O. Wobbrock', 'Anhong Guo', 'Liang He']}, {'id': 'http://arxiv.org/abs/2508.03829v1', 'title': 'Majority Bit-Aware Watermarking For Large Language Models', 'published': '2025-08-05T18:19:00Z', 'updated': '2025-08-05T18:19:00Z', 'abstract': 'The growing deployment of Large Language Models (LLMs) in real-world applications has raised concerns about their potential misuse in generating harmful or deceptive content. To address this issue, watermarking techniques have emerged as a promising solution by embedding identifiable binary messages into generated text for origin verification and misuse tracing. While recent efforts have explored multi-bit watermarking schemes capable of embedding rich information such as user identifiers, they typically suffer from the fundamental trade-off between text quality and decoding accuracy: to ensure reliable message decoding, they have to restrict the size of preferred token sets during encoding, yet such restrictions reduce the quality of the generated content. In this work, we propose MajorMark, a novel watermarking method that improves this trade-off through majority bit-aware encoding. MajorMark selects preferred token sets based on the majority bit of the message, enabling a larger and more flexible sampling of tokens. In contrast to prior methods that rely on token frequency analysis for decoding, MajorMark employs a clustering-based decoding strategy, which maintains high decoding accuracy even when the preferred token set is large, thus preserving both content quality and decoding accuracy. We further introduce MajorMark$^+$, which partitions the message into multiple blocks to independently encode and deterministically decode each block, thereby further enhancing the quality of watermarked text and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs demonstrate that our methods significantly enhance both decoding accuracy and text generation quality, outperforming prior multi-bit watermarking baselines.', 'authors': ['Jiahao Xu', 'Rui Hu', 'Zikai Zhang']}, {'id': 'http://arxiv.org/abs/2508.03793v1', 'title': 'AttnTrace: Attention-based Context Traceback for Long-Context LLMs', 'published': '2025-08-05T17:56:51Z', 'updated': '2025-08-05T17:56:51Z', 'abstract': 'Long-context large language models (LLMs), such as Gemini-2.5-Pro and Claude-Sonnet-4, are increasingly used to empower advanced AI systems, including retrieval-augmented generation (RAG) pipelines and autonomous agents. In these systems, an LLM receives an instruction along with a context--often consisting of texts retrieved from a knowledge database or memory--and generates a response that is contextually grounded by following the instruction. Recent studies have designed solutions to trace back to a subset of texts in the context that contributes most to the response generated by the LLM. These solutions have numerous real-world applications, including performing post-attack forensic analysis and improving the interpretability and trustworthiness of LLM outputs. While significant efforts have been made, state-of-the-art solutions such as TracLLM often lead to a high computation cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a single response-context pair. In this work, we propose AttnTrace, a new context traceback method based on the attention weights produced by an LLM for a prompt. To effectively utilize attention weights, we introduce two techniques designed to enhance the effectiveness of AttnTrace, and we provide theoretical insights for our design choice. We also perform a systematic evaluation for AttnTrace. The results demonstrate that AttnTrace is more accurate and efficient than existing state-of-the-art context traceback methods. We also show that AttnTrace can improve state-of-the-art methods in detecting prompt injection under long contexts through the attribution-before-detection paradigm. As a real-world application, we demonstrate that AttnTrace can effectively pinpoint injected instructions in a paper designed to manipulate LLM-generated reviews. The code is at https://github.com/Wang-Yanting/AttnTrace.', 'authors': ['Yanting Wang', 'Runpeng Geng', 'Ying Chen', 'Jinyuan Jia']}, {'id': 'http://arxiv.org/abs/2508.03686v1', 'title': 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and   Outcome Reward', 'published': '2025-08-05T17:55:24Z', 'updated': '2025-08-05T17:55:24Z', 'abstract': 'Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.', 'authors': ['Shudong Liu', 'Hongwei Liu', 'Junnan Liu', 'Linchen Xiao', 'Songyang Gao', 'Chengqi Lyu', 'Yuzhe Gu', 'Wenwei Zhang', 'Derek F. Wong', 'Songyang Zhang', 'Kai Chen']}, {'id': 'http://arxiv.org/abs/2508.03685v1', 'title': "No LLM Solved Yu Tsumura's 554th Problem", 'published': '2025-08-05T17:55:20Z', 'updated': '2025-08-05T17:55:20Z', 'abstract': "We show, contrary to the optimism about LLM's problem-solving abilities, fueled by the recent gold medals that were attained, that a problem exists -- Yu Tsumura's 554th problem -- that a) is within the scope of an IMO problem in terms of proof sophistication, b) is not a combinatorics problem which has caused issues for LLMs, c) requires fewer proof techniques than typical hard IMO problems, d) has a publicly available solution (likely in the training data of LLMs), and e) that cannot be readily solved by any existing off-the-shelf LLM (commercial or open-source).", 'authors': ['Simon Frieder', 'William Hart']}, {'id': 'http://arxiv.org/abs/2508.03680v1', 'title': 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'published': '2025-08-05T17:50:13Z', 'updated': '2025-08-05T17:50:13Z', 'abstract': "We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.", 'authors': ['Xufang Luo', 'Yuge Zhang', 'Zhiyuan He', 'Zilong Wang', 'Siyun Zhao', 'Dongsheng Li', 'Luna K. Qiu', 'Yuqing Yang']}, {'id': 'http://arxiv.org/abs/2508.03678v1', 'title': 'More Than a Score: Probing the Impact of Prompt Specificity on LLM Code   Generation', 'published': '2025-08-05T17:49:48Z', 'updated': '2025-08-05T17:49:48Z', 'abstract': 'State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general benchmarks like HumanEval but underperform on specialized suites such as ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt detail is given? To answer this, we introduce PartialOrderEval, which augments any code generation benchmark with a partial order of prompts from minimal to maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets of ParEval, we measure how pass@1 scales with prompt specificity. Our experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of prompt sensitivity across different tasks, and a qualitative analysis highlights explicit I/O specifications, edge-case handling, and stepwise breakdowns as the key drivers of prompt detail improvement.', 'authors': ['Yangtian Zi', 'Harshitha Menon', 'Arjun Guha']}, {'id': 'http://arxiv.org/abs/2508.03665v1', 'title': 'A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design', 'published': '2025-08-05T17:24:50Z', 'updated': '2025-08-05T17:24:50Z', 'abstract': 'Generative models, particularly Large Language Models (LLMs), produce fluent outputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and type-theoretic principles to introduce a contract layer that mediates every LLM call. Contracts stipulate semantic and type requirements on inputs and outputs, coupled with probabilistic remediation to steer generation toward compliance. The layer exposes the dual view of LLMs as semantic parsers and probabilistic black-box components. Contract satisfaction is probabilistic and semantic validation is operationally defined through programmer-specified conditions on well-typed data structures. More broadly, this work postulates that any two agents satisfying the same contracts are \\emph{functionally equivalent} with respect to those contracts.', 'authors': ['Claudiu Leoveanu-Condrei']}, {'id': 'http://arxiv.org/abs/2508.03661v1', 'title': 'Automated Algorithmic Discovery for Gravitational-Wave Detection Guided   by LLM-Informed Evolutionary Monte Carlo Tree Search', 'published': '2025-08-05T17:18:20Z', 'updated': '2025-08-05T17:18:20Z', 'abstract': "Computational scientific discovery increasingly relies on algorithms to process complex data and identify meaningful patterns - yet faces persistent challenges in gravitational-wave signal identification. While existing algorithmic approaches like matched filtering (MF) and deep neural networks (DNNs) have achieved partial success, their limitations directly stem from fundamental limitations: MF's excessive computational demands arise from its reliance on predefined theoretical waveform templates, while DNNs' black-box architectures obscure decision logic and introduce hidden biases. We propose Evolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses these limitations through systematic algorithm space exploration guided by domain-aware physical constraints. Our approach combines tree-structured search with evolutionary optimization and large language model heuristics to create interpretable algorithmic solutions. Our Evo-MCTS framework demonstrates substantial improvements, achieving a 20.2\\% improvement over state-of-the-art gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset. High-performing algorithm variants consistently exceed thresholds. The framework generates human-interpretable algorithmic pathways that reveal distinct performance patterns. Beyond performance improvements, our framework discovers novel algorithmic combinations, thereby establishing a transferable methodology for automated algorithmic discovery across computational science domains.", 'authors': ['He Wang', 'Liang Zeng']}, {'id': 'http://arxiv.org/abs/2508.03628v1', 'title': 'LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for   Advertiser Keyphrase Recommendations at eBay', 'published': '2025-08-05T16:47:17Z', 'updated': '2025-08-05T16:47:17Z', 'abstract': 'Sellers at eBay are recommended keyphrases to bid on to enhance the performance of their advertising campaigns. The relevance of these keyphrases is crucial in avoiding the overcrowding of search systems with irrelevant items and maintaining a positive seller perception. It is essential that keyphrase recommendations align with both seller and Search judgments regarding auctions. Due to the difficulty in procuring negative human judgment at scale, employing LLM-as-a-judge to mimic seller judgment has been established as the norm in several studies. This study introduces a novel two-step LLM distillation process from a LLM-judge used to debias our Embedding Based Retrieval (EBR) model from the various biases that exist in click-data. We distill from an LLM teacher via a cross-encoder assistant into a bi-encoder student using a multi-task training approach, ultimately employing the student bi-encoder to retrieve relevant advertiser keyphrases. We show that integrating a knowledge distillation process from LLMs in a multi-task training setup enhances bi-encoder performance in retrieving relevant advertiser keyphrases at eBay.', 'authors': ['Soumik Dey', 'Benjamin Braun', 'Naveen Ravipati', 'Hansi Wu', 'Binbin Li']}, {'id': 'http://arxiv.org/abs/2508.03622v1', 'title': 'Refining Critical Thinking in LLM Code Generation: A Faulty   Premise-based Evaluation Framework', 'published': '2025-08-05T16:39:39Z', 'updated': '2025-08-05T16:39:39Z', 'abstract': 'With the advancement of code generation capabilities in large language models (LLMs), their reliance on input premises has intensified. When users provide inputs containing faulty premises, the probability of code generation hallucinations rises significantly, exposing deficiencies in their self-scrutiny capabilities. This paper proposes Faulty Premises Bench (FPBench), the first code generation evaluation framework targeting faulty premises. By systematically constructing three categories of faulty premises and integrating multi-dimensional evaluation metrics, it conducts in-depth assessments of 15 representative LLMs. The key findings are as follows: (1) Most models exhibit poor reasoning abilities and suboptimal code generation performance under faulty premises, heavily relying on explicit prompts for error detection, with limited self-scrutiny capabilities; (2) Faulty premises trigger a point of diminishing returns in resource investment, leading to blindly increasing length fails to enhance quality; (3) The three types of faulty premises respectively activate distinct defect patterns in models, revealing a triple dissociation in the cognitive mechanisms of code generation models. This study not only highlights the urgent need for LLMs to proactively verify premises in code generation but also, through the proposed FPBench framework and multi-dimensional evaluation system, provides a theoretical foundation and practical pathway for developing reliable, human-centric code generation models.', 'authors': ['Jialin Li', 'Jinzhe Li', 'Gengxu Li', 'Yi Chang', 'Yuan Wu']}, {'id': 'http://arxiv.org/abs/2508.03611v1', 'title': 'Block: Balancing Load in LLM Serving with Context, Knowledge and   Predictive Scheduling', 'published': '2025-08-05T16:27:10Z', 'updated': '2025-08-05T16:27:10Z', 'abstract': 'This paper presents Block, a distributed scheduling framework designed to optimize load balancing and auto-provisioning across instances in large language model serving frameworks by leveraging contextual information from incoming requests. Unlike popular model serving systems that rely on monolithic and heuristic task schedulers, Block operates as a fully distributed, stateless, and predictive scheduling system to achieve low overhead, reliability, and scalability. It leverages the deterministic and predictable characteristics of LLM inferences, such as host configurations, response lengths, and hardware performance, to make scheduling decisions based on accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block significantly outperforms heuristic schedulers, boosting serving capacity by up to 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance gains remain consistent across diverse models, workloads and configurations. Code and data are open-sourced.', 'authors': ['Wei Da', 'Evangelia Kalyvianaki']}, {'id': 'http://arxiv.org/abs/2508.03603v1', 'title': 'ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated   Test Programs', 'published': '2025-08-05T16:17:02Z', 'updated': '2025-08-05T16:17:02Z', 'abstract': "Existing LLM-based compiler fuzzers often produce syntactically or semantically invalid test programs, limiting their effectiveness in exercising compiler optimizations and backend components. We introduce ReFuzzer, a framework for refining LLM-generated test programs by systematically detecting and correcting compilation and runtime violations (e.g. division by zero or array out-of-bounds accesses). ReFuzzer employs a feedback loop with a local LLM to validate and filter erroneous programs before execution, improving fuzzing effectiveness beyond crash detection and enabling the generation of diverse yet valid test programs.   We evaluated ReFuzzer's effectiveness across black-, grey- and white-box fuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs' validity from 47.0-49.4% to 96.6-97.3%, with an average processing time of 2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing significantly increased code coverage in critical optimization and IR generation components. For example, vectorization coverage had an absolute improvement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing, enhancing testing effectiveness.", 'authors': ['Iti Shree', 'Karine Even-Mendoz', 'Tomasz Radzik']}, {'id': 'http://arxiv.org/abs/2508.03583v1', 'title': 'OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering   Dataset', 'published': '2025-08-05T15:50:16Z', 'updated': '2025-08-05T15:50:16Z', 'abstract': 'Lifelogging refers to the process of passively collecting, storing, and analysing personal daily life data using wearable devices. This data can support applications in memory preservation and enhancement. For example, using an ask-and-answer strategy, question-answering (QA) on lifelog data opens an interactive and interesting way to explore memorable events and insights into daily life. However, research resources for QA on lifelog data are limited to small-sized or synthetic QA datasets. In this paper, we present a novel lifelog QA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our dataset focuses on an open-ended and practical QA with real-world application in daily lifelog usage. We construct 14,187 pairs of Q&A with diverse types and difficulty levels. A baseline experiment is reported for this dataset with competitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665 LLM Score from LLaVA-NeXT-Interleave 7B model. We release this Q&A dataset to the research community to support new research into lifelog technologies, such as enabling personal chat-based assistants for lifelog data to become a reality.', 'authors': ['Quang-Linh Tran', 'Binh Nguyen', 'Gareth J. F. Jones', 'Cathal Gurrin']}]