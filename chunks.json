[
    {
        "Table_of_Contents": "Table of Contents\n  1. [ Abstract  ](https://arxiv.org/html/2508.09129v1#abstract \"Abstract\")\n  2. [1 Introduction](https://arxiv.org/html/2508.09129v1#S1 \"In BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n  3. [2 Planner-Executor Agent Pair](https://arxiv.org/html/2508.09129v1#S2 \"In BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    1. [2.1 Workflow Overview](https://arxiv.org/html/2508.09129v1#S2.SS1 \"In 2 Planner-Executor Agent Pair \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    2. [2.2 Planner: Confidence-Guided Replanning for Persistent Exploration](https://arxiv.org/html/2508.09129v1#S2.SS2 \"In 2 Planner-Executor Agent Pair \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    3. [2.3 Executor: Tool-Augmented Browse Worker Mechanism](https://arxiv.org/html/2508.09129v1#S2.SS3 \"In 2 Planner-Executor Agent Pair \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n  4. [3 Tool-Augmented Programmatic Sandbox](https://arxiv.org/html/2508.09129v1#S3 \"In BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    1. [3.1 Standardized Search Programming Primitives](https://arxiv.org/html/2508.09129v1#S3.SS1 \"In 3 Tool-Augmented Programmatic Sandbox \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    2. [3.2 Tools](https://arxiv.org/html/2508.09129v1#S3.SS2 \"In 3 Tool-Augmented Programmatic Sandbox \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    3. [3.3 Execution Environment](https://arxiv.org/html/2508.09129v1#S3.SS3 \"In 3 Tool-Augmented Programmatic Sandbox \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n  5. [4 Experiments](https://arxiv.org/html/2508.09129v1#S4 \"In BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    1. [4.1 Experimental Setups](https://arxiv.org/html/2508.09129v1#S4.SS1 \"In 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    2. [4.2 Main Results](https://arxiv.org/html/2508.09129v1#S4.SS2 \"In 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n    3. [4.3 Analysis](https://arxiv.org/html/2508.09129v1#S4.SS3 \"In 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n  6. [5 Related Works](https://arxiv.org/html/2508.09129v1#S5 \"In BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n  7. [6 Conclusions](https://arxiv.org/html/2508.09129v1#S6 \"In BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n  8. [A Cases](https://arxiv.org/html/2508.09129v1#A1 \"In BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")\n  9. [ References  ](https://arxiv.org/html/2508.09129v1#bib \"References\")\n\n\n# BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\nReport issue for preceding element\nXianghe Pang* Shuo Tang* Rui Ye Yuwen Du Yaxin Du Siheng Chen\u2020 School of Artificial Intelligence, Shanghai Jiao Tong University [https://github.com/sjtu-sai-agents/BrowseMaster](https://github.com/sjtu-sai-agents/Browse-Master)\nReport issue for preceding element\n###### Abstract\nReport issue for preceding element\nEffective information seeking in the vast and ever-growing digital landscape requires balancing expansive search with strategic reasoning. Current large language model (LLM)-based agents struggle to achieve this balance due to limitations in search breadth and reasoning depth, where slow, serial querying restricts coverage of relevant sources and noisy raw inputs disrupt the continuity of multi-step reasoning. To address these challenges, we propose BrowseMaster, a scalable framework built around a programmatically augmented planner-executor agent pair. The planner formulates and adapts search strategies based on task constraints, while the executor conducts efficient, targeted retrieval to supply the planner with concise, relevant evidence. This division of labor preserves coherent, long-horizon reasoning while sustaining broad and systematic exploration, overcoming the trade-off that limits existing agents. Extensive experiments on challenging English and Chinese benchmarks show that BrowseMaster consistently outperforms open-source and proprietary baselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh, which demonstrates its strong capability in complex, reasoning-heavy information-seeking tasks at scale.\nReport issue for preceding element\n\u2020\u2020footnotetext: * Equal contributions. \u2020 Corresponding author: sihengc@sjtu.edu.cn. ![Refer to caption](https://arxiv.org/html/x1.png) Figure 1: Comparisons on BrowseComp. Our _BrowseMaster_ achieves the score of 30%, surpassing deep research products from other baselines. Report issue for preceding element"
    },
    {
        "1_Introduction": "1 Introduction\nReport issue for preceding element\nInformation seeking has been the engine of human progress, fueling discovery, shaping collective knowledge, and steering societal evolution (Marchionini, [1995](https://arxiv.org/html/2508.09129v1#bib.bib21); Given et al., [2023](https://arxiv.org/html/2508.09129v1#bib.bib8)). The advent of search engines (e.g., Google Search (Brin and Page, [1998](https://arxiv.org/html/2508.09129v1#bib.bib1))) constituted a paradigm shift, replacing slow, geographically constrained exploration with instantaneous, large-scale access to the world\u2019s digitized knowledge. Now, the rise of large language model (LLM)-based agents (e.g., Deep Research from OpenAI (OpenAI, [2025](https://arxiv.org/html/2508.09129v1#bib.bib26))) signals the next revolution: systems capable of autonomously and tirelessly retrieving, synthesizing, and reasoning over web information\u2014transcending the cognitive and operational limits of humans\u2019 search and charting a path toward automated information seeking.\nReport issue for preceding element\nEffective information seeking requires reasoning to formulate precise search strategies and breadth to ensure comprehensive coverage of relevant information. For example, identifying the title of a 2018\u20132023 EMNLP paper whose first author studied at Dartmouth College and whose fourth author studied at the University of Pennsylvania demands reasoning over these constraints to devise an efficient search plan, while sustaining broad exploration to avoid missing the correct result. Without sufficient reasoning, the process devolves into brute-force examination of thousands of papers; without sufficient breadth, it risks prematurely excluding the correct target. By uniting strategic reasoning with expansive search, agents can tackle such tasks both effectively and at scale.\nReport issue for preceding element\nHowever, current LLM-based agents, remain constrained in their ability to combine expansive search with strategic reasoning. First, their search breadth is limited: most invoke web browsing tools via natural language and process queries serially, resulting in a one-page-at-a-time workflow that drastically reduces the number of sources examined and undermines comprehensive coverage (Wu et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib33)). Second, their reasoning depth is shallow: each tool invocation injects raw web content into the agent\u2019s context, interrupting the flow of reasoning and fragmenting multi-step inference (Li et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib18)). These limitations, acting in tandem, lead to near-zero accuracy on challenging information-seeking tasks (Li et al., [2025b](https://arxiv.org/html/2508.09129v1#bib.bib19); Jin et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib13); Li et al., [2025c](https://arxiv.org/html/2508.09129v1#bib.bib20)), highlighting the urgent need for architectures that can maintain broad exploration while preserving coherent reasoning.\nReport issue for preceding element\nTo address the limitations in achieving both search breadth and reasoning depth, we present BrowseMaster, a framework for scalable, reasoning-intensive web browsing built around a tightly coordinated planner\u2013executor agent pair. In our design, the planner focuses on high-level reasoning, formulating strategies and delegating well-defined sub-tasks to the executor; while the executor concentrates on executing these tasks through multi-step interactions with the environment. This separation keeps the planner\u2019s context clean, shielding its reasoning process from noisy environmental outputs, and allows the executor to remain fully engaged with sub-task execution and high-volume interactions.\nReport issue for preceding element\nThe two components in BrowseMaster play distinct yet complementary roles: (1) Planner: long-horizon strategist. The planner interprets the task, extracts key constraints, and formulates a search strategy that incrementally refines the problem space. Operating solely over structured outputs returned by the executor, it avoids the fragmentation of multi-step reasoning caused by direct exposure to raw, unprocessed web content. It further employs confidence-guided replanning, which resets its context and revises the strategy when confidence is low, thus preventing premature convergence and enabling adaptive reasoning over extended horizons. (2) Executor: scalable search engine. The executor enables expansive, efficient search at scale by interacting with tools programmatically, representing operations such as search, parse, and check as composable code primitives. This design allows selective extraction of relevant information (e.g., printing only pertinent pages), drastically reducing context size and processing overhead. By encoding complex search workflows in compact code, the executor can sustain a high volume of environment interactions without overloading the reasoning proces, overcoming the inefficiencies of prior agents that rely on slow, natural-language tool calls. Together, the planner maintains coherent reasoning while the executor ensures broad, systematic exploration, enabling BrowseMaster to achieve scalable and effective information seeking.\nReport issue for preceding element\nExperimentally, we evaluate BrowseMaster on challenging web browsing benchmarks covering both English and Chinese tasks, against a range of open-source and proprietary agents. Results demonstrate that BrowseMaster leverages creative, code-based search strategies to efficiently navigate thousands of pages and reason effectively over diverse search cues, consistently delivering strong performance on long-horizon, information-rich tasks. On BrowseComp-en (Wei et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib32)), it achieves a score of 30.0, becoming the first open-source agent to reach this milestone. On BrowseComp-zh (Zhou et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib40)), it surpasses OpenAI\u2019s DeepResearch (OpenAI, [2025](https://arxiv.org/html/2508.09129v1#bib.bib26)) by 4% and outperforms other advanced proprietary models such as o1 (OpenAI, [2024b](https://arxiv.org/html/2508.09129v1#bib.bib25)) and Doubao (ByteDance Doubao, [2025](https://arxiv.org/html/2508.09129v1#bib.bib2)).\nReport issue for preceding element"
    },
    {
        "2_Planner-Executor Agent Pair": "2 Planner-Executor Agent Pair\nReport issue for preceding element\nThis section presents the design of our _Planner-Executor Agent Pair_ , beginning with an overview, followed by the design of the planner and executor components.\nReport issue for preceding element\n![Refer to caption](https://arxiv.org/html/x2.png) Figure 2: The architecture of _BrowseMaster_. Report issue for preceding element",
        "2_1_Workflow Overview": "2.1 Workflow Overview\nReport issue for preceding element\nOur workflow primarily focuses on providing a more efficient context management mechanism to further enhance the search breadth and the reasoning depth during agent browsing. This improvement targets two key performance dimensions: 1) Complex reasoning and plannin, the agent must adapt search strategies dynamically by leveraging diverse clues encountered during browsing; 2) Execution capability \u2013 the agent must sustain a high volume of tool calls to gather necessary information, while detecting and recovering from tool failures or network issues. To this end, we extend the standard ReAct architecture by introducing two specialized agents (Sections [2.2](https://arxiv.org/html/2508.09129v1#S2.SS2 \"2.2 Planner: Confidence-Guided Replanning for Persistent Exploration \u2023 2 Planner-Executor Agent Pair \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\") and [2.3](https://arxiv.org/html/2508.09129v1#S2.SS3 \"2.3 Executor: Tool-Augmented Browse Worker Mechanism \u2023 2 Planner-Executor Agent Pair \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\")): a planner responsible for strategic reasoning and planning, and an executor responsible for tool-augmented task execution. In each execution cycle, the planner processes the user query, performs reasoning, and decomposes the task into subtasks. Information retrieval subtasks are delegated to the executor, which interacts with tools programmatically. Through a sequence of tool invocations, the executor produces distilled intermediate results and returns them to the planner for coordination and integration.\nReport issue for preceding element\nThis design offers two main advantages. (1) Preserving reasoning depth. By isolating tool execution from the planner, we prevent noisy execution details from disrupting multi-step inference. (2) Expanding search breadth. By delegating well-defined subtasks, the executor can perform searches that are both more targeted and more extensive. The overall architecture is illustrated in Figure [2](https://arxiv.org/html/2508.09129v1#S2.F2 \"Figure 2 \u2023 2 Planner-Executor Agent Pair \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\").\nReport issue for preceding element",
        "2_2_Planner: Confidence-Guided Replanning for Persistent Exploration": "2.2 Planner: Confidence-Guided Replanning for Persistent Exploration\nReport issue for preceding element\nThe planner performs long-horizon reasoning over the input search task by decomposing it into manageable sub-tasks and delegating their execution to the executor. During reasoning, the planner invokes the executor by enclosing the assigned sub-task within a <task>Report issue for preceding element </task>Report issue for preceding elementblock. Upon completion, the executor\u2019s outputs are inserted into the <result>Report issue for preceding element </result>Report issue for preceding elementblock, after which the planner continues reasoning with its updated context. To enhance inference-time scalability, the planner produces a confidence score when arriving at a final answer; if the score is below a predefined threshold, it triggers replanning to refine the solution.\nReport issue for preceding element\nHere, the planner is driven by a reasoning model, leveraging the model\u2019s inherent logical reasoning capabilities to analyze and decompose complex tasks, rather than relying on a fixed workflow.\nReport issue for preceding element",
        "2_3_Executor: Tool-Augmented Browse Worker Mechanism": "2.3 Executor: Tool-Augmented Browse Worker Mechanism\nReport issue for preceding element\nThe executor is responsible for maximizing both the quantity and quality of tool calls to collect as much accurate and relevant information as possible for the planner. Since task decomposition is handled by the planner, the executor\u2019s role is not to break down tasks, but to explore unsearched aspects of the information space. Its behavior is therefore primarily operational, involving systematic web browsing, information gathering, and refinement. To ensure efficient and comprehensive information collection, the executor incorporates the following key components:\nReport issue for preceding element\nUsing code execution as interaction. We enable the model to invoke tools by generating executable code within <code>Report issue for preceding element </code>Report issue for preceding elementtags. The extracted code segment, identified via matching rules, is executed in a sandboxed environment with the relevant tool functions pre-imported. Execution outputs are then wrapped in <execution_results>Report issue for preceding element </execution_results>Report issue for preceding elementtags and appended to the model\u2019s context, allowing inference to continue seamlessly. Details of the available tools and execution environment are provided in Sections [3.2](https://arxiv.org/html/2508.09129v1#S3.SS2 \"3.2 Tools \u2023 3 Tool-Augmented Programmatic Sandbox \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\") and [3.3](https://arxiv.org/html/2508.09129v1#S3.SS3 \"3.3 Execution Environment \u2023 3 Tool-Augmented Programmatic Sandbox \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\").\nReport issue for preceding element\nStandardized search programming primitives. Just as Python ships with a rich standard library to encapsulate common operations, web search agents can benefit from built-in, task-specific primitives. In large-scale information seeking, certain patterns recur frequently\u2014such as expanding a query with multiple keyword variants or verifying whether a retrieved page contains target information.\nReport issue for preceding element\nWithout such primitives, these steps must be reimplemented from scratch, causing redundancy and a higher risk of errors. Abstracting them into _modular, reusable functions_ that encapsulate common search behaviors gives the agent a stable, high-level API for tool interaction.\nReport issue for preceding element\nThis design offers two main benefits: i) reducing redundancy, as the same primitive can serve diverse tasks without rewriting low-level logic; and ii) improving flexibility and scalability, as primitives can be composed or customized to dynamically refine search strategies. Overall, encapsulating search logic in such modular units enables efficient, adaptable, and extensible web exploration.\nReport issue for preceding element"
    },
    {
        "3_Tool-Augmented Programmatic Sandbox": "3 Tool-Augmented Programmatic Sandbox\nReport issue for preceding element\nTo equip the executor with reliable and expressive tool-use capabilities, we introduce the _tool-augmented programmatic sandbox_ , a unified framework for precise and controllable interaction with the external environment. The sandbox exposes standardized programming primitives tailored for web-based tasks and supports code execution within a lightweight, isolated runtime. It serves as the execution backbone of our agent, translating the planner\u2019s strategic intent into actionable and verifiable operations.\nReport issue for preceding element",
        "3_1_Standardized Search Programming Primitives": "3.1 Standardized Search Programming Primitives\nReport issue for preceding element\nIn web search tasks, procedural control structures (e.g., loops and conditional branches) can substantially improve execution efficiency. For example, a single code execution may generate numerous search queries, perform concurrent retrieval via multithreading, and filter the results according to unified rules. However, directly prompting the model to write complete control code often leads to instability: webpages differ widely in format and structure, making it challenging to implement universal filtering strategies. As a result, generated code frequently fails in handling corner cases, causing wasted time on debugging and error correction.\nReport issue for preceding element\nTo address this, we design a set of standardized programming primitives specifically for agent-based web search: generate_keywords, batch_search, and check_condition. These encapsulate the key capabilities of generating search queries, performing parallel retrieval, and applying programmable filtering logic.\nReport issue for preceding element\ngenerate_keywords(seed_keyword) generates a set of search terms starting from a seed keyword, producing advanced search expressions such as conditional filters or domain-specific queries (e.g., restricting to Wikipedia). The goal is to broaden coverage and capture semantically related content that may not be retrieved with a single query.\nReport issue for preceding element\nbatch_search(key_words) executes multiple web searches in parallel, substantially improving efficiency over traditional step-by-step querying. Rather than issuing individual search requests sequentially, the agent can submit an entire batch of queries simultaneously and receive all results in a single step. The input is a list of search keywords, either generated directly by the agent or derived from the output of generate_keywords. This parallel execution enables the agent to retrieve information from a large number of webpages quickly, while maintaining both coverage and speed.\nReport issue for preceding element\ncheck_condition(web_page, condition) In large-scale web search, agents must process and analyze substantial volumes of information, making efficient filtering and conditional evaluation essential. The check_condition primitive offers a programmable interface for code-driven, large-scale content evaluation, replacing slow, sequential manual inspection by the model. It accepts two inputs: (1) a batch of document contents (e.g., webpage text), and (2) a declarative condition expressed as a model-generated predicate or logical statement. It returns a Boolean value for each input\u2014True if the condition is met, and False if it is not satisfied or cannot be determined from the content. By leveraging check_condition, agents can construct efficient, logic-based filtering pipelines and make control-flow decisions grounded in semantic conditions. This abstraction supports scalable post-processing of web data and fine-grained control over downstream decision-making, all within a code-executed framework.\nReport issue for preceding element\nBy using these structured functions, the model can write more reliable and maintainable code, significantly improving execution stability and reducing implementation complexity.\nReport issue for preceding element",
        "3_2_Tools": "3.2 Tools\nReport issue for preceding element\nTo mimic human-like online information-seeking, we design two essential tools: web search and web parse. The web search tool empowers the agent to identify relevant web pages based on the question. It delivers concise summaries for each retrieved page, allowing the agent to strategically determine which links warrant deeper exploration. The web parse tool is employed when the agent requires in-depth analysis of a selected webpage to extract information directly related to the user query.\nReport issue for preceding element\nWeb search. The web search tool utilizes Google search engine to pinpoint the most relevant webpages based on a user\u2019s query. It delivers three key categories of valuable information: (i) Entity-related facts: For queries involving recognizable entities (such as a company or software application), the tool identifies them and pulls structured facts from its knowledge graph. This includes the entity\u2019s name, a brief description, and essential attributes. By extracting these details, the agent can quickly grasp the query\u2019s central concept, offering vital context for further analysis. (ii) Relevant webpage previews: For each matching page, the tool supplies a preview that includes the title, URL, and an informative snippet. This allows the agent to rapidly evaluate the page\u2019s relevance and decide which ones merit closer inspection. (iii) Related search queries: The tool also suggests common follow-up searches, giving the agent options to refine or expand the investigation and foster a more comprehensive grasp of the topic.\nReport issue for preceding element\nWeb parse. The web parse tool supports two specialized parsing approaches, one for standard webpages and another for scientific papers: (i) General webpage parsing: This strategy starts by extracting the main content from the target webpage. To ensure robust operation, a fallback mechanism is incorporated to manage instances where direct content extraction fails. Once the content is obtained, the tool highlights sections most pertinent to the query. It also automatically identifies and lists links to related subpages, complete with short descriptions. This capability lets the agent delve deeper into connected content, mimicking human web navigation\u2014scanning links, following trails, and building a fuller picture of the topic. (ii) Scientific paper parsing: For scientific papers, the tool uses a two-step strategy to ensure reliable content retrieval. It first attempts to fetch an HTML version of the publication from ar5iv. In the event of an unsuccessful or incomplete HTML fetch, the system switches to downloading the official PDF. With the full document in hand, the tool then extracts details directly tied to the query.\nReport issue for preceding element\nTogether, the web search and web parse tools empower the agent not just to locate key information, but to navigate the web in a natural, human-inspired manner\u2014through iterative searching, previewing, linking, and in-depth exploration as required.\nReport issue for preceding element",
        "3_3_Execution Environment": "3.3 Execution Environment\nReport issue for preceding element\nWe enable agents to invoke tools through code generation. However, conventional stateless code execution sandboxes are poorly suited for multi-step tool use, as agents often define functions or variables in earlier code blocks and reference them later. In a stateless sandbox, each execution occurs in an isolated memory space, preventing access to previously defined entities and severely restricting coding flexibility.\nReport issue for preceding element\nTo overcome this limitation, we design a stateful code execution sandbox. Each agent is allocated an isolated execution environment with persistent memory, allowing the execution state to be preserved and restored between runs. This design offers a Jupyter Notebook\u2013like experience, enabling agents to flexibly define and reuse functions, classes, and objects across multiple steps. Meanwhile, different queries are executed in fully isolated contexts, ensuring clean separation and preventing cross-task interference.\nReport issue for preceding element"
    },
    {
        "4_Experiments": "4 Experiments\nReport issue for preceding element",
        "4_1_Experimental Setups": "4.1 Experimental Setups\nReport issue for preceding element\nAgent. We employ DeepSeek-R1-0528 (DeepSeek-AI, [2025](https://arxiv.org/html/2508.09129v1#bib.bib7)) to drive the planner and DeepSeek-R1 for the executor. The maximum completion of tokens is set to 64k with a temperature of 0.6.\nReport issue for preceding element\nBenchmarks. We evaluate our method on five challenging benchmarks: BrowseComp (Wei et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib32)), a highly demanding benchmark designed to assess the ability to locate complex, entangled information; BrowseComp-zh (Zhou et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib40)), a Chinese counterpart to BrowseComp with similar objectives; xBench-DeepResearch (Chen et al., [2025b](https://arxiv.org/html/2508.09129v1#bib.bib5)), a dynamic benchmark focused on evaluating tool usage in search and information retrieval tasks; GAIA (Mialon et al., [2023](https://arxiv.org/html/2508.09129v1#bib.bib23)), which tests reasoning, web browsing, and general tool-use capabilities; and WebWalkerQA (Wu et al., [2025b](https://arxiv.org/html/2508.09129v1#bib.bib34)), which assesses agents\u2019 ability to navigate and process complex, multi-layered web information.\nReport issue for preceding element\nDue to resource constraints of the search API, we randomly sample 200 examples for BrowseComp and BrowseComp-zh. For GAIA, we use the text-only queries from its validation set (103 samples). Evaluation employs xVerify-9B (Chen et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib4)) for BrowseComp, BrowseComp-zh, and xBench-DeepResearch, GPT-4o (OpenAI, [2024a](https://arxiv.org/html/2508.09129v1#bib.bib24)) for WebWalkerQA and GAIA following Wu et al. ([2025b](https://arxiv.org/html/2508.09129v1#bib.bib34)).\nReport issue for preceding element\nBaselines. We compare our performance against systems from three categories: proprietary deep research agents (OpenAI (OpenAI, [2025](https://arxiv.org/html/2508.09129v1#bib.bib26)), Gemini 2.5 (Google, [2025](https://arxiv.org/html/2508.09129v1#bib.bib9)), Grok3 (x.ai, [2025](https://arxiv.org/html/2508.09129v1#bib.bib36)), Doubao (ByteDance Doubao, [2025](https://arxiv.org/html/2508.09129v1#bib.bib2)), and Metaso (Metaso, [2025](https://arxiv.org/html/2508.09129v1#bib.bib22))), advanced models (QwQ (Qwen Team, [2025](https://arxiv.org/html/2508.09129v1#bib.bib28)), DeepSeek-R1 (DeepSeek-AI, [2025](https://arxiv.org/html/2508.09129v1#bib.bib7)), GPT-4o (OpenAI, [2024a](https://arxiv.org/html/2508.09129v1#bib.bib24)), Gemini 2.5 Pro (DeepMind, [2025](https://arxiv.org/html/2508.09129v1#bib.bib6)), and o1 (OpenAI, [2024b](https://arxiv.org/html/2508.09129v1#bib.bib25))), and open-source agents (WebThinker (Li et al., [2025c](https://arxiv.org/html/2508.09129v1#bib.bib20)), WebDancer (Wu et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib33)), WebSailor (Li et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib18)), WebShaper (Tao et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib30)), and Agentic Reasoning (Wu et al., [2025c](https://arxiv.org/html/2508.09129v1#bib.bib35))). Due to limited API access for proprietary agents and models, not all systems were evaluated across every benchmark. For open-source agents without full accessibility, we use their officially reported results from their respective papers (Li et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib18); Tao et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib30)).\nReport issue for preceding element\nTable 1: Performance comparison against proprietary agents, advanced models, and open-source agents on five benchmarks. BrowseMaster outperforms all open-source agents and advanced models, as well as most proprietary deep research agents.\n| BrowseComp | BrowseComp-zh | xbench-DeepSearch | GAIA | WebWalkerQA  \n---|---|---|---|---|---  \nProprietary Agents |  |  |  |  |   \nOpenAI DeepResearch | 51.5 | 42.9 | - | 67.4 | -  \nGrok3 DeepResearch | - | 12.9 | 50+ | - | -  \nDoubao DeepResearch | - | 26.0 | 50+ | - | -  \nMetaso DeepResearch | 12.0 | 45.3 | 64.0 | - | -  \nModels |  |  |  |  |   \nQwQ | 0.5 | 10.0 | 10.7 | 22.3 | 4.3  \nDeepSeek-R1 | 2.0 | 23.2 | 32.7 | 16.5 | 10.0  \nGPT-4o | 0.6 | 6.2 | 18.0 | 17.5 | 5.5  \nGemini 2.5 Pro | 7.6 | 27.3 | - | - | -  \nOpenAI o1 | 9.9 | 29.1 | - | - | 9.9  \nOpen-source Agents |  |  |  |  |   \nWebThinker | 1.5 | 7.3 | 24.0 | 48.5 | 39.4  \nWebDancer | 3.8 | 18.0 | 39.0 | 51.5 | 43.2  \nWebSailor | 12.0 | 30.1 | 55.0 | 55.4 | -  \nWebShaper | - | - | - | 60.2 | 52.2  \nAgentic Reasoning | 5.5 | 29.0 | 40.0 | 42.2 | 36.9  \nBrowseMaster | 30.0 | 46.5 | 66.0 | 68.0 | 62.1  \nReport issue for preceding element",
        "4_2_Main Results": "4.2 Main Results\nReport issue for preceding element\nBrowseMaster achieves superior performance over both open-source and proprietary agents. As the first open-source agent to exceed a 30% score, BrowseMaster represents a significant leap forward, showcasing the power of programmatic execution and agentic workflows. While leading deep research agents are typically proprietary, BrowseMaster establishes an open-source paradigm for tackling challenging search tasks. Notably, it to outperform systems like Grok3 and Doubao DeepResearch and achieves a 4% performance advantage over OpenAI\u2019s DeepResearch on BrowseComp-zh.\nReport issue for preceding element\nBrowseMaster excels consistently across diverse benchmarks and languages. BrowseMaster adaptively handles both complex search tasks like BrowseComp and web traversal challenges like WebWalkerQA in both Chinese and English, demonstrating its versatility. Performance gain is particularly impressive on deep research benchmarks, where persistent exploration and broad coverage are critical, underscoring BrowseMaster\u2019s exceptional design for search breadth and reasoning depth.\nReport issue for preceding element\nTool-augmented reasoning significantly boosts performance on information-seeking tasks. Advanced standalone models like GPT-4o and DeepSeek-R1 achieve near-zero performance on BrowseComp, indicating that raw models struggle without web interaction. Equipped with web-browsing capabilities, BrowseMaster substantially enhances DeepSeek-R1\u2019s performance, surpassing proprietary models like Gemini 2.5 Pro and o1. By accessing, filtering, and reasoning over vast web data, BrowseMaster tackles real-world challenges unattainable by pure language models.\nReport issue for preceding element",
        "4_3_Analysis": "4.3 Analysis\nReport issue for preceding element\n![Refer to caption](https://arxiv.org/html/x3.png) Report issue for preceding element\n![Refer to caption](https://arxiv.org/html/x4.png) Report issue for preceding element\nFigure 3: Performance comparison in terms of search call volume and total token usage. Scaling search calls and computation drives performance gains. Report issue for preceding element\n![Refer to caption](https://arxiv.org/html/x5.png) Figure 4: Comparison of tool calls per invocation. Code-driven execution enables highly efficient tool calls. Report issue for preceding element\nReport issue for preceding element\nScaling search calls empowers BrowseMaster to achieve performance breakthrough. Figure [4](https://arxiv.org/html/2508.09129v1#S4.F4 \"Figure 4 \u2023 4.3 Analysis \u2023 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\") illustrates the performance of BrowseMaster and baselines on BrowseComp as a function of search call. We evaluate BrowseMaster across configurations combining the executor with and without primitives and planner. The results show that i) at equivalent search call levels, BrowseMaster surpasses existing open-source agents; ii) scaling search call volume is critical for enhancing agent performance, as relying on fewer than 10 searches is often impractical for challenging search tasks; and iii) BrowseMaster\u2019s search capabilities significantly enhance the performance of the pure model.\nReport issue for preceding element\nScaling computation empowers BrowseMaster to achieve performance breakthrough. Figure [4](https://arxiv.org/html/2508.09129v1#S4.F4 \"Figure 4 \u2023 4.3 Analysis \u2023 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\") illustrates the performance of BrowseMaster and baselines on BrowseComp as a function of total token usage. The results demonstrate that BrowseMaster significantly enhances agent performance by scaling computation. This scaling arises from the synergistic collaboration between the planner and executor. The planner decomposes complex problems into manageable subtasks, allowing the executor to tackle lower-difficulty tasks incrementally, progressively solving the overall problem. Increased computational resources enable BrowseMaster to reason deeply, connect clues, optimize search directions, and validate results.\nReport issue for preceding element\n![Refer to caption](https://arxiv.org/html/x6.png) Figure 5: Interaction times between planner and executor across benchmarks. Complex tasks require increased task decomposition and confidence-guided retries. Report issue for preceding element\n![Refer to caption](https://arxiv.org/html/x7.png) (a) BrowseMaster. Report issue for preceding element\n![Refer to caption](https://arxiv.org/html/x8.png) (b) WebDancer. Report issue for preceding element\nFigure 6: Visualization of pages visited by BrowseMaster versus WebDancer on BrowseComp. BrowseMaster\u2019s search covers more diverse sources. Report issue for preceding element\nReport issue for preceding element\nProgrammatic tool use enhances search efficiency and enables broader exploration. Figure [4](https://arxiv.org/html/2508.09129v1#S4.F4 \"Figure 4 \u2023 4.3 Analysis \u2023 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\") compares the number of tool calls per invocation between BrowseMaster and WebThinker on BrowseComp. BrowseMaster averages 12.11 tool calls per invocation, with a maximum of 244.76 calls, while WebThinker is limited to one call per invocation. This efficiency stems from BrowseMaster\u2019s code-driven approach, which integrates loops, parallel processing, and conditional logic within a single tool invocation. By selectively adjusting printed variables, BrowseMaster minimizes context usage, allowing for scalable and efficient tool calls. This enhanced efficiency enables broader search coverage, as shown in Figure [6](https://arxiv.org/html/2508.09129v1#S4.F6 \"Figure 6 \u2023 4.3 Analysis \u2023 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\"), which visualizes the diverse pages visited by BrowseMaster compared to WebThinker. The ability to scale up exploration across a wider range of sources significantly boosts BrowseMaster\u2019s performance on complex information-seeking tasks.\nReport issue for preceding element\nInteraction times reveals task complexity and BrowseMaster\u2019s adaptability. Figure [6](https://arxiv.org/html/2508.09129v1#S4.F6 \"Figure 6 \u2023 4.3 Analysis \u2023 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\") illustrates the interaction times between planner and executor across benchmarks. Key observations include: (i) complex benchmarks like BrowseComp demand more interactions, while simpler ones like GAIA require fewer, reflecting varying task difficulties; (ii) for complex tasks, the planner breaks problems into more subtasks and triggers retries when confidence is low, boosting interaction counts for thorough and confident solutions; and (iii) BrowseMaster adeptly scale interactions for complex tasks while maintain efficiency for simpler ones, showcasing its versatility.\nReport issue for preceding element\nTable 2: Progressive accuracy gains on Browse-Comp across components. Pragmatic execution and agentic workflows drive performance gains. Executor | Primitives | Planner | Accuracy (%)  \n---|---|---|---  \n\u2713\\checkmark | \u2717 | \u2717 | 9.5  \n\u2713\\checkmark | \u2717 | \u2713\\checkmark | 11.0  \n\u2713\\checkmark | \u2713\\checkmark | \u2717 | 15.0  \n\u2713\\checkmark | \u2713\\checkmark | \u2713\\checkmark | 30.0  \nReport issue for preceding element\nIncorporating collaborative pair and programmatic tool use enhances performance. Table [2](https://arxiv.org/html/2508.09129v1#S4.T2 \"Table 2 \u2023 4.3 Analysis \u2023 4 Experiments \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\") presents the results of an ablation study evaluating BrowseMaster with and without its planner and primitives. Without these components, the executor relies on simple code to invoke tools, achieving a performance of 9.5%. Integrating the planner, which enhances task decomposition and leverages increased computation, boosts performance to 11.0%. Equipping the executor with primitives enables efficient scaling of tool usage, increasing performance to 15.0%. Combining both planner and primitives balances search breadth and reasoning depth, maximizing overall effectiveness.\nReport issue for preceding element\nExamples. We provide examples of BrowseMaster\u2019s solution trajectories in Figure [7](https://arxiv.org/html/2508.09129v1#A1.F7 \"Figure 7 \u2023 Appendix A Cases \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\"), [8](https://arxiv.org/html/2508.09129v1#A1.F8 \"Figure 8 \u2023 Appendix A Cases \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\"), [9](https://arxiv.org/html/2508.09129v1#A1.F9 \"Figure 9 \u2023 Appendix A Cases \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\"), [10](https://arxiv.org/html/2508.09129v1#A1.F10 \"Figure 10 \u2023 Appendix A Cases \u2023 BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair\").\nReport issue for preceding element"
    },
    {
        "5_Related Works": "5 Related Works\nReport issue for preceding element\nRetrieval-augmented generation. Retrieval-augmented generation (RAG) enables large language models (LLMs) to leverage external knowledge through search engines, enhancing their ability to tackle complex tasks (Lewis et al., [2020](https://arxiv.org/html/2508.09129v1#bib.bib17); Guu et al., [2020](https://arxiv.org/html/2508.09129v1#bib.bib11)). To assess retrieval capabilities, various benchmarks have been developed. Early benchmarks, such as NQ (Kwiatkowski et al., [2019](https://arxiv.org/html/2508.09129v1#bib.bib16)) and TriviaQA (Joshi et al., [2017](https://arxiv.org/html/2508.09129v1#bib.bib15)), focused on fact-checking, while later ones, including HotPotQA (Yang et al., [2018](https://arxiv.org/html/2508.09129v1#bib.bib37)), Musique (Trivedi et al., [2022](https://arxiv.org/html/2508.09129v1#bib.bib31)), and GAIA (Mialon et al., [2023](https://arxiv.org/html/2508.09129v1#bib.bib23)), emphasized multi-hop reasoning. However, these benchmarks often rely on simple keyword searches, requiring limited query iterations and following straightforward search workflows. Recently, more challenging benchmarks (Chen et al., [2025b](https://arxiv.org/html/2508.09129v1#bib.bib5); Zhou et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib40)) like BrowseComp (Wei et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib32)) have emerged, demanding that agents locate deeply entangled, hard-to-find information. These tasks present exceptionally formidable challenges, serving as rigorous testbeds for evaluating agents\u2019 abilities to conduct broad, strategic, and sustained web research.\nReport issue for preceding element\nEarly RAG methods employed single-step or iterative pipelines with predefined workflows, limiting adaptive decision-making for complex queries. Recent advances with large reasoning models integrate retrieval into the reasoning process (Wu et al., [2025c](https://arxiv.org/html/2508.09129v1#bib.bib35); Song et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib29); Chai et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib3)), adopting frameworks like ReAct (Yao et al., [2023](https://arxiv.org/html/2508.09129v1#bib.bib38)) to interleave thinking, searching, and observation. Existing approaches often focus on training search capabilities from scratch (Jin et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib13)) or generating synthetic training data (Wu et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib33); Li et al., [2025a](https://arxiv.org/html/2508.09129v1#bib.bib18); Tao et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib30)). To guide tool invocation, these methods typically use raw natural language with special tokens (e.g., \"search\"), restricting agents to sequential, single-query searches that cause context to grow linearly with each step (Li et al., [2025c](https://arxiv.org/html/2508.09129v1#bib.bib20), [b](https://arxiv.org/html/2508.09129v1#bib.bib19); Jin et al., [2025b](https://arxiv.org/html/2508.09129v1#bib.bib14)). In contrast, our approach leverages Python code as an interaction language, enabling agents to use built-in functions (e.g., web_search) for concurrent searches and programmatic extraction of web content. This empowers our agent to efficiently meet the demands of complex, real-world information-seeking tasks.\nReport issue for preceding element\nAgentic workflows. Agentic workflows enhance large language models (LLMs) by orchestrating multiple LLM calls and tool interactions to tackle complex tasks. For example, AI Co-Scientist (Gottweis et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib10)) integrates multiple agents and tools for scientific research, while ChatDev (Qian et al., [2024](https://arxiv.org/html/2508.09129v1#bib.bib27)) and MetaGPT (Hong et al., [2024](https://arxiv.org/html/2508.09129v1#bib.bib12)) develop workflows for software development, and MAS-GPT (Ye et al., [2025](https://arxiv.org/html/2508.09129v1#bib.bib39)) generates query-specific workflows represented as Python code. However, current approaches are constrained by single-turn agents limited to one action per step (text or tool use) and fixed collaboration patterns that hinder adaptability. In contrast, our framework build flexible multi-turn agents that dynamically interleave reasoning with tool usage, combined with an adaptive collaboration mechanism where planner agents intelligently invoke executors based on task demands. This approach enables more dynamic and adaptive problem-solving over existing paradigms.\nReport issue for preceding element"
    },
    {
        "6_Conclusions": "6 Conclusions\nReport issue for preceding element\nThis paper presents BrowseMaster, a novel framework that combines programmatic tool execution with strategic reasoning to enhance scalable web browsing. At its core, BrowseMaster utilizes a planner-executor agent pair, where the planner focuses on high-level reasoning and strategy formulation, while the executor ensures efficient, expansive search through code-driven interactions. This collaborative design allows BrowseMaster to achieve exceptional performance on complex information-seeking tasks. Our experimental results highlight the framework\u2019s ability to outperform both proprietary and open-source agents across multiple challenging benchmarks, demonstrating its potential for scalable and effective information retrieval. In future work, we aim to improve the executor\u2019s use of primitives for efficient search and the planner\u2019s reasoning and task allocation via model training, to optimize the overall system.\nReport issue for preceding element"
    },
    {
        "References": "References\nReport issue for preceding element\n  * Brin and Page [1998] Sergey Brin and Lawrence Page.  The anatomy of a large-scale hypertextual web search engine.  _Computer networks and ISDN systems_ , 30(1-7):107\u2013117, 1998. \n  * ByteDance Doubao [2025] ByteDance Doubao.  Doubao, 2025.  URL <http://www.doubao.com/>.  Accessed: 2025-08-05. \n  * Chai et al. [2025] Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Siheng Chen, et al.  Scimaster: Towards general-purpose scientific ai agents, part i. x-master as foundation: Can we lead on humanity\u2019s last exam?  _arXiv preprint arXiv:2507.05241_ , 2025. \n  * Chen et al. [2025a] Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li.  xverify: Efficient answer verifier for reasoning model evaluations.  _arXiv preprint arXiv:2504.10481_ , 2025a. \n  * Chen et al. [2025b] Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al.  xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations.  _arXiv preprint arXiv:2506.13651_ , 2025b. \n  * DeepMind [2025] Google DeepMind.  Gemini 2.5: Our most intelligent ai model.  <https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking>, 2025.  Accessed: 2025-06-24. \n  * DeepSeek-AI [2025] DeepSeek-AI.  Deepseek-r1-0528.  <https://huggingface.co/deepseek-ai/DeepSeek-R1-0528>, 2025.  Accessed: 2025-06-27. \n  * Given et al. [2023] Lisa M Given, Donald O Case, and Rebekah Willson.  _Looking for information: Examining research on how people engage with information_.  Emerald Publishing Limited, 2023. \n  * Google [2025] Google.  Gemini Deep Research: your personal research assistant, May 2025.  URL <https://gemini.google/overview/deep-research/>.  Accessed: 2025-08-05. \n  * Gottweis et al. [2025] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al.  Towards an ai co-scientist.  _arXiv preprint arXiv:2502.18864_ , 2025. \n  * Guu et al. [2020] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.  Retrieval augmented language model pre-training.  In _International conference on machine learning_ , pages 3929\u20133938. PMLR, 2020. \n  * Hong et al. [2024] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al.  Metagpt: Meta programming for a multi-agent collaborative framework.  In _The Twelfth International Conference on Learning Representations_ , 2024. \n  * Jin et al. [2025a] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han.  Search-r1: Training llms to reason and leverage search engines with reinforcement learning.  _arXiv preprint arXiv:2503.09516_ , 2025a. \n  * Jin et al. [2025b] Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, and Zhicheng Dou.  Decoupled planning and execution: A hierarchical reasoning framework for deep search.  _arXiv preprint arXiv:2507.02652_ , 2025b. \n  * Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.  Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.  In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pages 1601\u20131611, 2017. \n  * Kwiatkowski et al. [2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.  Natural questions: a benchmark for question answering research.  _Transactions of the Association for Computational Linguistics_ , 7:453\u2013466, 2019. \n  * Lewis et al. [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al.  Retrieval-augmented generation for knowledge-intensive nlp tasks.  _Advances in neural information processing systems_ , 33:9459\u20139474, 2020. \n  * Li et al. [2025a] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al.  Websailor: Navigating super-human reasoning for web agent.  _arXiv preprint arXiv:2507.02592_ , 2025a. \n  * Li et al. [2025b] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.  Search-o1: Agentic search-enhanced large reasoning models.  _arXiv preprint arXiv:2501.05366_ , 2025b. \n  * Li et al. [2025c] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou.  Webthinker: Empowering large reasoning models with deep research capability.  _arXiv preprint arXiv:2504.21776_ , 2025c. \n  * Marchionini [1995] Gary Marchionini.  _Information seeking in electronic environments_.  Number 9. Cambridge university press, 1995. \n  * Metaso [2025] Metaso.  Metaso, 2025.  URL <https://metaso.cn/>.  Accessed: 2025-08-05. \n  * Mialon et al. [2023] Gr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom.  Gaia: a benchmark for general ai assistants.  In _The Twelfth International Conference on Learning Representations_ , 2023. \n  * OpenAI [2024a] OpenAI.  Hello gpt-4o.  <https://openai.com/index/hello-gpt-4o/>, 2024a.  Accessed: 2025-01-23. \n  * OpenAI [2024b] OpenAI.  Introducing openai o1-preview.  <https://openai.com/index/introducing-openai-o1-preview/>, 2024b.  Accessed: 2025-01-22. \n  * OpenAI [2025] OpenAI.  Introducing deep research.  <https://openai.com/index/introducing-deep-research/>, 2025.  Accessed: 2025-06-26. \n  * Qian et al. [2024] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al.  Chatdev: Communicative agents for software development.  In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pages 15174\u201315186, 2024. \n  * Qwen Team [2025] Qwen Team.  QwQ-32B: Embracing the power of reinforcement learning, March 2025.  URL <https://qwenlm.github.io/blog/qwq-32b/>.  Accessed: 2025-08-05. \n  * Song et al. [2025] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen.  R1-searcher: Incentivizing the search capability in llms via reinforcement learning.  _arXiv preprint arXiv:2503.05592_ , 2025. \n  * Tao et al. [2025] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al.  Webshaper: Agentically data synthesizing via information-seeking formalization.  _arXiv preprint arXiv:2507.15061_ , 2025. \n  * Trivedi et al. [2022] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.  Musique: Multihop questions via single-hop question composition.  _Transactions of the Association for Computational Linguistics_ , 10:539\u2013554, 2022. \n  * Wei et al. [2025] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese.  Browsecomp: A simple yet challenging benchmark for browsing agents.  _arXiv preprint arXiv:2504.12516_ , 2025. \n  * Wu et al. [2025a] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, et al.  Webdancer: Towards autonomous information seeking agency.  _arXiv preprint arXiv:2505.22648_ , 2025a. \n  * Wu et al. [2025b] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al.  Webwalker: Benchmarking llms in web traversal.  _arXiv preprint arXiv:2501.07572_ , 2025b. \n  * Wu et al. [2025c] Junde Wu, Jiayuan Zhu, and Yuyuan Liu.  Agentic reasoning: Reasoning llms with tools for the deep research.  _arXiv preprint arXiv:2502.04644_ , 2025c. \n  * x.ai [2025] x.ai.  Grok 3 beta \u2014 The age of reasoning agents, 2025.  URL <https://x.ai/news/grok-3>.  Accessed: 2025-08-05. \n  * Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning.  Hotpotqa: A dataset for diverse, explainable multi-hop question answering.  In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_ , pages 2369\u20132380, 2018. \n  * Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.  React: Synergizing reasoning and acting in language models.  In _International Conference on Learning Representations (ICLR)_ , 2023. \n  * Ye et al. [2025] Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, and Jing Shao.  Mas-gpt: Training llms to build llm-based multi-agent systems.  _arXiv preprint arXiv:2503.03686_ , 2025. \n  * Zhou et al. [2025] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al.  Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese.  _arXiv preprint arXiv:2504.19314_ , 2025."
    },
    {
        "Appendix_A_Cases": "Appendix A Cases\nReport issue for preceding element ![Refer to caption](https://arxiv.org/html/x9.png) Figure 7: Case 1. The planner identifies the most tractable clue to narrow the search scope, then efficiently delegates tasks to the executor. Report issue for preceding element ![Refer to caption](https://arxiv.org/html/x10.png) Figure 8: Case 2. The planner leverages its internal knowledge to infer key features of the search target, and performs calculations to validate candidates. Report issue for preceding element ![Refer to caption](https://arxiv.org/html/x11.png) Figure 9: Case 3. The executor strategically expands search keywords and filters relevant pages with primitives, maintaining context efficiency. Report issue for preceding element ![Refer to caption](https://arxiv.org/html/x12.png) Figure 10: Case 4. The executor customize filtering functions by combining primitives, maintaining context efficiency. Report issue for preceding element\nGenerated on Tue Aug 12 17:56:47 2025 by [LaTeXML![Mascot Sammy](https://arxiv.org/html/2508.09129v1)](http://dlmf.nist.gov/LaTeXML/)\nReport Issue\n##### Report Github Issue\nTitle:Content selection saved. Describe the issue below:Description:\nSubmit without GithubSubmit in Github\nReport Issue for Selection"
    }
]